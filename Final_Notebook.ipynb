{
 "cells": [
  {
   "attachments": {
    "Robot%20Precision%20Agriculture.jpeg": {
     "image/jpeg": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Crop and Weed Object Detection</center>\n",
    "![Robot%20Precision%20Agriculture.jpeg](attachment:Robot%20Precision%20Agriculture.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weeds have always been a problem in edible crop agriculture. They compete with crops for vital resources and can reduce their overall production. One of the most common practices to deal with weeds is to broadcast spray foliar herbicide over an entire field. However, herbicide usage in crop production can have many unintended negative effects not only on the environment but the humans who end up ingesting the crops as well. Currently, there are several companies who are working on solutions to this problem. One approach involves using robots who rely on machine vision to selectively spray herbicide or use some other elimination technique only on the areas that contain the weeds. In order to do this the robot must be able to distinguish the difference between the weed and the edible crop. Additionally, the robot must also be able to locate (draw bounding boxes around) both the weed and crop in order to properly preserve the crop and eliminate the weed. Below are the steps necessary for developing an object detection model which would the driving mechanism behind this machine vision task. Ultimately, this would allow farmers to reduce the use of herbicides, potentially lowering production costs as well as improving the health of humans and the planet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data used in this project was sourced from a [Kaggle](https://www.kaggle.com/datasets/ravirajsinh45/crop-and-weed-detection-data-with-bounding-boxes) dataset. The dataset contained 546 origial colored (RGB) 512x512 images of sesame crops and weeds. The original images were augmented to produce 1300 unique images total in the dataset. Each image had an associated .txt file annotation in YOLO format which contained the ground truth class labels and bounding box locations. The YOLO annotation format is as follows: \n",
    "\n",
    "(Class Label, Bounding Box \"X\" Center Coordinate, Bounding Box \"Y\" Center Coordinate, Bounding Box Width, Bounding Box Height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infomation from this [PyImageSearch](https://pyimagesearch.com/2020/10/12/multi-class-object-detection-and-bounding-box-regression-with-keras-tensorflow-and-deep-learning/) tutorial was very helpful in understanding the process and workflow for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to import the necessary libraries for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.13) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "# The libraries we need for this project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import Xception, VGG16, ResNet152V2, InceptionResNetV2, MobileNetV2, DenseNet201, NASNetLarge, EfficientNetB7\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from mean_average_precision import MetricBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to import the data from the repositories 'Data' folder. To do this we need to create a path to the data and then make a seperate list for the images and assocatied annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting data paths and making list of each file type\n",
    "path = '../Data/'\n",
    "\n",
    "# creating list of images\n",
    "image_list = [x for x in os.listdir(path) if x.endswith('.jpeg')]\n",
    "\n",
    "# creating list of labels\n",
    "label_list = [x for x in os.listdir(path) if x.endswith('.txt')]\n",
    "\n",
    "# sorting the list of images and labels so order matches\n",
    "image_list.sort()\n",
    "label_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking image and label lists\n",
    "image_list_check = [x[:-5] for x in image_list]\n",
    "label_list_check = [x[:-4] for x in label_list]\n",
    "                    \n",
    "image_list_check == label_list_check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we abstract the annotation data drom the .txt file and put it into a dataframe in YOLO format. Yolo annotation format is as follows:\n",
    "\n",
    "(Image File Name, Class Label, Bounding Box \"X\" Center Coordinate, Bounding Box \"Y\" Center Coordinate, Bounding Box Width, Bounding Box Height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating initial dataframe from txt files (YOLO format)\n",
    "df = pd.DataFrame(columns=['image_file', 'class', 'x_center', 'y_center', 'width', 'height'])\n",
    "\n",
    "df['image_file'] = image_list\n",
    "\n",
    "i=0\n",
    "for file in df['image_file']:\n",
    "    label = open(path + file[:-5] +'.txt', 'r').readline().split()\n",
    "    df.loc[i, 'class'] = int(label[0])\n",
    "    df.loc[i, 'x_center'] = float(label[1])\n",
    "    df.loc[i, 'y_center'] = float(label[2])\n",
    "    df.loc[i, 'width'] = float(label[3])\n",
    "    df.loc[i, 'height'] = float(label[4])\n",
    "    i +=1\n",
    "\n",
    "# checking dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking all data got pulled into dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting Data for Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is in a dataframe we conrvet from YOLO to Pascal VOC format. The Pascal VOC annotation format is as follows:\n",
    "\n",
    "(Image File Name, Image Width, Image Height, Bounding Box \"X\" Minimum, Bounding Box \"Y\" Minimum, Bounding Box \"X\" Maximum, Bounding Box \"Y\" Maximum, Class Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataframe in Pascal VOC format with bounding boxes scaled (from originl data)\n",
    "df_pas_voc = pd.DataFrame(columns = ['filename', 'width', 'height', 'x_min', 'y_min', 'x_max', 'y_max', 'class'])\n",
    "\n",
    "df_pas_voc['filename'] = df['image_file']\n",
    "df_pas_voc['class'] = df['class']\n",
    "df_pas_voc['width'] = 512\n",
    "df_pas_voc['height'] = 512\n",
    "\n",
    "for i in range(len(df_pas_voc)):\n",
    "    df_pas_voc.loc[i, 'x_min'] = df.loc[i, 'x_center'] - (df.loc[i, 'width']/2)\n",
    "    df_pas_voc.loc[i, 'y_min'] = df.loc[i, 'y_center'] - (df.loc[i, 'height']/2)\n",
    "    df_pas_voc.loc[i, 'x_max'] = df.loc[i, 'x_center'] + (df.loc[i, 'width']/2)\n",
    "    df_pas_voc.loc[i, 'y_max'] = df.loc[i, 'y_center'] + (df.loc[i, 'height']/2)\n",
    "\n",
    "# checking pascal voc dataframe\n",
    "df_pas_voc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then load the images, reduce their size to 224x224 and scale their pixel intensities to between 0 and 1 to reduce the computational requirements during modeling. Then we format the image data into a numpy array which necessary for modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading images and preprocessing into and array of pixel values\n",
    "data = []\n",
    "for filename in df_pas_voc['filename']:\n",
    "    image = load_img(path + filename, target_size=(224, 224))\n",
    "    image = img_to_array(image)\n",
    "    data.append(image)\n",
    "    \n",
    "# converting data (images) into numpy array and scaling pixel intensities\n",
    "data = np.array(data, dtype='float32') / 255.0\n",
    "\n",
    "# checking format of data\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we one hot encode our class labels and format them into a numpy array for modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating list of labels\n",
    "labels = []\n",
    "for i in range(len(df_pas_voc)):\n",
    "    labels.append(df_pas_voc.loc[i, 'class']) \n",
    "\n",
    "# converting labels list into numpy array\n",
    "labels = np.array(labels)    \n",
    "\n",
    "# one hot encoding labels using tf's utility function\n",
    "labels = to_categorical(labels)\n",
    "\n",
    "# checking format of labels    \n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we format our bounding box location data into a numpy array for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating list of bounding box values\n",
    "bboxes = []\n",
    "for i in range(len(df_pas_voc)):\n",
    "    bboxes.append((df_pas_voc.loc[i, 'x_min'], df_pas_voc.loc[i, 'y_min'], df_pas_voc.loc[i, 'x_max'], df_pas_voc.loc[i, 'y_max']))\n",
    "\n",
    "# converting bounding boxes into numpy array\n",
    "bboxes = np.array(bboxes, dtype=\"float32\")\n",
    "\n",
    "# checking format of bounding boxes\n",
    "bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we create a numyp array of image paths for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating list of image paths\n",
    "image_paths = []\n",
    "for i in range(len(df_pas_voc)):\n",
    "    image_paths.append((path + df_pas_voc.loc[i, \"filename\"]))\n",
    "    \n",
    "# converting image paths into numpy array\n",
    "image_paths = np.array(image_paths)\n",
    "\n",
    "# checking format of image paths\n",
    "image_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we split our data into training and testing sets using sklearns train_test_split function. The training set contains 80% of the original data and the test set contains 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating train test split\n",
    "split1 = train_test_split(data, labels, bboxes, image_paths, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting variables from train test split\n",
    "(x_train_0, x_test) = split1[:2]\n",
    "(labels_train_0, labels_test) = split1[2:4]\n",
    "(bboxes_train_0, bboxes_test) = split1[4:6]\n",
    "(image_paths_train_0, image_paths_test) = split1[6:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we split our training data so we can have a validation set. The validation set contains 10% of the original training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splittng training data for validation\n",
    "split2 = train_test_split(x_train_0, labels_train_0, bboxes_train_0, image_paths_train_0, test_size=0.1, random_state=42)\n",
    "\n",
    "(x_train, x_val) = split2[:2]\n",
    "(labels_train, labels_val) = split2[2:4]\n",
    "(bboxes_train, bboxes_val) = split2[4:6]\n",
    "(image_paths_train, image_paths_val) = split2[6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking shape from train test split\n",
    "print(f\"x_train shape : {x_train.shape}\")\n",
    "print(f\"x_val shape : {x_val.shape}\")\n",
    "print(f\"x_test shape : {x_test.shape}\")\n",
    "print(f\"labels_train shape : {labels_train.shape}\")\n",
    "print(f\"labels_val shape : {labels_val.shape}\")\n",
    "print(f\"labels_test shape : {labels_test.shape}\")\n",
    "print(f\"bboxes_train shape : {bboxes_train.shape}\")\n",
    "print(f\"bboxes_val shape : {bboxes_val.shape}\")\n",
    "print(f\"bboxes_test shape : {bboxes_test.shape}\")\n",
    "print(f\"image_paths_train shape : {image_paths_train.shape}\")\n",
    "print(f\"image_paths_val shape : {image_paths_val.shape}\")\n",
    "print(f\"image_paths_test shape : {image_paths_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Evaluation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create some useful functions that will help us plot the loss and accuracy of our different model iterations during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating function for plotting loss data after model training\n",
    "def plot_loss(model_name, num_epochs):\n",
    "    # plotting the total loss, class label loss, and bounding box loss during model training\n",
    "    loss_names = [\"loss\", \"class_label_loss\", \"bounding_box_loss\"]\n",
    "    N = np.arange(0, num_epochs)\n",
    "    plt.style.use(\"ggplot\")\n",
    "    (fig, ax) = plt.subplots(3, 1, figsize=(13, 13))\n",
    "    fig.tight_layout(pad=5.0)\n",
    "   \n",
    "\n",
    "    # loop over the loss names\n",
    "    for (i, l) in enumerate(loss_names):\n",
    "        # plot the loss for both the training and validation data\n",
    "        title = \"Loss for {}\".format(l) if l != \"loss\" else \"Total loss\"\n",
    "        ax[i].set_title(title)\n",
    "        ax[i].set_xlabel(\"Epoch #\")\n",
    "        ax[i].set_ylabel(\"Loss\")\n",
    "        ax[i].plot(N, H.history[l], label=l)\n",
    "        ax[i].plot(N, H.history[\"val_\" + l], label=\"val_\" + l)\n",
    "        ax[i].legend()\n",
    "    \n",
    "    # printing final epoch total loss, class label loss and bounding box loss\n",
    "    print(f\"Final epoch training total loss during {model_name} training: {H.history['loss'][num_epochs-1]}\")\n",
    "    print(f\"Final epoch training class label loss during {model_name} training: {H.history['class_label_loss'][num_epochs-1]}\")\n",
    "    print(f\"Final epoch training bounding box loss during {model_name} training: {H.history['bounding_box_loss'][num_epochs-1]}\")\n",
    "    \n",
    "    print(f\"Final epoch validation total loss during {model_name} training: {H.history['val_loss'][num_epochs-1]}\")\n",
    "    print(f\"Final epoch validation class label loss during {model_name} training: {H.history['val_class_label_loss'][num_epochs-1]}\")\n",
    "    print(f\"Final epoch validation bounding box loss during {model_name} training: {H.history['val_bounding_box_loss'][num_epochs-1]}\")\n",
    "    \n",
    "    # saving plot\n",
    "    plt.savefig(f\"../Plots/Loss_Plots/{model_name}_training_loss\")\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating function for plotting accuracy data after model training\n",
    "def plot_accuracy(model_name, num_epochs):\n",
    "    # plotting the class label accuracies during model training\n",
    "    N = np.arange(0, num_epochs) #number of epochs\n",
    "    plt.style.use(\"ggplot\")\n",
    "    plt.figure()\n",
    "    plt.plot(N, H.history[\"class_label_accuracy\"],\n",
    "    label=\"class_label_train_acc\")\n",
    "    plt.plot(N, H.history[\"val_class_label_accuracy\"],\n",
    "    label=\"class_label_val_acc\")\n",
    "    plt.title(\"Class Label Accuracy\")\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    # saving plot\n",
    "    plt.savefig(f\"../Plots/Accuracy_Plots{model_name}_training_label_accuracy\");\n",
    "    \n",
    "    # printing final epoch accuracy\n",
    "    print(f\"Final epoch training class label accurracy during {model_name} training: {H.history['class_label_accuracy'][num_epochs-1]}\")\n",
    "    print(f\"Final epoch validation class label accurracy during {model_name} training: {H.history['val_class_label_accuracy'][num_epochs-1]}\")\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we created a set of functions that will help us calculate the mean average precision scores according to Pascal VOC and COCO formats as well as the average inference time to predict an image's labels. Ultiamtely, these metrics will help us evaluate a model's performance, driving the iteration process in order to select the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function that computes three different mAP metrics for evaluating object detection models\n",
    "def evaluate_val(model, model_name):\n",
    "    # getting the predictions for the bounding boxes, class label and confidence for the test set and avg inference time to make predictions on image\n",
    "    preds = []\n",
    "    inf_time = []\n",
    "    \n",
    "    for i in range(len(image_paths_val)):\n",
    "        image = load_img(image_paths_val[i], target_size=(224, 224))\n",
    "        image = img_to_array(image) / 255.0\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "    \n",
    "        start_time = time.time()\n",
    "        (bboxPred, labelPred) = model.predict(image)\n",
    "        stop_time= time.time()\n",
    "        duration = stop_time - start_time\n",
    "        inf_time.append(duration)\n",
    "    \n",
    "        (startX, startY, endX, endY) = bboxPred[0]\n",
    "        startX = int(startX * 224)\n",
    "        startY = int(startY * 224)\n",
    "        endX = int(endX * 224)\n",
    "        endY = int(endY * 224)\n",
    "        lbl = int(np.argmax(labelPred, axis=1)) #predicted class label\n",
    "        cf = np.max(labelPred) #predicted class confidence\n",
    "        preds.append([startX, startY, endX, endY, lbl, cf])\n",
    "\n",
    "    # preds format = [xmin, ymin, xmax, ymax, class_id, confidence]\n",
    "    preds = np.array(preds)\n",
    "    \n",
    "    # computing avg inference time for each image\n",
    "    avg_inf_time = np.mean(inf_time)\n",
    "                 \n",
    "    # getting the ground truths for bounding boxes and class labels (and setting \"difficult\" and \"crowd\" to 0)\n",
    "    gts = []\n",
    "    \n",
    "    for i in range(len(image_paths_val)):\n",
    "        gt_label = int(np.argmax(labels_val[i]))\n",
    "        (gt_startX, gt_startY, gt_endX, gt_endY) = bboxes_val[i]\n",
    "        gt_startX = int(gt_startX * 224)\n",
    "        gt_startY = int(gt_startY * 224)\n",
    "        gt_endX = int(gt_endX * 224)\n",
    "        gt_endY = int(gt_endY * 224)\n",
    "        gts.append([gt_startX, gt_startY, gt_endX, gt_endY, gt_label, 0, 0])\n",
    "\n",
    "    # gts format = [xmin, ymin, xmax, ymax, class_id, difficult, crowd]\n",
    "    gts = np.array(gts)\n",
    "    \n",
    "    # instantiating map function and adding preds and gts to it               \n",
    "    map_fn = MetricBuilder.build_evaluation_metric(\"map_2d\", async_mode=True, num_classes=2)\n",
    "    for i in range(len(image_paths_val)):\n",
    "        map_fn.add(preds, gts)\n",
    "    \n",
    "    # computing PASCAL VOC metric\n",
    "    voc_pascal_map = map_fn.value(iou_thresholds=0.5, recall_thresholds=np.arange(0., 1.1, 0.1))['mAP']\n",
    "    # computing PASCAL VOC metric at the all points\n",
    "    voc_pascal_map_allpts = map_fn.value(iou_thresholds=0.5)['mAP']\n",
    "    # computing metric COCO metric\n",
    "    coco_map = map_fn.value(iou_thresholds=np.arange(0.5, 1.0, 0.05), recall_thresholds=np.arange(0., 1.01, 0.01), mpolicy='soft')['mAP']\n",
    "    \n",
    "    print(f\"VOC PASCAL mAP for {model_name} validation data: {voc_pascal_map}\")\n",
    "    print(f\"VOC PASCAL mAP in all points for {model_name} validation data: {voc_pascal_map_allpts}\")\n",
    "    print(f\"COCO mAP for {model_name} validation data: {coco_map}\")\n",
    "    print(f\"Average inference time for {model_name} validation data: {avg_inf_time}\")  \n",
    "    \n",
    "    # returning a dictionary of the mAP scores and inference time to be added to the results dataframe (df_results)\n",
    "    result_val = {'model': model_name, 'voc_pascal_map': voc_pascal_map, 'voc_pascal_map_allpts': voc_pascal_map_allpts, 'coco_map': coco_map, 'avg_inf_time': avg_inf_time}\n",
    "    return result_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function that computes three different mAP metrics for evaluating object detection models\n",
    "def evaluate_test(model, model_name):\n",
    "    # getting the predictions for the bounding boxes, class label and confidence for the test set and avg inference time to make predictions on image\n",
    "    preds = []\n",
    "    inf_time = []\n",
    "    \n",
    "    for i in range(len(image_paths_test)):\n",
    "        image = load_img(image_paths_test[i], target_size=(224, 224))\n",
    "        image = img_to_array(image) / 255.0\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "    \n",
    "        start_time = time.time()\n",
    "        (bboxPred, labelPred) = model.predict(image)\n",
    "        stop_time= time.time()\n",
    "        duration = stop_time - start_time\n",
    "        inf_time.append(duration)\n",
    "    \n",
    "        (startX, startY, endX, endY) = bboxPred[0]\n",
    "        startX = int(startX * 224)\n",
    "        startY = int(startY * 224)\n",
    "        endX = int(endX * 224)\n",
    "        endY = int(endY * 224)\n",
    "        lbl = int(np.argmax(labelPred, axis=1)) #predicted class label\n",
    "        cf = np.max(labelPred) #predicted class confidence\n",
    "        preds.append([startX, startY, endX, endY, lbl, cf])\n",
    "\n",
    "    # preds format = [xmin, ymin, xmax, ymax, class_id, confidence]\n",
    "    preds = np.array(preds)\n",
    "    \n",
    "    # computing avg inference time for each image\n",
    "    avg_inf_time = np.mean(inf_time)\n",
    "                 \n",
    "    # getting the ground truths for bounding boxes and class labels (and setting \"difficult\" and \"crowd\" to 0)\n",
    "    gts = []\n",
    "    \n",
    "    for i in range(len(image_paths_test)):\n",
    "        gt_label = int(np.argmax(labels_test[i]))\n",
    "        (gt_startX, gt_startY, gt_endX, gt_endY) = bboxes_test[i]\n",
    "        gt_startX = int(gt_startX * 224)\n",
    "        gt_startY = int(gt_startY * 224)\n",
    "        gt_endX = int(gt_endX * 224)\n",
    "        gt_endY = int(gt_endY * 224)\n",
    "        gts.append([gt_startX, gt_startY, gt_endX, gt_endY, gt_label, 0, 0])\n",
    "\n",
    "    # gts format = [xmin, ymin, xmax, ymax, class_id, difficult, crowd]\n",
    "    gts = np.array(gts)\n",
    "    \n",
    "    # instantiating map function and adding preds and gts to it               \n",
    "    map_fn = MetricBuilder.build_evaluation_metric(\"map_2d\", async_mode=True, num_classes=2)\n",
    "    for i in range(len(image_paths_test)):\n",
    "        map_fn.add(preds, gts)\n",
    "    \n",
    "    # computing PASCAL VOC metric\n",
    "    voc_pascal_map = map_fn.value(iou_thresholds=0.5, recall_thresholds=np.arange(0., 1.1, 0.1))['mAP']\n",
    "    # computing PASCAL VOC metric at the all points\n",
    "    voc_pascal_map_allpts = map_fn.value(iou_thresholds=0.5)['mAP']\n",
    "    # computing metric COCO metric\n",
    "    coco_map = map_fn.value(iou_thresholds=np.arange(0.5, 1.0, 0.05), recall_thresholds=np.arange(0., 1.01, 0.01), mpolicy='soft')['mAP']\n",
    "    \n",
    "    print(f\"VOC PASCAL mAP for {model_name} test data: {voc_pascal_map}\")\n",
    "    print(f\"VOC PASCAL mAP in all points for {model_name} test data: {voc_pascal_map_allpts}\")\n",
    "    print(f\"COCO mAP for {model_name} test data: {coco_map}\")\n",
    "    print(f\"Average inference time for {model_name} test data: {avg_inf_time}\")  \n",
    "    \n",
    "    # returning a dictionary of the mAP scores and inference time to be added to the results dataframe (df_results)\n",
    "    result_test = {'model': model_name, 'voc_pascal_map': voc_pascal_map, 'voc_pascal_map_allpts': voc_pascal_map_allpts, 'coco_map': coco_map, 'avg_inf_time': avg_inf_time}\n",
    "    return result_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created two data frames, one to store the mean average precion scores and average inference time during the modeling process and the other to store the final models evualuation metrics for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating data frame which stores mAp scores and inference time for each model's validation data\n",
    "df_results_val= pd.DataFrame(columns = ['model', 'voc_pascal_map', 'voc_pascal_map_allpts', 'coco_map', 'avg_inf_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating data frame which stores mAp scores and inference time for each model's test data\n",
    "df_results_test = pd.DataFrame(columns = ['model', 'voc_pascal_map', 'voc_pascal_map_allpts', 'coco_map', 'avg_inf_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling - Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the modeling process we utlized transfer learning techniques by using the structure and weights from seven different keras models which were pretrained on the imagenet dataset to create the 'backbone' of our model. These models included Xception, VGG16, ResNet152V2, MobileNetV2, DenseNet201 and NASNetLarge. We also created one custom simple model \"backbone\" which was trained on the crop and weed dataset. The \"backbones\" were connected to a pair of different model \"heads\", one for predicting class lables and one for predicting bounding box labels. \n",
    "We then used the training loss information and the COCO mean average precision (mAP) metric to iterate through several different versions of the model heads for each backbone. Below is the structure of the custom backbone and each model head:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Model Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating custom model backbone\n",
    "input_layer = Input(shape=(224,224,3), name='input_layer')\n",
    "conv1 = Conv2D(32, (3,3), activation='relu')(input_layer)\n",
    "pool1 = MaxPool2D(pool_size=(2, 2))(conv1)\n",
    "conv2 = Conv2D(64, (3,3), activation='relu')(pool1)\n",
    "pool2 = MaxPool2D(pool_size=(2, 2))(conv2)\n",
    "conv3 = Conv2D(128, (3,3), activation='relu')(pool2)\n",
    "pool3 = MaxPool2D(pool_size=(2, 2))(conv3)\n",
    "conv4 = Conv2D(256, (3,3), activation='relu')(pool3)\n",
    "pool4 = MaxPool2D(pool_size=(2, 2))(conv4)\n",
    "conv5 = Conv2D(512, (3,3), activation='relu')(pool4)\n",
    "pool5 = MaxPool2D(pool_size=(2, 2))(conv5)\n",
    "flat = Flatten()(pool5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Heads V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing an output layer to predict bounding box coordinates\n",
    "bbox_head1 = Dense(128, activation=\"relu\")(flat)\n",
    "bbox_head2 = Dense(64, activation=\"relu\")(bbox_head1)\n",
    "bbox_head3 = Dense(32, activation=\"relu\")(bbox_head2)\n",
    "bbox_head_v1 = Dense(4, activation=\"sigmoid\", name=\"bounding_box\")(bbox_head3)\n",
    "\n",
    "# constructing an output layer to predict class labels\n",
    "class_head1 = Dense(512, activation=\"relu\")(flat)\n",
    "class_head2 = Dropout(0.5)(class_head1)\n",
    "class_head3 = Dense(512, activation=\"relu\")(class_head2)\n",
    "class_head4 = Dropout(0.5)(class_head3)\n",
    "class_head_v1 = Dense(2, activation=\"sigmoid\", name=\"class_label\")(class_head4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Heads V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing an output layer to predict bounding box coordinates\n",
    "bbox_head1 = Dense(128, activation=\"relu\")(flat)\n",
    "bbox_head2 = Dropout(0.5)(bbox_head1)\n",
    "bbox_head3 = Dense(64, activation=\"relu\")(bbox_head2)\n",
    "bbox_head4 = Dropout(0.5)(bbox_head3)\n",
    "bbox_head5 = Dense(32, activation=\"relu\")(bbox_head4)\n",
    "bbox_head6 = Dropout(0.5)(bbox_head5)\n",
    "bbox_head_v2 = Dense(4, activation=\"sigmoid\", name=\"bounding_box\")(bbox_head6)\n",
    "\n",
    "# constructing an output layer to predict class labels\n",
    "class_head1 = Dense(512, activation=\"relu\")(flat)\n",
    "class_head2 = Dropout(0.5)(class_head1)\n",
    "class_head3 = Dense(512, activation=\"relu\")(class_head2)\n",
    "class_head4 = Dropout(0.5)(class_head3)\n",
    "class_head5 = Dense(512, activation=\"relu\")(class_head4)\n",
    "class_head6 = Dropout(0.5)(class_head5)\n",
    "class_head_v2 = Dense(2, activation=\"sigmoid\", name=\"class_label\")(class_head6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Heads V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing an output layer to predict bounding box coordinates\n",
    "bbox_head1 = Dense(128, activation=\"relu\")(flat)\n",
    "bbox_head2 = Dropout(0.5)(bbox_head1)\n",
    "bbox_head3 = Dense(64, activation=\"relu\")(bbox_head2)\n",
    "bbox_head4 = Dense(32, activation=\"relu\")(bbox_head3)\n",
    "bbox_head_v3 = Dense(4, activation=\"sigmoid\", name=\"bounding_box\")(bbox_head4)\n",
    "\n",
    "# constructing an output layer to predict class labels\n",
    "class_head1 = Dense(512, activation=\"relu\")(flat)\n",
    "class_head2 = Dropout(0.5)(class_head1)\n",
    "class_head3 = Dense(512, activation=\"relu\")(class_head2)\n",
    "class_head4 = Dropout(0.5)(class_head3)\n",
    "class_head5 = Dense(256, activation=\"relu\")(class_head4)\n",
    "class_head6 = Dropout(0.5)(class_head5)\n",
    "class_head7 = Dense(128, activation=\"relu\")(class_head6)\n",
    "class_head8 = Dropout(0.5)(class_head7)\n",
    "class_head_v3 = Dense(2, activation=\"sigmoid\", name=\"class_label\")(class_head8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Heads V4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing an output layer to predict bounding box coordinates\n",
    "bbox_head1 = Dense(256, activation=\"relu\")(flat)\n",
    "bbox_head2 = Dropout(0.5)(bbox_head1)\n",
    "bbox_head3 = Dense(128, activation=\"relu\")(bbox_head2)\n",
    "bbox_head4 = Dense(64, activation=\"relu\")(bbox_head3)\n",
    "bbox_head5 = Dense(32, activation=\"relu\")(bbox_head4)\n",
    "bbox_head_v4 = Dense(4, activation=\"sigmoid\", name=\"bounding_box\")(bbox_head5)\n",
    "\n",
    "# constructing an output layer to predict class labels\n",
    "class_head1 = Dense(512, activation=\"relu\")(flat)\n",
    "class_head2 = Dropout(0.5)(class_head1)\n",
    "class_head3 = Dense(512, activation=\"relu\")(class_head2)\n",
    "class_head4 = Dropout(0.5)(class_head3)\n",
    "class_head5 = Dense(256, activation=\"relu\")(class_head4)\n",
    "class_head6 = Dropout(0.5)(class_head5)\n",
    "class_head7 = Dense(128, activation=\"relu\")(class_head6)\n",
    "class_head8 = Dropout(0.5)(class_head7)\n",
    "class_head_v4 = Dense(2, activation=\"sigmoid\", name=\"class_label\")(class_head8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Heads V5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing an output layer to predict bounding box coordinates\n",
    "bbox_head1 = Dense(256, activation=\"relu\")(flat)\n",
    "bbox_head2 = Dropout(0.5)(bbox_head1)\n",
    "bbox_head3 = Dense(128, activation=\"relu\")(bbox_head2)\n",
    "bbox_head4 = Dense(64, activation=\"relu\")(bbox_head3)\n",
    "bbox_head5 = Dense(32, activation=\"relu\")(bbox_head4)\n",
    "bbox_head_v5 = Dense(4, activation=\"sigmoid\", name=\"bounding_box\")(bbox_head5)\n",
    "\n",
    "# constructing an output layer to predict class labels\n",
    "class_head1 = Dense(512, activation=\"relu\")(flat)\n",
    "class_head2 = Dropout(0.5)(class_head1)\n",
    "class_head3 = Dense(256, activation=\"relu\")(class_head2)\n",
    "class_head4 = Dropout(0.5)(class_head3)\n",
    "class_head_v5 = Dense(2, activation=\"sigmoid\", name=\"class_label\")(class_head4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Heads V6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing an output layer to predict bounding box coordinates\n",
    "bbox_head1 = Dense(256, activation=\"relu\")(flat)\n",
    "bbox_head2 = Dense(128, activation=\"relu\")(bbox_head1)\n",
    "bbox_head3 = Dense(64, activation=\"relu\")(bbox_head2)\n",
    "bbox_head4 = Dense(32, activation=\"relu\")(bbox_head3)\n",
    "bbox_head_v6 = Dense(4, activation=\"sigmoid\", name=\"bounding_box\")(bbox_head4)\n",
    "\n",
    "# constructing an output layer to predict class labels\n",
    "class_head1 = Dense(256, activation=\"relu\")(flat)\n",
    "class_head2 = Dropout(0.5)(class_head1)\n",
    "class_head3 = Dense(128, activation=\"relu\")(class_head2)\n",
    "class_head4 = Dropout(0.5)(class_head3)\n",
    "class_head_v6 = Dense(2, activation=\"sigmoid\", name=\"class_label\")(class_head4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Heads V7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing an output layer to predict bounding box coordinates\n",
    "bbox_head1 = Dense(128, activation=\"relu\")(flat)\n",
    "bbox_head2 = Dropout(0.5)(bbox_head1)\n",
    "bbox_head3 = Dense(64, activation=\"relu\")(bbox_head2)\n",
    "bbox_head4 = Dense(32, activation=\"relu\")(bbox_head3)\n",
    "bbox_head_v7 = Dense(4, activation=\"sigmoid\", name=\"bounding_box\")(bbox_head4)\n",
    "\n",
    "# constructing an output layer to predict class labels\n",
    "class_head1 = Dense(128, activation=\"relu\")(flat)\n",
    "class_head2 = Dropout(0.5)(class_head1)\n",
    "class_head3 = Dense(64, activation=\"relu\")(class_head2)\n",
    "class_head4 = Dropout(0.5)(class_head3)\n",
    "class_head_v7 = Dense(2, activation=\"sigmoid\", name=\"class_label\")(class_head4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the parameters for the model training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dictionary for setting loss functions\n",
    "losses = {\"class_label\": \"binary_crossentropy\", \"bounding_box\": \"mean_squared_error\"}\n",
    "\n",
    "# creating dictionry for weighting loss functions\n",
    "loss_weights = {\"class_label\": 1.0, \"bounding_box\": 1.0}\n",
    "\n",
    "# initilizing an \"Adam\" optimizer with learning rate of 0.0001\n",
    "opt = Adam(learning_rate = 0.0001)\n",
    "\n",
    "# setting batch size\n",
    "batch_size = 32\n",
    "\n",
    "# setting number of epcochs\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading network but leaving off output layers\n",
    "xception = Xception(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "# freezing all network layers during training so they are not updated\n",
    "xception.trainable = False\n",
    "\n",
    "# flattening the max-pooling output of network\n",
    "flatten = xception.output\n",
    "flat = Flatten()(flatten)\n",
    "\n",
    "# constructing an output layer to predict bounding box coordinates\n",
    "bbox_head1 = Dense(128, activation=\"relu\")(flat)\n",
    "bbox_head2 = Dense(64, activation=\"relu\")(bbox_head1)\n",
    "bbox_head3 = Dense(32, activation=\"relu\")(bbox_head2)\n",
    "bbox_head_v1 = Dense(4, activation=\"sigmoid\", name=\"bounding_box\")(bbox_head3)\n",
    "\n",
    "# constructing an output layer to predict class labels\n",
    "class_head1 = Dense(512, activation=\"relu\")(flat)\n",
    "class_head2 = Dropout(0.5)(class_head1)\n",
    "class_head3 = Dense(512, activation=\"relu\")(class_head2)\n",
    "class_head4 = Dropout(0.5)(class_head3)\n",
    "class_head_v1 = Dense(2, activation=\"sigmoid\", name=\"class_label\")(class_head4)\n",
    "\n",
    "# creating a model that accepts an input image and outputs bounding box coordinates and a class label\n",
    "xception_model_v1 = Model(inputs=xception.input, outputs=(bbox_head_v1, class_head_v1))\n",
    "\n",
    "# creating dictionary for setting loss functions\n",
    "losses = {\"class_label\": \"binary_crossentropy\", \"bounding_box\": \"mean_squared_error\"}\n",
    "\n",
    "# creating dictionry for weighting loss functions\n",
    "loss_weights = {\"class_label\": 1.0, \"bounding_box\": 1.0}\n",
    "\n",
    "# initilizing an \"Adam\" optimizer with learning rate of 0.0001\n",
    "opt = Adam(learning_rate = 0.0001)\n",
    "\n",
    "# compiling model\n",
    "xception_model_v1.compile(loss=losses, optimizer=opt, metrics=[\"accuracy\"], loss_weights=loss_weights)\n",
    "print(xception_model_v1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing a dictionary for our training outputs\n",
    "train_targets = {\"class_label\": labels_train, \"bounding_box\": bboxes_train}\n",
    "\n",
    "# constructing a dictionary for our test outputs\n",
    "val_targets = {\"class_label\": labels_val, \"bounding_box\": bboxes_val}\n",
    "\n",
    "# training model for class and bounding box predictions\n",
    "H = xception_model_v1.fit(x_train, train_targets, validation_data=(x_val, val_targets), batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "\n",
    "# saving trained model\n",
    "xception_model_v1.save('../Models/xception_model_v1.h5', save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 1/30\n",
    "# 30/30 [==============================] - 70s 2s/step - loss: 0.3647 - bounding_box_loss: 0.0253 - class_label_loss: 0.3394 - bounding_box_accuracy: 0.5598 - class_label_accuracy: 0.9092 - val_loss: 0.3784 - val_bounding_box_loss: 0.0141 - val_class_label_loss: 0.3643 - val_bounding_box_accuracy: 0.6635 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 2/30\n",
    "# 30/30 [==============================] - 90s 3s/step - loss: 0.2137 - bounding_box_loss: 0.0105 - class_label_loss: 0.2032 - bounding_box_accuracy: 0.6571 - class_label_accuracy: 0.9615 - val_loss: 0.3603 - val_bounding_box_loss: 0.0110 - val_class_label_loss: 0.3493 - val_bounding_box_accuracy: 0.7019 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 3/30\n",
    "# 30/30 [==============================] - 82s 3s/step - loss: 0.1976 - bounding_box_loss: 0.0070 - class_label_loss: 0.1906 - bounding_box_accuracy: 0.7254 - class_label_accuracy: 0.9615 - val_loss: 0.3134 - val_bounding_box_loss: 0.0089 - val_class_label_loss: 0.3045 - val_bounding_box_accuracy: 0.7115 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 4/30\n",
    "# 30/30 [==============================] - 79s 3s/step - loss: 0.1476 - bounding_box_loss: 0.0054 - class_label_loss: 0.1423 - bounding_box_accuracy: 0.7639 - class_label_accuracy: 0.9701 - val_loss: 0.3621 - val_bounding_box_loss: 0.0082 - val_class_label_loss: 0.3539 - val_bounding_box_accuracy: 0.7019 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 5/30\n",
    "# 30/30 [==============================] - 80s 3s/step - loss: 0.1297 - bounding_box_loss: 0.0045 - class_label_loss: 0.1253 - bounding_box_accuracy: 0.7853 - class_label_accuracy: 0.9712 - val_loss: 0.3076 - val_bounding_box_loss: 0.0082 - val_class_label_loss: 0.2994 - val_bounding_box_accuracy: 0.7500 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 6/30\n",
    "# 30/30 [==============================] - 81s 3s/step - loss: 0.1193 - bounding_box_loss: 0.0038 - class_label_loss: 0.1155 - bounding_box_accuracy: 0.8066 - class_label_accuracy: 0.9712 - val_loss: 0.3610 - val_bounding_box_loss: 0.0080 - val_class_label_loss: 0.3529 - val_bounding_box_accuracy: 0.7212 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 7/30\n",
    "# 30/30 [==============================] - 82s 3s/step - loss: 0.0888 - bounding_box_loss: 0.0039 - class_label_loss: 0.0849 - bounding_box_accuracy: 0.7981 - class_label_accuracy: 0.9797 - val_loss: 0.3455 - val_bounding_box_loss: 0.0081 - val_class_label_loss: 0.3374 - val_bounding_box_accuracy: 0.7596 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 8/30\n",
    "# 30/30 [==============================] - 80s 3s/step - loss: 0.0861 - bounding_box_loss: 0.0028 - class_label_loss: 0.0834 - bounding_box_accuracy: 0.8526 - class_label_accuracy: 0.9808 - val_loss: 0.3387 - val_bounding_box_loss: 0.0080 - val_class_label_loss: 0.3307 - val_bounding_box_accuracy: 0.7404 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 9/30\n",
    "# 30/30 [==============================] - 81s 3s/step - loss: 0.0815 - bounding_box_loss: 0.0025 - class_label_loss: 0.0790 - bounding_box_accuracy: 0.8440 - class_label_accuracy: 0.9765 - val_loss: 0.4087 - val_bounding_box_loss: 0.0085 - val_class_label_loss: 0.4002 - val_bounding_box_accuracy: 0.7692 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 10/30\n",
    "# 30/30 [==============================] - 81s 3s/step - loss: 0.0726 - bounding_box_loss: 0.0023 - class_label_loss: 0.0702 - bounding_box_accuracy: 0.8697 - class_label_accuracy: 0.9797 - val_loss: 0.3284 - val_bounding_box_loss: 0.0082 - val_class_label_loss: 0.3202 - val_bounding_box_accuracy: 0.7500 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 11/30\n",
    "# 30/30 [==============================] - 80s 3s/step - loss: 0.0503 - bounding_box_loss: 0.0024 - class_label_loss: 0.0480 - bounding_box_accuracy: 0.8707 - class_label_accuracy: 0.9882 - val_loss: 0.4617 - val_bounding_box_loss: 0.0083 - val_class_label_loss: 0.4535 - val_bounding_box_accuracy: 0.7596 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 12/30\n",
    "# 30/30 [==============================] - 80s 3s/step - loss: 0.0634 - bounding_box_loss: 0.0021 - class_label_loss: 0.0613 - bounding_box_accuracy: 0.8611 - class_label_accuracy: 0.9840 - val_loss: 0.3530 - val_bounding_box_loss: 0.0082 - val_class_label_loss: 0.3447 - val_bounding_box_accuracy: 0.7404 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 13/30\n",
    "# 30/30 [==============================] - 80s 3s/step - loss: 0.0594 - bounding_box_loss: 0.0023 - class_label_loss: 0.0571 - bounding_box_accuracy: 0.8707 - class_label_accuracy: 0.9861 - val_loss: 0.3808 - val_bounding_box_loss: 0.0080 - val_class_label_loss: 0.3728 - val_bounding_box_accuracy: 0.7500 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 14/30\n",
    "# 30/30 [==============================] - 80s 3s/step - loss: 0.0421 - bounding_box_loss: 0.0018 - class_label_loss: 0.0404 - bounding_box_accuracy: 0.8932 - class_label_accuracy: 0.9882 - val_loss: 0.3745 - val_bounding_box_loss: 0.0084 - val_class_label_loss: 0.3661 - val_bounding_box_accuracy: 0.7692 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 15/30\n",
    "# 30/30 [==============================] - 79s 3s/step - loss: 0.0432 - bounding_box_loss: 0.0015 - class_label_loss: 0.0418 - bounding_box_accuracy: 0.8974 - class_label_accuracy: 0.9882 - val_loss: 0.3322 - val_bounding_box_loss: 0.0079 - val_class_label_loss: 0.3243 - val_bounding_box_accuracy: 0.7788 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 16/30\n",
    "# 30/30 [==============================] - 78s 3s/step - loss: 0.0426 - bounding_box_loss: 0.0013 - class_label_loss: 0.0413 - bounding_box_accuracy: 0.8857 - class_label_accuracy: 0.9925 - val_loss: 0.3584 - val_bounding_box_loss: 0.0078 - val_class_label_loss: 0.3506 - val_bounding_box_accuracy: 0.7500 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 17/30\n",
    "# 30/30 [==============================] - 78s 3s/step - loss: 0.0456 - bounding_box_loss: 0.0013 - class_label_loss: 0.0443 - bounding_box_accuracy: 0.8793 - class_label_accuracy: 0.9893 - val_loss: 0.4576 - val_bounding_box_loss: 0.0087 - val_class_label_loss: 0.4489 - val_bounding_box_accuracy: 0.7212 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 18/30\n",
    "# 30/30 [==============================] - 79s 3s/step - loss: 0.0347 - bounding_box_loss: 0.0015 - class_label_loss: 0.0332 - bounding_box_accuracy: 0.8996 - class_label_accuracy: 0.9882 - val_loss: 0.4639 - val_bounding_box_loss: 0.0082 - val_class_label_loss: 0.4557 - val_bounding_box_accuracy: 0.7308 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 19/30\n",
    "# 30/30 [==============================] - 78s 3s/step - loss: 0.0276 - bounding_box_loss: 0.0013 - class_label_loss: 0.0263 - bounding_box_accuracy: 0.9017 - class_label_accuracy: 0.9936 - val_loss: 0.4368 - val_bounding_box_loss: 0.0081 - val_class_label_loss: 0.4287 - val_bounding_box_accuracy: 0.7019 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 20/30\n",
    "# 30/30 [==============================] - 78s 3s/step - loss: 0.0334 - bounding_box_loss: 0.0013 - class_label_loss: 0.0321 - bounding_box_accuracy: 0.9135 - class_label_accuracy: 0.9947 - val_loss: 0.4170 - val_bounding_box_loss: 0.0073 - val_class_label_loss: 0.4097 - val_bounding_box_accuracy: 0.7692 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 21/30\n",
    "# 30/30 [==============================] - 76s 3s/step - loss: 0.0373 - bounding_box_loss: 0.0013 - class_label_loss: 0.0360 - bounding_box_accuracy: 0.8996 - class_label_accuracy: 0.9872 - val_loss: 0.3947 - val_bounding_box_loss: 0.0076 - val_class_label_loss: 0.3871 - val_bounding_box_accuracy: 0.7115 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 22/30\n",
    "# 30/30 [==============================] - 75s 3s/step - loss: 0.0414 - bounding_box_loss: 0.0014 - class_label_loss: 0.0399 - bounding_box_accuracy: 0.9092 - class_label_accuracy: 0.9915 - val_loss: 0.3962 - val_bounding_box_loss: 0.0080 - val_class_label_loss: 0.3882 - val_bounding_box_accuracy: 0.7404 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 23/30\n",
    "# 30/30 [==============================] - 75s 3s/step - loss: 0.0412 - bounding_box_loss: 0.0012 - class_label_loss: 0.0400 - bounding_box_accuracy: 0.9038 - class_label_accuracy: 0.9882 - val_loss: 0.4259 - val_bounding_box_loss: 0.0076 - val_class_label_loss: 0.4184 - val_bounding_box_accuracy: 0.7404 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 24/30\n",
    "# 30/30 [==============================] - 75s 3s/step - loss: 0.0262 - bounding_box_loss: 8.9989e-04 - class_label_loss: 0.0253 - bounding_box_accuracy: 0.9103 - class_label_accuracy: 0.9904 - val_loss: 0.4708 - val_bounding_box_loss: 0.0077 - val_class_label_loss: 0.4631 - val_bounding_box_accuracy: 0.7596 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 25/30\n",
    "# 30/30 [==============================] - 78s 3s/step - loss: 0.0210 - bounding_box_loss: 7.5693e-04 - class_label_loss: 0.0202 - bounding_box_accuracy: 0.9338 - class_label_accuracy: 0.9936 - val_loss: 0.5129 - val_bounding_box_loss: 0.0074 - val_class_label_loss: 0.5055 - val_bounding_box_accuracy: 0.7212 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 26/30\n",
    "# 30/30 [==============================] - 80s 3s/step - loss: 0.0189 - bounding_box_loss: 8.7472e-04 - class_label_loss: 0.0180 - bounding_box_accuracy: 0.9241 - class_label_accuracy: 0.9968 - val_loss: 0.4750 - val_bounding_box_loss: 0.0076 - val_class_label_loss: 0.4675 - val_bounding_box_accuracy: 0.7500 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 27/30\n",
    "# 30/30 [==============================] - 79s 3s/step - loss: 0.0346 - bounding_box_loss: 0.0011 - class_label_loss: 0.0335 - bounding_box_accuracy: 0.9124 - class_label_accuracy: 0.9915 - val_loss: 0.5005 - val_bounding_box_loss: 0.0084 - val_class_label_loss: 0.4921 - val_bounding_box_accuracy: 0.7596 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 28/30\n",
    "# 30/30 [==============================] - 84s 3s/step - loss: 0.0201 - bounding_box_loss: 0.0011 - class_label_loss: 0.0190 - bounding_box_accuracy: 0.9145 - class_label_accuracy: 0.9957 - val_loss: 0.5642 - val_bounding_box_loss: 0.0081 - val_class_label_loss: 0.5561 - val_bounding_box_accuracy: 0.7308 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 29/30\n",
    "# 30/30 [==============================] - 78s 3s/step - loss: 0.0311 - bounding_box_loss: 0.0010 - class_label_loss: 0.0301 - bounding_box_accuracy: 0.9028 - class_label_accuracy: 0.9904 - val_loss: 0.4871 - val_bounding_box_loss: 0.0081 - val_class_label_loss: 0.4790 - val_bounding_box_accuracy: 0.7885 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 30/30\n",
    "# 30/30 [==============================] - 77s 3s/step - loss: 0.0125 - bounding_box_loss: 0.0014 - class_label_loss: 0.0112 - bounding_box_accuracy: 0.9135 - class_label_accuracy: 0.9968 - val_loss: 0.5184 - val_bounding_box_loss: 0.0081 - val_class_label_loss: 0.5103 - val_bounding_box_accuracy: 0.7404 - val_class_label_accuracy: 0.9327"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting loss for modeling training\n",
    "plot_loss(model_name='xception_model_v1', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training total loss during xception_model_v1 training: 0.012523028999567032\n",
    "# Final epoch training class label loss during xception_model_v1 training: 0.011151311919093132\n",
    "# Final epoch training bounding box loss during xception_model_v1 training: 0.0013717153342440724\n",
    "# Final epoch validation total loss during xception_model_v1 training: 0.5184147357940674\n",
    "# Final epoch validation class label loss during xception_model_v1 training: 0.5102716684341431\n",
    "# Final epoch validation bounding box loss during xception_model_v1 training: 0.008143125101923943"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting accuracy for modeling training\n",
    "plot_accuracy(model_name='xception_model_v1', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training class label accurracy during xception_model_v1 training: 0.9967948794364929\n",
    "# Final epoch validation class label accurracy during xception_model_v1 training: 0.932692289352417"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating model for mAP metrics and inference time and storing result\n",
    "result_val = evaluate_val(model=xception_model_v1, model_name='xception_model_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC PASCAL mAP for xception_model_v1 validation data: 0.3962478041648865\n",
    "# VOC PASCAL mAP in all points for xception_model_v1 validation data: 0.3581908941268921\n",
    "# COCO mAP for xception_model_v1 validation data: 0.477825790643692\n",
    "# Average inference time for xception_model_v1 validation data: 0.13074778364254877"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending results of model evaluation to results dataframe (df_results)\n",
    "df_results_val = df_results_val.append(result_val, ignore_index=True)\n",
    "df_results_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading network but leaving off output layers\n",
    "xception = Xception(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "# freezing all VGG layers during training so they are not updated\n",
    "xception.trainable = False\n",
    "\n",
    "# flattening the max-pooling output of network\n",
    "flatten = xception.output\n",
    "flat = Flatten()(flatten)\n",
    "\n",
    "# constructing an output layer to predict bounding box coordinates\n",
    "bbox_head1 = Dense(128, activation=\"relu\")(flat)\n",
    "bbox_head2 = Dropout(0.5)(bbox_head1)\n",
    "bbox_head3 = Dense(64, activation=\"relu\")(bbox_head2)\n",
    "bbox_head4 = Dropout(0.5)(bbox_head3)\n",
    "bbox_head5 = Dense(32, activation=\"relu\")(bbox_head4)\n",
    "bbox_head6 = Dropout(0.5)(bbox_head5)\n",
    "bbox_head_v2 = Dense(4, activation=\"sigmoid\", name=\"bounding_box\")(bbox_head6)\n",
    "\n",
    "# constructing an output layer to predict class labels\n",
    "class_head1 = Dense(512, activation=\"relu\")(flat)\n",
    "class_head2 = Dropout(0.5)(class_head1)\n",
    "class_head3 = Dense(512, activation=\"relu\")(class_head2)\n",
    "class_head4 = Dropout(0.5)(class_head3)\n",
    "class_head5 = Dense(512, activation=\"relu\")(class_head4)\n",
    "class_head6 = Dropout(0.5)(class_head5)\n",
    "class_head_v2 = Dense(2, activation=\"sigmoid\", name=\"class_label\")(class_head6)\n",
    "\n",
    "# creating a model that accepts an input image and outputs bounding box coordinates and a class label\n",
    "xception_model_v2 = Model(inputs=xception.input, outputs=(bbox_head_v2, class_head_v2))\n",
    "\n",
    "# creating dictionary for setting loss functions\n",
    "losses = {\"class_label\": \"binary_crossentropy\", \"bounding_box\": \"mean_squared_error\"}\n",
    "\n",
    "# creating dictionry for weighting loss functions\n",
    "loss_weights = {\"class_label\": 1.0, \"bounding_box\": 1.0}\n",
    "\n",
    "# initilizing an \"Adam\" optimizer with learning rate of 0.0001\n",
    "opt = Adam(learning_rate = 0.0001)\n",
    "\n",
    "# compiling model\n",
    "xception_model_v2.compile(loss=losses, optimizer=opt, metrics=[\"accuracy\"], loss_weights=loss_weights)\n",
    "print(xception_model_v2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing a dictionary for our training outputs\n",
    "train_targets = {\"class_label\": labels_train, \"bounding_box\": bboxes_train}\n",
    "\n",
    "# constructing a dictionary for our test outputs\n",
    "val_targets = {\"class_label\": labels_val, \"bounding_box\": bboxes_val}\n",
    "\n",
    "# training model for class and bounding box predictions\n",
    "H = xception_model_v2.fit(x_train, train_targets, validation_data=(x_val, val_targets), batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "\n",
    "# saving trained model\n",
    "xception_model_v2.save('../Models/xception_model_v2.h5', save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 1/30\n",
    "# 30/30 [==============================] - 72s 2s/step - loss: 0.5697 - bounding_box_loss: 0.1009 - class_label_loss: 0.4688 - bounding_box_accuracy: 0.4658 - class_label_accuracy: 0.8558 - val_loss: 0.3770 - val_bounding_box_loss: 0.0233 - val_class_label_loss: 0.3537 - val_bounding_box_accuracy: 0.6058 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 2/30\n",
    "# 30/30 [==============================] - 87s 3s/step - loss: 0.3543 - bounding_box_loss: 0.0925 - class_label_loss: 0.2618 - bounding_box_accuracy: 0.4402 - class_label_accuracy: 0.9498 - val_loss: 0.3620 - val_bounding_box_loss: 0.0201 - val_class_label_loss: 0.3419 - val_bounding_box_accuracy: 0.4904 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 3/30\n",
    "# 30/30 [==============================] - 87s 3s/step - loss: 0.2799 - bounding_box_loss: 0.0716 - class_label_loss: 0.2083 - bounding_box_accuracy: 0.4701 - class_label_accuracy: 0.9594 - val_loss: 0.3355 - val_bounding_box_loss: 0.0187 - val_class_label_loss: 0.3168 - val_bounding_box_accuracy: 0.3846 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 4/30\n",
    "# 30/30 [==============================] - 81s 3s/step - loss: 0.2348 - bounding_box_loss: 0.0668 - class_label_loss: 0.1680 - bounding_box_accuracy: 0.4509 - class_label_accuracy: 0.9615 - val_loss: 0.3189 - val_bounding_box_loss: 0.0167 - val_class_label_loss: 0.3021 - val_bounding_box_accuracy: 0.3654 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 5/30\n",
    "# 30/30 [==============================] - 85s 3s/step - loss: 0.2301 - bounding_box_loss: 0.0596 - class_label_loss: 0.1705 - bounding_box_accuracy: 0.4551 - class_label_accuracy: 0.9637 - val_loss: 0.2922 - val_bounding_box_loss: 0.0165 - val_class_label_loss: 0.2757 - val_bounding_box_accuracy: 0.4038 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 6/30\n",
    "# 30/30 [==============================] - 82s 3s/step - loss: 0.1991 - bounding_box_loss: 0.0563 - class_label_loss: 0.1427 - bounding_box_accuracy: 0.4733 - class_label_accuracy: 0.9690 - val_loss: 0.2962 - val_bounding_box_loss: 0.0155 - val_class_label_loss: 0.2808 - val_bounding_box_accuracy: 0.3846 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 7/30\n",
    "# 30/30 [==============================] - 82s 3s/step - loss: 0.1570 - bounding_box_loss: 0.0505 - class_label_loss: 0.1065 - bounding_box_accuracy: 0.4637 - class_label_accuracy: 0.9722 - val_loss: 0.3056 - val_bounding_box_loss: 0.0173 - val_class_label_loss: 0.2883 - val_bounding_box_accuracy: 0.4712 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 8/30\n",
    "# 30/30 [==============================] - 82s 3s/step - loss: 0.1584 - bounding_box_loss: 0.0450 - class_label_loss: 0.1134 - bounding_box_accuracy: 0.4850 - class_label_accuracy: 0.9733 - val_loss: 0.3529 - val_bounding_box_loss: 0.0139 - val_class_label_loss: 0.3390 - val_bounding_box_accuracy: 0.5000 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 9/30\n",
    "# 30/30 [==============================] - 83s 3s/step - loss: 0.1522 - bounding_box_loss: 0.0481 - class_label_loss: 0.1041 - bounding_box_accuracy: 0.5096 - class_label_accuracy: 0.9712 - val_loss: 0.3038 - val_bounding_box_loss: 0.0149 - val_class_label_loss: 0.2889 - val_bounding_box_accuracy: 0.4519 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 10/30\n",
    "# 30/30 [==============================] - 82s 3s/step - loss: 0.1424 - bounding_box_loss: 0.0461 - class_label_loss: 0.0963 - bounding_box_accuracy: 0.4829 - class_label_accuracy: 0.9776 - val_loss: 0.3190 - val_bounding_box_loss: 0.0144 - val_class_label_loss: 0.3046 - val_bounding_box_accuracy: 0.5000 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 11/30\n",
    "# 30/30 [==============================] - 83s 3s/step - loss: 0.1310 - bounding_box_loss: 0.0427 - class_label_loss: 0.0884 - bounding_box_accuracy: 0.5075 - class_label_accuracy: 0.9765 - val_loss: 0.3228 - val_bounding_box_loss: 0.0150 - val_class_label_loss: 0.3078 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 12/30\n",
    "# 30/30 [==============================] - 81s 3s/step - loss: 0.1152 - bounding_box_loss: 0.0410 - class_label_loss: 0.0742 - bounding_box_accuracy: 0.4979 - class_label_accuracy: 0.9797 - val_loss: 0.3221 - val_bounding_box_loss: 0.0139 - val_class_label_loss: 0.3082 - val_bounding_box_accuracy: 0.5288 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 13/30\n",
    "# 30/30 [==============================] - 78s 3s/step - loss: 0.1149 - bounding_box_loss: 0.0370 - class_label_loss: 0.0779 - bounding_box_accuracy: 0.5171 - class_label_accuracy: 0.9808 - val_loss: 0.4228 - val_bounding_box_loss: 0.0150 - val_class_label_loss: 0.4079 - val_bounding_box_accuracy: 0.4135 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 14/30\n",
    "# 30/30 [==============================] - 78s 3s/step - loss: 0.0960 - bounding_box_loss: 0.0348 - class_label_loss: 0.0612 - bounding_box_accuracy: 0.4754 - class_label_accuracy: 0.9797 - val_loss: 0.3762 - val_bounding_box_loss: 0.0139 - val_class_label_loss: 0.3623 - val_bounding_box_accuracy: 0.4231 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 15/30\n",
    "# 30/30 [==============================] - 78s 3s/step - loss: 0.0957 - bounding_box_loss: 0.0376 - class_label_loss: 0.0580 - bounding_box_accuracy: 0.4808 - class_label_accuracy: 0.9861 - val_loss: 0.3705 - val_bounding_box_loss: 0.0141 - val_class_label_loss: 0.3564 - val_bounding_box_accuracy: 0.4423 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 16/30\n",
    "# 30/30 [==============================] - 78s 3s/step - loss: 0.0790 - bounding_box_loss: 0.0367 - class_label_loss: 0.0423 - bounding_box_accuracy: 0.5128 - class_label_accuracy: 0.9829 - val_loss: 0.3438 - val_bounding_box_loss: 0.0142 - val_class_label_loss: 0.3296 - val_bounding_box_accuracy: 0.4231 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 17/30\n",
    "# 30/30 [==============================] - 78s 3s/step - loss: 0.0872 - bounding_box_loss: 0.0342 - class_label_loss: 0.0531 - bounding_box_accuracy: 0.4840 - class_label_accuracy: 0.9797 - val_loss: 0.3605 - val_bounding_box_loss: 0.0163 - val_class_label_loss: 0.3441 - val_bounding_box_accuracy: 0.4615 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 18/30\n",
    "# 30/30 [==============================] - 78s 3s/step - loss: 0.0674 - bounding_box_loss: 0.0341 - class_label_loss: 0.0333 - bounding_box_accuracy: 0.4818 - class_label_accuracy: 0.9904 - val_loss: 0.4138 - val_bounding_box_loss: 0.0146 - val_class_label_loss: 0.3992 - val_bounding_box_accuracy: 0.3942 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 19/30\n",
    "# 30/30 [==============================] - 78s 3s/step - loss: 0.0888 - bounding_box_loss: 0.0323 - class_label_loss: 0.0565 - bounding_box_accuracy: 0.4882 - class_label_accuracy: 0.9818 - val_loss: 0.3948 - val_bounding_box_loss: 0.0149 - val_class_label_loss: 0.3799 - val_bounding_box_accuracy: 0.4327 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 20/30\n",
    "# 30/30 [==============================] - 78s 3s/step - loss: 0.0704 - bounding_box_loss: 0.0307 - class_label_loss: 0.0397 - bounding_box_accuracy: 0.5246 - class_label_accuracy: 0.9872 - val_loss: 0.4502 - val_bounding_box_loss: 0.0156 - val_class_label_loss: 0.4346 - val_bounding_box_accuracy: 0.4038 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 21/30\n",
    "# 30/30 [==============================] - 79s 3s/step - loss: 0.0835 - bounding_box_loss: 0.0310 - class_label_loss: 0.0525 - bounding_box_accuracy: 0.4818 - class_label_accuracy: 0.9776 - val_loss: 0.3405 - val_bounding_box_loss: 0.0138 - val_class_label_loss: 0.3267 - val_bounding_box_accuracy: 0.4904 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 22/30\n",
    "# 30/30 [==============================] - 78s 3s/step - loss: 0.0644 - bounding_box_loss: 0.0318 - class_label_loss: 0.0326 - bounding_box_accuracy: 0.5021 - class_label_accuracy: 0.9915 - val_loss: 0.4512 - val_bounding_box_loss: 0.0137 - val_class_label_loss: 0.4375 - val_bounding_box_accuracy: 0.5769 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 23/30\n",
    "# 30/30 [==============================] - 78s 3s/step - loss: 0.0573 - bounding_box_loss: 0.0280 - class_label_loss: 0.0294 - bounding_box_accuracy: 0.5118 - class_label_accuracy: 0.9893 - val_loss: 0.4533 - val_bounding_box_loss: 0.0147 - val_class_label_loss: 0.4386 - val_bounding_box_accuracy: 0.5192 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 24/30\n",
    "# 30/30 [==============================] - 77s 3s/step - loss: 0.0615 - bounding_box_loss: 0.0296 - class_label_loss: 0.0319 - bounding_box_accuracy: 0.5267 - class_label_accuracy: 0.9904 - val_loss: 0.4332 - val_bounding_box_loss: 0.0139 - val_class_label_loss: 0.4193 - val_bounding_box_accuracy: 0.4038 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 25/30\n",
    "# 30/30 [==============================] - 77s 3s/step - loss: 0.0476 - bounding_box_loss: 0.0297 - class_label_loss: 0.0179 - bounding_box_accuracy: 0.5053 - class_label_accuracy: 0.9936 - val_loss: 0.5128 - val_bounding_box_loss: 0.0139 - val_class_label_loss: 0.4988 - val_bounding_box_accuracy: 0.4519 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 26/30\n",
    "# 30/30 [==============================] - 78s 3s/step - loss: 0.0618 - bounding_box_loss: 0.0295 - class_label_loss: 0.0323 - bounding_box_accuracy: 0.4979 - class_label_accuracy: 0.9893 - val_loss: 0.4797 - val_bounding_box_loss: 0.0137 - val_class_label_loss: 0.4659 - val_bounding_box_accuracy: 0.5769 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 27/30\n",
    "# 30/30 [==============================] - 81s 3s/step - loss: 0.0560 - bounding_box_loss: 0.0295 - class_label_loss: 0.0265 - bounding_box_accuracy: 0.4979 - class_label_accuracy: 0.9915 - val_loss: 0.4655 - val_bounding_box_loss: 0.0143 - val_class_label_loss: 0.4513 - val_bounding_box_accuracy: 0.4423 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 28/30\n",
    "# 30/30 [==============================] - 80s 3s/step - loss: 0.0471 - bounding_box_loss: 0.0299 - class_label_loss: 0.0172 - bounding_box_accuracy: 0.4882 - class_label_accuracy: 0.9904 - val_loss: 0.4911 - val_bounding_box_loss: 0.0139 - val_class_label_loss: 0.4771 - val_bounding_box_accuracy: 0.4423 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 29/30\n",
    "# 30/30 [==============================] - 79s 3s/step - loss: 0.0553 - bounding_box_loss: 0.0264 - class_label_loss: 0.0289 - bounding_box_accuracy: 0.4989 - class_label_accuracy: 0.9915 - val_loss: 0.5264 - val_bounding_box_loss: 0.0145 - val_class_label_loss: 0.5119 - val_bounding_box_accuracy: 0.4904 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 30/30\n",
    "# 30/30 [==============================] - 82s 3s/step - loss: 0.0575 - bounding_box_loss: 0.0275 - class_label_loss: 0.0300 - bounding_box_accuracy: 0.5000 - class_label_accuracy: 0.9882 - val_loss: 0.4550 - val_bounding_box_loss: 0.0139 - val_class_label_loss: 0.4411 - val_bounding_box_accuracy: 0.5865 - val_class_label_accuracy: 0.9327"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting loss for modeling training\n",
    "plot_loss(model_name='xception_model_v2', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training total loss during xception_model_v2 training: 0.0574774332344532\n",
    "# Final epoch training class label loss during xception_model_v2 training: 0.029951905831694603\n",
    "# Final epoch training bounding box loss during xception_model_v2 training: 0.02752552554011345\n",
    "# Final epoch validation total loss during xception_model_v2 training: 0.45502156019210815\n",
    "# Final epoch validation class label loss during xception_model_v2 training: 0.4411075711250305\n",
    "# Final epoch validation bounding box loss during xception_model_v2 training: 0.013913992792367935"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting accuracy for modeling training\n",
    "plot_accuracy(model_name='xception_model_v2', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training class label accurracy during xception_model_v2 training: 0.9882478713989258\n",
    "# Final epoch validation class label accurracy during xception_model_v2 training: 0.932692289352417"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating model for mAP metrics and inference time and storing result\n",
    "result_val = evaluate_val(model=xception_model_v2, model_name='xception_model_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC PASCAL mAP for xception_model_v2 validation data: 0.12991276383399963\n",
    "# VOC PASCAL mAP in all points for xception_model_v2 validation data: 0.09690123796463013\n",
    "# COCO mAP for xception_model_v2 validation data: 0.33803850412368774\n",
    "# Average inference time for xception_model_v2 validation data: 0.1181832620730767"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending results of model evaluation to results dataframe (df_results)\n",
    "df_results_val = df_results_val.append(result_val, ignore_index=True)\n",
    "df_results_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading network but leaving off output layers\n",
    "vgg16 = VGG16(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "# freezing all VGG layers during training so they are not updated\n",
    "vgg16.trainable = False\n",
    "\n",
    "# flattening the max-pooling output of network\n",
    "flatten = vgg16.output\n",
    "flat = Flatten()(flatten)\n",
    "\n",
    "# constructing an output layer to predict bounding box coordinates\n",
    "bbox_head1 = Dense(128, activation=\"relu\")(flat)\n",
    "bbox_head2 = Dense(64, activation=\"relu\")(bbox_head1)\n",
    "bbox_head3 = Dense(32, activation=\"relu\")(bbox_head2)\n",
    "bbox_head_v1 = Dense(4, activation=\"sigmoid\", name=\"bounding_box\")(bbox_head3)\n",
    "\n",
    "# constructing an output layer to predict class labels\n",
    "class_head1 = Dense(512, activation=\"relu\")(flat)\n",
    "class_head2 = Dropout(0.5)(class_head1)\n",
    "class_head3 = Dense(512, activation=\"relu\")(class_head2)\n",
    "class_head4 = Dropout(0.5)(class_head3)\n",
    "class_head_v1 = Dense(2, activation=\"sigmoid\", name=\"class_label\")(class_head4)\n",
    "\n",
    "# creating a model that accepts an input image and outputs bounding box coordinates and a class label\n",
    "vgg16_model_v1 = Model(inputs=vgg16.input, outputs=(bbox_head_v1, class_head_v1))\n",
    "\n",
    "# creating dictionary for setting loss functions\n",
    "losses = {\"class_label\": \"binary_crossentropy\", \"bounding_box\": \"mean_squared_error\"}\n",
    "\n",
    "# creating dictionry for weighting loss functions\n",
    "loss_weights = {\"class_label\": 1.0, \"bounding_box\": 1.0}\n",
    "\n",
    "# initilizing an \"Adam\" optimizer with learning rate of 0.0001\n",
    "opt = Adam(learning_rate = 0.0001)\n",
    "\n",
    "# compiling model\n",
    "vgg16_model_v1.compile(loss=losses, optimizer=opt, metrics=[\"accuracy\"], loss_weights=loss_weights)\n",
    "print(vgg16_model_v1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing a dictionary for our training outputs\n",
    "train_targets = {\"class_label\": labels_train, \"bounding_box\": bboxes_train}\n",
    "\n",
    "# constructing a dictionary for our test outputs\n",
    "val_targets = {\"class_label\": labels_val, \"bounding_box\": bboxes_val}\n",
    "\n",
    "# training model for class and bounding box predictions\n",
    "H = vgg16_model_v1.fit(x_train, train_targets, validation_data=(x_val, val_targets), batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "\n",
    "# saving trained model\n",
    "vgg16_model_v1.save('../Models/vgg16_model_v1.h5', save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 1/30\n",
    "# 30/30 [==============================] - 157s 5s/step - loss: 0.3568 - bounding_box_loss: 0.0246 - class_label_loss: 0.3322 - bounding_box_accuracy: 0.6132 - class_label_accuracy: 0.8632 - val_loss: 0.2370 - val_bounding_box_loss: 0.0132 - val_class_label_loss: 0.2238 - val_bounding_box_accuracy: 0.4519 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 2/30\n",
    "# 30/30 [==============================] - 162s 5s/step - loss: 0.2269 - bounding_box_loss: 0.0118 - class_label_loss: 0.2150 - bounding_box_accuracy: 0.6528 - class_label_accuracy: 0.9348 - val_loss: 0.2596 - val_bounding_box_loss: 0.0108 - val_class_label_loss: 0.2488 - val_bounding_box_accuracy: 0.6827 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 3/30\n",
    "# 30/30 [==============================] - 166s 6s/step - loss: 0.1725 - bounding_box_loss: 0.0093 - class_label_loss: 0.1632 - bounding_box_accuracy: 0.6923 - class_label_accuracy: 0.9583 - val_loss: 0.2410 - val_bounding_box_loss: 0.0096 - val_class_label_loss: 0.2314 - val_bounding_box_accuracy: 0.7500 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 4/30\n",
    "# 30/30 [==============================] - 167s 6s/step - loss: 0.1714 - bounding_box_loss: 0.0077 - class_label_loss: 0.1637 - bounding_box_accuracy: 0.7436 - class_label_accuracy: 0.9562 - val_loss: 0.2611 - val_bounding_box_loss: 0.0094 - val_class_label_loss: 0.2517 - val_bounding_box_accuracy: 0.6731 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 5/30\n",
    "# 30/30 [==============================] - 168s 6s/step - loss: 0.1549 - bounding_box_loss: 0.0066 - class_label_loss: 0.1483 - bounding_box_accuracy: 0.7244 - class_label_accuracy: 0.9658 - val_loss: 0.2345 - val_bounding_box_loss: 0.0081 - val_class_label_loss: 0.2263 - val_bounding_box_accuracy: 0.7212 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 6/30\n",
    "# 30/30 [==============================] - 169s 6s/step - loss: 0.1296 - bounding_box_loss: 0.0057 - class_label_loss: 0.1238 - bounding_box_accuracy: 0.7639 - class_label_accuracy: 0.9679 - val_loss: 0.2217 - val_bounding_box_loss: 0.0079 - val_class_label_loss: 0.2138 - val_bounding_box_accuracy: 0.7692 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 7/30\n",
    "# 30/30 [==============================] - 170s 6s/step - loss: 0.1152 - bounding_box_loss: 0.0052 - class_label_loss: 0.1100 - bounding_box_accuracy: 0.7831 - class_label_accuracy: 0.9701 - val_loss: 0.2418 - val_bounding_box_loss: 0.0077 - val_class_label_loss: 0.2341 - val_bounding_box_accuracy: 0.7212 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 8/30\n",
    "# 30/30 [==============================] - 170s 6s/step - loss: 0.1030 - bounding_box_loss: 0.0045 - class_label_loss: 0.0984 - bounding_box_accuracy: 0.8098 - class_label_accuracy: 0.9744 - val_loss: 0.3015 - val_bounding_box_loss: 0.0075 - val_class_label_loss: 0.2940 - val_bounding_box_accuracy: 0.7692 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 9/30\n",
    "# 30/30 [==============================] - 171s 6s/step - loss: 0.0951 - bounding_box_loss: 0.0042 - class_label_loss: 0.0910 - bounding_box_accuracy: 0.8098 - class_label_accuracy: 0.9744 - val_loss: 0.2535 - val_bounding_box_loss: 0.0077 - val_class_label_loss: 0.2458 - val_bounding_box_accuracy: 0.7788 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 10/30\n",
    "# 30/30 [==============================] - 171s 6s/step - loss: 0.0833 - bounding_box_loss: 0.0037 - class_label_loss: 0.0795 - bounding_box_accuracy: 0.8280 - class_label_accuracy: 0.9786 - val_loss: 0.2729 - val_bounding_box_loss: 0.0073 - val_class_label_loss: 0.2656 - val_bounding_box_accuracy: 0.7404 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 11/30\n",
    "# 30/30 [==============================] - 171s 6s/step - loss: 0.0874 - bounding_box_loss: 0.0036 - class_label_loss: 0.0838 - bounding_box_accuracy: 0.8494 - class_label_accuracy: 0.9744 - val_loss: 0.2531 - val_bounding_box_loss: 0.0071 - val_class_label_loss: 0.2461 - val_bounding_box_accuracy: 0.7692 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 12/30\n",
    "# 30/30 [==============================] - 171s 6s/step - loss: 0.0717 - bounding_box_loss: 0.0032 - class_label_loss: 0.0684 - bounding_box_accuracy: 0.8387 - class_label_accuracy: 0.9786 - val_loss: 0.2819 - val_bounding_box_loss: 0.0070 - val_class_label_loss: 0.2749 - val_bounding_box_accuracy: 0.7692 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 13/30\n",
    "# 30/30 [==============================] - 172s 6s/step - loss: 0.0667 - bounding_box_loss: 0.0033 - class_label_loss: 0.0634 - bounding_box_accuracy: 0.8194 - class_label_accuracy: 0.9797 - val_loss: 0.3182 - val_bounding_box_loss: 0.0073 - val_class_label_loss: 0.3109 - val_bounding_box_accuracy: 0.7500 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 14/30\n",
    "# 30/30 [==============================] - 172s 6s/step - loss: 0.0518 - bounding_box_loss: 0.0029 - class_label_loss: 0.0489 - bounding_box_accuracy: 0.8355 - class_label_accuracy: 0.9850 - val_loss: 0.3615 - val_bounding_box_loss: 0.0073 - val_class_label_loss: 0.3542 - val_bounding_box_accuracy: 0.8077 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 15/30\n",
    "# 30/30 [==============================] - 172s 6s/step - loss: 0.0596 - bounding_box_loss: 0.0025 - class_label_loss: 0.0571 - bounding_box_accuracy: 0.8697 - class_label_accuracy: 0.9786 - val_loss: 0.3212 - val_bounding_box_loss: 0.0074 - val_class_label_loss: 0.3138 - val_bounding_box_accuracy: 0.7788 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 16/30\n",
    "# 30/30 [==============================] - 172s 6s/step - loss: 0.0568 - bounding_box_loss: 0.0025 - class_label_loss: 0.0543 - bounding_box_accuracy: 0.8729 - class_label_accuracy: 0.9818 - val_loss: 0.3964 - val_bounding_box_loss: 0.0070 - val_class_label_loss: 0.3894 - val_bounding_box_accuracy: 0.7692 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 17/30\n",
    "# 30/30 [==============================] - 173s 6s/step - loss: 0.0507 - bounding_box_loss: 0.0023 - class_label_loss: 0.0484 - bounding_box_accuracy: 0.8462 - class_label_accuracy: 0.9829 - val_loss: 0.3326 - val_bounding_box_loss: 0.0069 - val_class_label_loss: 0.3257 - val_bounding_box_accuracy: 0.8173 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 18/30\n",
    "# 30/30 [==============================] - 172s 6s/step - loss: 0.0487 - bounding_box_loss: 0.0022 - class_label_loss: 0.0465 - bounding_box_accuracy: 0.8761 - class_label_accuracy: 0.9840 - val_loss: 0.3543 - val_bounding_box_loss: 0.0072 - val_class_label_loss: 0.3471 - val_bounding_box_accuracy: 0.7885 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 19/30\n",
    "# 30/30 [==============================] - 172s 6s/step - loss: 0.0446 - bounding_box_loss: 0.0021 - class_label_loss: 0.0425 - bounding_box_accuracy: 0.8771 - class_label_accuracy: 0.9850 - val_loss: 0.4190 - val_bounding_box_loss: 0.0068 - val_class_label_loss: 0.4122 - val_bounding_box_accuracy: 0.8173 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 20/30\n",
    "# 30/30 [==============================] - 173s 6s/step - loss: 0.0491 - bounding_box_loss: 0.0019 - class_label_loss: 0.0472 - bounding_box_accuracy: 0.8900 - class_label_accuracy: 0.9872 - val_loss: 0.3751 - val_bounding_box_loss: 0.0068 - val_class_label_loss: 0.3683 - val_bounding_box_accuracy: 0.7788 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 21/30\n",
    "# 30/30 [==============================] - 172s 6s/step - loss: 0.0379 - bounding_box_loss: 0.0018 - class_label_loss: 0.0361 - bounding_box_accuracy: 0.8942 - class_label_accuracy: 0.9861 - val_loss: 0.3738 - val_bounding_box_loss: 0.0069 - val_class_label_loss: 0.3669 - val_bounding_box_accuracy: 0.8077 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 22/30\n",
    "# 30/30 [==============================] - 173s 6s/step - loss: 0.0365 - bounding_box_loss: 0.0019 - class_label_loss: 0.0346 - bounding_box_accuracy: 0.8868 - class_label_accuracy: 0.9904 - val_loss: 0.4203 - val_bounding_box_loss: 0.0069 - val_class_label_loss: 0.4135 - val_bounding_box_accuracy: 0.8077 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 23/30\n",
    "# 30/30 [==============================] - 173s 6s/step - loss: 0.0422 - bounding_box_loss: 0.0017 - class_label_loss: 0.0405 - bounding_box_accuracy: 0.8718 - class_label_accuracy: 0.9850 - val_loss: 0.4563 - val_bounding_box_loss: 0.0067 - val_class_label_loss: 0.4496 - val_bounding_box_accuracy: 0.8077 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 24/30\n",
    "# 30/30 [==============================] - 174s 6s/step - loss: 0.0410 - bounding_box_loss: 0.0016 - class_label_loss: 0.0394 - bounding_box_accuracy: 0.8878 - class_label_accuracy: 0.9861 - val_loss: 0.4247 - val_bounding_box_loss: 0.0075 - val_class_label_loss: 0.4172 - val_bounding_box_accuracy: 0.7692 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 25/30\n",
    "# 30/30 [==============================] - 173s 6s/step - loss: 0.0404 - bounding_box_loss: 0.0016 - class_label_loss: 0.0389 - bounding_box_accuracy: 0.9017 - class_label_accuracy: 0.9861 - val_loss: 0.4501 - val_bounding_box_loss: 0.0069 - val_class_label_loss: 0.4432 - val_bounding_box_accuracy: 0.7885 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 26/30\n",
    "# 30/30 [==============================] - 173s 6s/step - loss: 0.0371 - bounding_box_loss: 0.0015 - class_label_loss: 0.0356 - bounding_box_accuracy: 0.9124 - class_label_accuracy: 0.9872 - val_loss: 0.5205 - val_bounding_box_loss: 0.0069 - val_class_label_loss: 0.5136 - val_bounding_box_accuracy: 0.7981 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 27/30\n",
    "# 30/30 [==============================] - 176s 6s/step - loss: 0.0374 - bounding_box_loss: 0.0014 - class_label_loss: 0.0360 - bounding_box_accuracy: 0.9209 - class_label_accuracy: 0.9872 - val_loss: 0.4459 - val_bounding_box_loss: 0.0070 - val_class_label_loss: 0.4389 - val_bounding_box_accuracy: 0.8269 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 28/30\n",
    "# 30/30 [==============================] - 173s 6s/step - loss: 0.0335 - bounding_box_loss: 0.0013 - class_label_loss: 0.0321 - bounding_box_accuracy: 0.9006 - class_label_accuracy: 0.9872 - val_loss: 0.4561 - val_bounding_box_loss: 0.0070 - val_class_label_loss: 0.4491 - val_bounding_box_accuracy: 0.7981 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 29/30\n",
    "# 30/30 [==============================] - 173s 6s/step - loss: 0.0348 - bounding_box_loss: 0.0016 - class_label_loss: 0.0332 - bounding_box_accuracy: 0.9081 - class_label_accuracy: 0.9861 - val_loss: 0.4558 - val_bounding_box_loss: 0.0081 - val_class_label_loss: 0.4478 - val_bounding_box_accuracy: 0.7788 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 30/30\n",
    "# 30/30 [==============================] - 171s 6s/step - loss: 0.0376 - bounding_box_loss: 0.0014 - class_label_loss: 0.0362 - bounding_box_accuracy: 0.9209 - class_label_accuracy: 0.9808 - val_loss: 0.4495 - val_bounding_box_loss: 0.0071 - val_class_label_loss: 0.4424 - val_bounding_box_accuracy: 0.7885 - val_class_label_accuracy: 0.9231"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting loss for modeling training\n",
    "plot_loss(model_name='vgg16_model_v1', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training total loss during vgg16_model_v1 training: 0.03759124130010605\n",
    "# Final epoch training class label loss during vgg16_model_v1 training: 0.036184925585985184\n",
    "# Final epoch training bounding box loss during vgg16_model_v1 training: 0.0014063188573345542\n",
    "# Final epoch validation total loss during vgg16_model_v1 training: 0.4494522511959076\n",
    "# Final epoch validation class label loss during vgg16_model_v1 training: 0.4423692524433136\n",
    "# Final epoch validation bounding box loss during vgg16_model_v1 training: 0.007083018310368061"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting accuracy for modeling training\n",
    "plot_accuracy(model_name='vgg16_model_v1', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training class label accurracy during vgg16_model_v1 training: 0.9807692170143127\n",
    "# Final epoch validation class label accurracy during vgg16_model_v1 training: 0.9230769276618958"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating model for mAP metrics and inference time and storing result\n",
    "result_val = evaluate_val(model=vgg16_model_v1, model_name='vgg16_model_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC PASCAL mAP for vgg16_model_v1 validation data: 0.47773295640945435\n",
    "# VOC PASCAL mAP in all points for vgg16_model_v1 validation data: 0.44967031478881836\n",
    "# COCO mAP for vgg16_model_v1 validation data: 0.5303829312324524\n",
    "# Average inference time for vgg16_model_v1 validation data: 0.16057342749375564"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending results of model evaluation to results dataframe (df_results)\n",
    "df_results_val = df_results_val.append(result_val, ignore_index=True)\n",
    "df_results_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading network but leaving off output layers\n",
    "vgg16 = VGG16(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "# freezing all VGG layers during training so they are not updated\n",
    "vgg16.trainable = False\n",
    "\n",
    "# flattening the max-pooling output of network\n",
    "flatten = vgg16.output\n",
    "flat = Flatten()(flatten)\n",
    "\n",
    "# constructing an output layer to predict bounding box coordinates\n",
    "bbox_head1 = Dense(128, activation=\"relu\")(flat)\n",
    "bbox_head2 = Dropout(0.5)(bbox_head1)\n",
    "bbox_head3 = Dense(64, activation=\"relu\")(bbox_head2)\n",
    "bbox_head4 = Dropout(0.5)(bbox_head3)\n",
    "bbox_head5 = Dense(32, activation=\"relu\")(bbox_head4)\n",
    "bbox_head6 = Dropout(0.5)(bbox_head5)\n",
    "bbox_head_v2 = Dense(4, activation=\"sigmoid\", name=\"bounding_box\")(bbox_head6)\n",
    "\n",
    "# constructing an output layer to predict class labels\n",
    "class_head1 = Dense(512, activation=\"relu\")(flat)\n",
    "class_head2 = Dropout(0.5)(class_head1)\n",
    "class_head3 = Dense(512, activation=\"relu\")(class_head2)\n",
    "class_head4 = Dropout(0.5)(class_head3)\n",
    "class_head5 = Dense(512, activation=\"relu\")(class_head4)\n",
    "class_head6 = Dropout(0.5)(class_head5)\n",
    "class_head_v2 = Dense(2, activation=\"sigmoid\", name=\"class_label\")(class_head6)\n",
    "\n",
    "# creating a model that accepts an input image and outputs bounding box coordinates and a class label\n",
    "vgg16_model_v2 = Model(inputs=vgg16.input, outputs=(bbox_head_v2, class_head_v2))\n",
    "\n",
    "# creating dictionary for setting loss functions\n",
    "losses = {\"class_label\": \"binary_crossentropy\", \"bounding_box\": \"mean_squared_error\"}\n",
    "\n",
    "# creating dictionry for weighting loss functions\n",
    "loss_weights = {\"class_label\": 1.0, \"bounding_box\": 1.0}\n",
    "\n",
    "# initilizing an \"Adam\" optimizer with learning rate of 0.0001\n",
    "opt = Adam(learning_rate = 0.0001)\n",
    "\n",
    "# compiling model\n",
    "vgg16_model_v2.compile(loss=losses, optimizer=opt, metrics=[\"accuracy\"], loss_weights=loss_weights)\n",
    "print(vgg16_model_v2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing a dictionary for our training outputs\n",
    "train_targets = {\"class_label\": labels_train, \"bounding_box\": bboxes_train}\n",
    "\n",
    "# constructing a dictionary for our test outputs\n",
    "val_targets = {\"class_label\": labels_val, \"bounding_box\": bboxes_val}\n",
    "\n",
    "# training model for class and bounding box predictions\n",
    "H = vgg16_model_v2.fit(x_train, train_targets, validation_data=(x_val, val_targets), batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "\n",
    "# saving trained model\n",
    "vgg16_model_v2.save('../Models/vgg16_model_v2.h5', save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 1/30\n",
    "# 30/30 [==============================] - 164s 5s/step - loss: 0.5964 - bounding_box_loss: 0.1341 - class_label_loss: 0.4623 - bounding_box_accuracy: 0.3483 - class_label_accuracy: 0.8301 - val_loss: 0.2669 - val_bounding_box_loss: 0.0641 - val_class_label_loss: 0.2028 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 2/30\n",
    "# 30/30 [==============================] - 168s 6s/step - loss: 0.3433 - bounding_box_loss: 0.1074 - class_label_loss: 0.2359 - bounding_box_accuracy: 0.3462 - class_label_accuracy: 0.9284 - val_loss: 0.2901 - val_bounding_box_loss: 0.0641 - val_class_label_loss: 0.2260 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 3/30\n",
    "# 30/30 [==============================] - 180s 6s/step - loss: 0.3014 - bounding_box_loss: 0.1002 - class_label_loss: 0.2013 - bounding_box_accuracy: 0.3675 - class_label_accuracy: 0.9348 - val_loss: 0.2995 - val_bounding_box_loss: 0.0561 - val_class_label_loss: 0.2433 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 4/30\n",
    "# 30/30 [==============================] - 177s 6s/step - loss: 0.2904 - bounding_box_loss: 0.0922 - class_label_loss: 0.1981 - bounding_box_accuracy: 0.3964 - class_label_accuracy: 0.9562 - val_loss: 0.2885 - val_bounding_box_loss: 0.0481 - val_class_label_loss: 0.2404 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 5/30\n",
    "# 30/30 [==============================] - 179s 6s/step - loss: 0.2512 - bounding_box_loss: 0.0847 - class_label_loss: 0.1664 - bounding_box_accuracy: 0.4220 - class_label_accuracy: 0.9541 - val_loss: 0.3062 - val_bounding_box_loss: 0.0378 - val_class_label_loss: 0.2684 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 6/30\n",
    "# 30/30 [==============================] - 177s 6s/step - loss: 0.2292 - bounding_box_loss: 0.0757 - class_label_loss: 0.1535 - bounding_box_accuracy: 0.4124 - class_label_accuracy: 0.9658 - val_loss: 0.2836 - val_bounding_box_loss: 0.0347 - val_class_label_loss: 0.2490 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 7/30\n",
    "# 30/30 [==============================] - 179s 6s/step - loss: 0.2002 - bounding_box_loss: 0.0718 - class_label_loss: 0.1284 - bounding_box_accuracy: 0.4220 - class_label_accuracy: 0.9658 - val_loss: 0.2624 - val_bounding_box_loss: 0.0264 - val_class_label_loss: 0.2360 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 8/30\n",
    "# 30/30 [==============================] - 180s 6s/step - loss: 0.1779 - bounding_box_loss: 0.0662 - class_label_loss: 0.1117 - bounding_box_accuracy: 0.4295 - class_label_accuracy: 0.9733 - val_loss: 0.2777 - val_bounding_box_loss: 0.0246 - val_class_label_loss: 0.2532 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 9/30\n",
    "# 30/30 [==============================] - 169s 6s/step - loss: 0.1726 - bounding_box_loss: 0.0598 - class_label_loss: 0.1128 - bounding_box_accuracy: 0.4444 - class_label_accuracy: 0.9733 - val_loss: 0.2777 - val_bounding_box_loss: 0.0183 - val_class_label_loss: 0.2594 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 10/30\n",
    "# 30/30 [==============================] - 173s 6s/step - loss: 0.1577 - bounding_box_loss: 0.0538 - class_label_loss: 0.1039 - bounding_box_accuracy: 0.4412 - class_label_accuracy: 0.9733 - val_loss: 0.2980 - val_bounding_box_loss: 0.0173 - val_class_label_loss: 0.2808 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 11/30\n",
    "# 30/30 [==============================] - 198s 7s/step - loss: 0.1467 - bounding_box_loss: 0.0509 - class_label_loss: 0.0959 - bounding_box_accuracy: 0.4797 - class_label_accuracy: 0.9744 - val_loss: 0.2588 - val_bounding_box_loss: 0.0153 - val_class_label_loss: 0.2434 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 12/30\n",
    "# 30/30 [==============================] - 206s 7s/step - loss: 0.1243 - bounding_box_loss: 0.0478 - class_label_loss: 0.0764 - bounding_box_accuracy: 0.4487 - class_label_accuracy: 0.9776 - val_loss: 0.3620 - val_bounding_box_loss: 0.0155 - val_class_label_loss: 0.3465 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 13/30\n",
    "# 30/30 [==============================] - 202s 7s/step - loss: 0.1281 - bounding_box_loss: 0.0492 - class_label_loss: 0.0789 - bounding_box_accuracy: 0.4455 - class_label_accuracy: 0.9776 - val_loss: 0.2839 - val_bounding_box_loss: 0.0142 - val_class_label_loss: 0.2698 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 14/30\n",
    "# 30/30 [==============================] - 200s 7s/step - loss: 0.1160 - bounding_box_loss: 0.0430 - class_label_loss: 0.0730 - bounding_box_accuracy: 0.4615 - class_label_accuracy: 0.9818 - val_loss: 0.3526 - val_bounding_box_loss: 0.0139 - val_class_label_loss: 0.3387 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 15/30\n",
    "# 30/30 [==============================] - 199s 7s/step - loss: 0.1051 - bounding_box_loss: 0.0439 - class_label_loss: 0.0612 - bounding_box_accuracy: 0.4733 - class_label_accuracy: 0.9818 - val_loss: 0.3352 - val_bounding_box_loss: 0.0129 - val_class_label_loss: 0.3224 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 16/30\n",
    "# 30/30 [==============================] - 199s 7s/step - loss: 0.0931 - bounding_box_loss: 0.0387 - class_label_loss: 0.0543 - bounding_box_accuracy: 0.4594 - class_label_accuracy: 0.9861 - val_loss: 0.3109 - val_bounding_box_loss: 0.0119 - val_class_label_loss: 0.2990 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 17/30\n",
    "# 30/30 [==============================] - 182s 6s/step - loss: 0.0922 - bounding_box_loss: 0.0386 - class_label_loss: 0.0536 - bounding_box_accuracy: 0.4487 - class_label_accuracy: 0.9797 - val_loss: 0.3378 - val_bounding_box_loss: 0.0118 - val_class_label_loss: 0.3260 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 18/30\n",
    "# 30/30 [==============================] - 185s 6s/step - loss: 0.0911 - bounding_box_loss: 0.0345 - class_label_loss: 0.0566 - bounding_box_accuracy: 0.4669 - class_label_accuracy: 0.9797 - val_loss: 0.3934 - val_bounding_box_loss: 0.0117 - val_class_label_loss: 0.3817 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 19/30\n",
    "# 30/30 [==============================] - 184s 6s/step - loss: 0.0810 - bounding_box_loss: 0.0364 - class_label_loss: 0.0445 - bounding_box_accuracy: 0.4423 - class_label_accuracy: 0.9818 - val_loss: 0.4196 - val_bounding_box_loss: 0.0119 - val_class_label_loss: 0.4077 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 20/30\n",
    "# 30/30 [==============================] - 186s 6s/step - loss: 0.0959 - bounding_box_loss: 0.0362 - class_label_loss: 0.0597 - bounding_box_accuracy: 0.4487 - class_label_accuracy: 0.9818 - val_loss: 0.4046 - val_bounding_box_loss: 0.0116 - val_class_label_loss: 0.3931 - val_bounding_box_accuracy: 0.3942 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 21/30\n",
    "# 30/30 [==============================] - 189s 6s/step - loss: 0.0807 - bounding_box_loss: 0.0334 - class_label_loss: 0.0473 - bounding_box_accuracy: 0.4509 - class_label_accuracy: 0.9818 - val_loss: 0.3574 - val_bounding_box_loss: 0.0116 - val_class_label_loss: 0.3457 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 22/30\n",
    "# 30/30 [==============================] - 183s 6s/step - loss: 0.0867 - bounding_box_loss: 0.0337 - class_label_loss: 0.0530 - bounding_box_accuracy: 0.4733 - class_label_accuracy: 0.9797 - val_loss: 0.4059 - val_bounding_box_loss: 0.0116 - val_class_label_loss: 0.3943 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 23/30\n",
    "# 30/30 [==============================] - 187s 6s/step - loss: 0.0778 - bounding_box_loss: 0.0319 - class_label_loss: 0.0459 - bounding_box_accuracy: 0.4498 - class_label_accuracy: 0.9818 - val_loss: 0.4592 - val_bounding_box_loss: 0.0114 - val_class_label_loss: 0.4478 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 24/30\n",
    "# 30/30 [==============================] - 231s 8s/step - loss: 0.0758 - bounding_box_loss: 0.0299 - class_label_loss: 0.0458 - bounding_box_accuracy: 0.4808 - class_label_accuracy: 0.9850 - val_loss: 0.4119 - val_bounding_box_loss: 0.0120 - val_class_label_loss: 0.3999 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 25/30\n",
    "# 30/30 [==============================] - 194s 6s/step - loss: 0.0690 - bounding_box_loss: 0.0296 - class_label_loss: 0.0394 - bounding_box_accuracy: 0.4712 - class_label_accuracy: 0.9829 - val_loss: 0.4529 - val_bounding_box_loss: 0.0116 - val_class_label_loss: 0.4413 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 26/30\n",
    "# 30/30 [==============================] - 263s 9s/step - loss: 0.0640 - bounding_box_loss: 0.0286 - class_label_loss: 0.0353 - bounding_box_accuracy: 0.4679 - class_label_accuracy: 0.9872 - val_loss: 0.4200 - val_bounding_box_loss: 0.0113 - val_class_label_loss: 0.4087 - val_bounding_box_accuracy: 0.3942 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 27/30\n",
    "# 30/30 [==============================] - 313s 10s/step - loss: 0.0713 - bounding_box_loss: 0.0294 - class_label_loss: 0.0419 - bounding_box_accuracy: 0.4679 - class_label_accuracy: 0.9818 - val_loss: 0.4324 - val_bounding_box_loss: 0.0115 - val_class_label_loss: 0.4210 - val_bounding_box_accuracy: 0.3846 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 28/30\n",
    "# 30/30 [==============================] - 336s 11s/step - loss: 0.0737 - bounding_box_loss: 0.0279 - class_label_loss: 0.0457 - bounding_box_accuracy: 0.4797 - class_label_accuracy: 0.9850 - val_loss: 0.4481 - val_bounding_box_loss: 0.0135 - val_class_label_loss: 0.4346 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 29/30\n",
    "# 30/30 [==============================] - 192s 6s/step - loss: 0.0672 - bounding_box_loss: 0.0269 - class_label_loss: 0.0404 - bounding_box_accuracy: 0.4594 - class_label_accuracy: 0.9861 - val_loss: 0.5334 - val_bounding_box_loss: 0.0116 - val_class_label_loss: 0.5218 - val_bounding_box_accuracy: 0.3942 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 30/30\n",
    "# 30/30 [==============================] - 207s 7s/step - loss: 0.0630 - bounding_box_loss: 0.0276 - class_label_loss: 0.0353 - bounding_box_accuracy: 0.4615 - class_label_accuracy: 0.9829 - val_loss: 0.5416 - val_bounding_box_loss: 0.0113 - val_class_label_loss: 0.5303 - val_bounding_box_accuracy: 0.4712 - val_class_label_accuracy: 0.9231"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting loss for modeling training\n",
    "plot_loss(model_name='vgg16_model_v2', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training total loss during vgg16_model_v2 training: 0.06295057386159897\n",
    "# Final epoch training class label loss during vgg16_model_v2 training: 0.03531641140580177\n",
    "# Final epoch training bounding box loss during vgg16_model_v2 training: 0.027634164318442345\n",
    "# Final epoch validation total loss during vgg16_model_v2 training: 0.5416238307952881\n",
    "# Final epoch validation class label loss during vgg16_model_v2 training: 0.5302935242652893\n",
    "# Final epoch validation bounding box loss during vgg16_model_v2 training: 0.011330231092870235"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting accuracy for modeling training\n",
    "plot_accuracy(model_name='vgg16_model_v2', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training class label accurracy during vgg16_model_v2 training: 0.9829059839248657\n",
    "# Final epoch validation class label accurracy during vgg16_model_v2 training: 0.9230769276618958"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating model for mAP metrics and inference time and storing result\n",
    "result_val = evaluate_val(model=vgg16_model_v2, model_name='vgg16_model_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC PASCAL mAP for vgg16_model_v2 validation data: 0.14338842034339905\n",
    "# VOC PASCAL mAP in all points for vgg16_model_v2 validation data: 0.11888428032398224\n",
    "# COCO mAP for vgg16_model_v2 validation data: 0.41136860847473145\n",
    "# Average inference time for vgg16_model_v2 validation data: 0.1447364596220163"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending results of model evaluation to results dataframe (df_results)\n",
    "df_results_val = df_results_val.append(result_val, ignore_index=True)\n",
    "df_results_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading network but leaving off output layers\n",
    "vgg16 = VGG16(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "# freezing all backbone layers during training so they are not updated\n",
    "vgg16.trainable = False\n",
    "\n",
    "# flattening the max-pooling output of network\n",
    "flatten = vgg16.output\n",
    "flat = Flatten()(flatten)\n",
    "\n",
    "# constructing an output layer to predict bounding box coordinates\n",
    "bbox_head1 = Dense(128, activation=\"relu\")(flat)\n",
    "bbox_head2 = Dropout(0.5)(bbox_head1)\n",
    "bbox_head3 = Dense(64, activation=\"relu\")(bbox_head2)\n",
    "bbox_head4 = Dense(32, activation=\"relu\")(bbox_head3)\n",
    "bbox_head_v3 = Dense(4, activation=\"sigmoid\", name=\"bounding_box\")(bbox_head4)\n",
    "\n",
    "# constructing an output layer to predict class labels\n",
    "class_head1 = Dense(512, activation=\"relu\")(flat)\n",
    "class_head2 = Dropout(0.5)(class_head1)\n",
    "class_head3 = Dense(512, activation=\"relu\")(class_head2)\n",
    "class_head4 = Dropout(0.5)(class_head3)\n",
    "class_head5 = Dense(512, activation=\"relu\")(class_head4)\n",
    "class_head6 = Dropout(0.5)(class_head5)\n",
    "class_head7 = Dense(512, activation=\"relu\")(class_head6)\n",
    "class_head8 = Dropout(0.5)(class_head7)\n",
    "class_head_v3 = Dense(2, activation=\"sigmoid\", name=\"class_label\")(class_head8)\n",
    "\n",
    "# creating a model that accepts an input image and outputs bounding box coordinates and a class label\n",
    "vgg16_model_v3 = Model(inputs=vgg16.input, outputs=(bbox_head_v3, class_head_v3))\n",
    "\n",
    "# creating dictionary for setting loss functions\n",
    "losses = {\"class_label\": \"binary_crossentropy\", \"bounding_box\": \"mean_squared_error\"}\n",
    "\n",
    "# creating dictionry for weighting loss functions\n",
    "loss_weights = {\"class_label\": 1.0, \"bounding_box\": 1.0}\n",
    "\n",
    "# initilizing an \"Adam\" optimizer with learning rate of 0.0001\n",
    "opt = Adam(learning_rate = 0.0001)\n",
    "\n",
    "# compiling model\n",
    "vgg16_model_v3.compile(loss=losses, optimizer=opt, metrics=[\"accuracy\"], loss_weights=loss_weights)\n",
    "print(vgg16_model_v3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing a dictionary for our training outputs\n",
    "train_targets = {\"class_label\": labels_train, \"bounding_box\": bboxes_train}\n",
    "\n",
    "# constructing a dictionary for our test outputs\n",
    "val_targets = {\"class_label\": labels_val, \"bounding_box\": bboxes_val}\n",
    "\n",
    "# training model for class and bounding box predictions\n",
    "H = vgg16_model_v3.fit(x_train, train_targets, validation_data=(x_val, val_targets), batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "\n",
    "# saving trained model\n",
    "vgg16_model_v3.save('../Models/vgg16_model_v3.h5', save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 1/30\n",
    "# 30/30 [==============================] - 153s 5s/step - loss: 0.7241 - bounding_box_loss: 0.0358 - class_label_loss: 0.6883 - bounding_box_accuracy: 0.5534 - class_label_accuracy: 0.6464 - val_loss: 0.3631 - val_bounding_box_loss: 0.0206 - val_class_label_loss: 0.3425 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9135\n",
    "# Epoch 2/30\n",
    "# 30/30 [==============================] - 153s 5s/step - loss: 0.4188 - bounding_box_loss: 0.0208 - class_label_loss: 0.3980 - bounding_box_accuracy: 0.4979 - class_label_accuracy: 0.8590 - val_loss: 0.2237 - val_bounding_box_loss: 0.0138 - val_class_label_loss: 0.2099 - val_bounding_box_accuracy: 0.4615 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 3/30\n",
    "# 30/30 [==============================] - 151s 5s/step - loss: 0.2613 - bounding_box_loss: 0.0174 - class_label_loss: 0.2439 - bounding_box_accuracy: 0.5192 - class_label_accuracy: 0.9327 - val_loss: 0.2309 - val_bounding_box_loss: 0.0126 - val_class_label_loss: 0.2182 - val_bounding_box_accuracy: 0.3654 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 4/30\n",
    "# 30/30 [==============================] - 152s 5s/step - loss: 0.2485 - bounding_box_loss: 0.0158 - class_label_loss: 0.2327 - bounding_box_accuracy: 0.5374 - class_label_accuracy: 0.9370 - val_loss: 0.2248 - val_bounding_box_loss: 0.0121 - val_class_label_loss: 0.2128 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 5/30\n",
    "# 30/30 [==============================] - 153s 5s/step - loss: 0.2035 - bounding_box_loss: 0.0140 - class_label_loss: 0.1895 - bounding_box_accuracy: 0.5566 - class_label_accuracy: 0.9541 - val_loss: 0.2562 - val_bounding_box_loss: 0.0109 - val_class_label_loss: 0.2453 - val_bounding_box_accuracy: 0.4231 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 6/30\n",
    "# 30/30 [==============================] - 153s 5s/step - loss: 0.1852 - bounding_box_loss: 0.0130 - class_label_loss: 0.1722 - bounding_box_accuracy: 0.5769 - class_label_accuracy: 0.9541 - val_loss: 0.2363 - val_bounding_box_loss: 0.0101 - val_class_label_loss: 0.2262 - val_bounding_box_accuracy: 0.4808 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 7/30\n",
    "# 30/30 [==============================] - 152s 5s/step - loss: 0.1758 - bounding_box_loss: 0.0124 - class_label_loss: 0.1634 - bounding_box_accuracy: 0.5908 - class_label_accuracy: 0.9605 - val_loss: 0.2584 - val_bounding_box_loss: 0.0110 - val_class_label_loss: 0.2475 - val_bounding_box_accuracy: 0.5000 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 8/30\n",
    "# 30/30 [==============================] - 152s 5s/step - loss: 0.1525 - bounding_box_loss: 0.0119 - class_label_loss: 0.1405 - bounding_box_accuracy: 0.5983 - class_label_accuracy: 0.9626 - val_loss: 0.2647 - val_bounding_box_loss: 0.0104 - val_class_label_loss: 0.2543 - val_bounding_box_accuracy: 0.5000 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 9/30\n",
    "# 30/30 [==============================] - 153s 5s/step - loss: 0.1394 - bounding_box_loss: 0.0118 - class_label_loss: 0.1276 - bounding_box_accuracy: 0.5897 - class_label_accuracy: 0.9669 - val_loss: 0.2605 - val_bounding_box_loss: 0.0095 - val_class_label_loss: 0.2510 - val_bounding_box_accuracy: 0.5769 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 10/30\n",
    "# 30/30 [==============================] - 152s 5s/step - loss: 0.1416 - bounding_box_loss: 0.0109 - class_label_loss: 0.1307 - bounding_box_accuracy: 0.6197 - class_label_accuracy: 0.9690 - val_loss: 0.2499 - val_bounding_box_loss: 0.0092 - val_class_label_loss: 0.2407 - val_bounding_box_accuracy: 0.5385 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 11/30\n",
    "# 30/30 [==============================] - 152s 5s/step - loss: 0.1261 - bounding_box_loss: 0.0103 - class_label_loss: 0.1157 - bounding_box_accuracy: 0.6357 - class_label_accuracy: 0.9712 - val_loss: 0.2894 - val_bounding_box_loss: 0.0087 - val_class_label_loss: 0.2807 - val_bounding_box_accuracy: 0.6058 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 12/30\n",
    "# 30/30 [==============================] - 153s 5s/step - loss: 0.1069 - bounding_box_loss: 0.0101 - class_label_loss: 0.0968 - bounding_box_accuracy: 0.6453 - class_label_accuracy: 0.9765 - val_loss: 0.2791 - val_bounding_box_loss: 0.0093 - val_class_label_loss: 0.2698 - val_bounding_box_accuracy: 0.6442 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 13/30\n",
    "# 30/30 [==============================] - 152s 5s/step - loss: 0.1130 - bounding_box_loss: 0.0101 - class_label_loss: 0.1029 - bounding_box_accuracy: 0.6132 - class_label_accuracy: 0.9722 - val_loss: 0.3133 - val_bounding_box_loss: 0.0086 - val_class_label_loss: 0.3047 - val_bounding_box_accuracy: 0.6442 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 14/30\n",
    "# 30/30 [==============================] - 153s 5s/step - loss: 0.0991 - bounding_box_loss: 0.0099 - class_label_loss: 0.0892 - bounding_box_accuracy: 0.6571 - class_label_accuracy: 0.9733 - val_loss: 0.3195 - val_bounding_box_loss: 0.0081 - val_class_label_loss: 0.3114 - val_bounding_box_accuracy: 0.6538 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 15/30\n",
    "# 30/30 [==============================] - 154s 5s/step - loss: 0.0879 - bounding_box_loss: 0.0095 - class_label_loss: 0.0783 - bounding_box_accuracy: 0.6239 - class_label_accuracy: 0.9754 - val_loss: 0.3828 - val_bounding_box_loss: 0.0084 - val_class_label_loss: 0.3744 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 16/30\n",
    "# 30/30 [==============================] - 154s 5s/step - loss: 0.0896 - bounding_box_loss: 0.0093 - class_label_loss: 0.0803 - bounding_box_accuracy: 0.6603 - class_label_accuracy: 0.9733 - val_loss: 0.3346 - val_bounding_box_loss: 0.0080 - val_class_label_loss: 0.3265 - val_bounding_box_accuracy: 0.6731 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 17/30\n",
    "# 30/30 [==============================] - 154s 5s/step - loss: 0.0830 - bounding_box_loss: 0.0091 - class_label_loss: 0.0738 - bounding_box_accuracy: 0.6442 - class_label_accuracy: 0.9754 - val_loss: 0.3578 - val_bounding_box_loss: 0.0085 - val_class_label_loss: 0.3493 - val_bounding_box_accuracy: 0.6538 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 18/30\n",
    "# 30/30 [==============================] - 153s 5s/step - loss: 0.0837 - bounding_box_loss: 0.0087 - class_label_loss: 0.0750 - bounding_box_accuracy: 0.6720 - class_label_accuracy: 0.9797 - val_loss: 0.3537 - val_bounding_box_loss: 0.0081 - val_class_label_loss: 0.3456 - val_bounding_box_accuracy: 0.5962 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 19/30\n",
    "# 30/30 [==============================] - 153s 5s/step - loss: 0.0628 - bounding_box_loss: 0.0087 - class_label_loss: 0.0541 - bounding_box_accuracy: 0.6314 - class_label_accuracy: 0.9840 - val_loss: 0.3604 - val_bounding_box_loss: 0.0079 - val_class_label_loss: 0.3525 - val_bounding_box_accuracy: 0.6538 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 20/30\n",
    "# 30/30 [==============================] - 155s 5s/step - loss: 0.0630 - bounding_box_loss: 0.0084 - class_label_loss: 0.0545 - bounding_box_accuracy: 0.6752 - class_label_accuracy: 0.9797 - val_loss: 0.4038 - val_bounding_box_loss: 0.0083 - val_class_label_loss: 0.3955 - val_bounding_box_accuracy: 0.7404 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 21/30\n",
    "# 30/30 [==============================] - 155s 5s/step - loss: 0.0609 - bounding_box_loss: 0.0085 - class_label_loss: 0.0524 - bounding_box_accuracy: 0.6613 - class_label_accuracy: 0.9840 - val_loss: 0.3957 - val_bounding_box_loss: 0.0081 - val_class_label_loss: 0.3877 - val_bounding_box_accuracy: 0.7212 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 22/30\n",
    "# 30/30 [==============================] - 155s 5s/step - loss: 0.0549 - bounding_box_loss: 0.0079 - class_label_loss: 0.0470 - bounding_box_accuracy: 0.6934 - class_label_accuracy: 0.9840 - val_loss: 0.4191 - val_bounding_box_loss: 0.0075 - val_class_label_loss: 0.4116 - val_bounding_box_accuracy: 0.7019 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 23/30\n",
    "# 30/30 [==============================] - 155s 5s/step - loss: 0.0650 - bounding_box_loss: 0.0080 - class_label_loss: 0.0570 - bounding_box_accuracy: 0.6955 - class_label_accuracy: 0.9840 - val_loss: 0.4375 - val_bounding_box_loss: 0.0081 - val_class_label_loss: 0.4294 - val_bounding_box_accuracy: 0.6827 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 24/30\n",
    "# 30/30 [==============================] - 156s 5s/step - loss: 0.0707 - bounding_box_loss: 0.0079 - class_label_loss: 0.0628 - bounding_box_accuracy: 0.6902 - class_label_accuracy: 0.9840 - val_loss: 0.4122 - val_bounding_box_loss: 0.0085 - val_class_label_loss: 0.4037 - val_bounding_box_accuracy: 0.7404 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 25/30\n",
    "# 30/30 [==============================] - 157s 5s/step - loss: 0.0521 - bounding_box_loss: 0.0077 - class_label_loss: 0.0444 - bounding_box_accuracy: 0.7297 - class_label_accuracy: 0.9840 - val_loss: 0.4169 - val_bounding_box_loss: 0.0076 - val_class_label_loss: 0.4092 - val_bounding_box_accuracy: 0.7212 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 26/30\n",
    "# 30/30 [==============================] - 174s 6s/step - loss: 0.0383 - bounding_box_loss: 0.0077 - class_label_loss: 0.0307 - bounding_box_accuracy: 0.7126 - class_label_accuracy: 0.9861 - val_loss: 0.5024 - val_bounding_box_loss: 0.0074 - val_class_label_loss: 0.4949 - val_bounding_box_accuracy: 0.7308 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 27/30\n",
    "# 30/30 [==============================] - 211s 7s/step - loss: 0.0465 - bounding_box_loss: 0.0075 - class_label_loss: 0.0390 - bounding_box_accuracy: 0.7030 - class_label_accuracy: 0.9872 - val_loss: 0.5467 - val_bounding_box_loss: 0.0083 - val_class_label_loss: 0.5384 - val_bounding_box_accuracy: 0.7404 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 28/30\n",
    "# 30/30 [==============================] - 313s 10s/step - loss: 0.0617 - bounding_box_loss: 0.0073 - class_label_loss: 0.0544 - bounding_box_accuracy: 0.7254 - class_label_accuracy: 0.9861 - val_loss: 0.4738 - val_bounding_box_loss: 0.0072 - val_class_label_loss: 0.4666 - val_bounding_box_accuracy: 0.6923 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 29/30\n",
    "# 30/30 [==============================] - 282s 9s/step - loss: 0.0549 - bounding_box_loss: 0.0073 - class_label_loss: 0.0476 - bounding_box_accuracy: 0.7169 - class_label_accuracy: 0.9840 - val_loss: 0.4775 - val_bounding_box_loss: 0.0073 - val_class_label_loss: 0.4702 - val_bounding_box_accuracy: 0.7019 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 30/30\n",
    "# 30/30 [==============================] - 246s 8s/step - loss: 0.0639 - bounding_box_loss: 0.0076 - class_label_loss: 0.0563 - bounding_box_accuracy: 0.7105 - class_label_accuracy: 0.9840 - val_loss: 0.5150 - val_bounding_box_loss: 0.0070 - val_class_label_loss: 0.5079 - val_bounding_box_accuracy: 0.6923 - val_class_label_accuracy: 0.9231"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting loss for modeling training\n",
    "plot_loss(model_name='vgg16_model_v3', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training total loss during vgg16_model_v3 training: 0.06387803703546524\n",
    "# Final epoch training class label loss during vgg16_model_v3 training: 0.0562562569975853\n",
    "# Final epoch training bounding box loss during vgg16_model_v3 training: 0.007621781434863806\n",
    "# Final epoch validation total loss during vgg16_model_v3 training: 0.5149593353271484\n",
    "# Final epoch validation class label loss during vgg16_model_v3 training: 0.5079251527786255\n",
    "# Final epoch validation bounding box loss during vgg16_model_v3 training: 0.0070341783575713634"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting accuracy for modeling training\n",
    "plot_accuracy(model_name='vgg16_model_v3', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training class label accurracy during vgg16_model_v3 training: 0.9839743375778198\n",
    "# Final epoch validation class label accurracy during vgg16_model_v3 training: 0.9230769276618958"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating model for mAP metrics and inference time and storing result\n",
    "result_val = evaluate_val(model=vgg16_model_v3, model_name='vgg16_model_v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC PASCAL mAP for vgg16_model_v3 validation data: 0.32520627975463867\n",
    "# VOC PASCAL mAP in all points for vgg16_model_v3 validation data: 0.32603466510772705\n",
    "# COCO mAP for vgg16_model_v3 validation data: 0.5432161092758179\n",
    "# Average inference time for vgg16_model_v3 validation data: 0.1499081185230842"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending results of model evaluation to results dataframe (df_results)\n",
    "df_results_val = df_results_val.append(result_val, ignore_index=True)\n",
    "df_results_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading network but leaving off output layers\n",
    "vgg16 = VGG16(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "# freezing all backbone layers during training so they are not updated\n",
    "vgg16.trainable = False\n",
    "\n",
    "# flattening the max-pooling output of network\n",
    "flatten = vgg16.output\n",
    "flat = Flatten()(flatten)\n",
    "\n",
    "# constructing an output layer to predict bounding box coordinates\n",
    "bbox_head1 = Dense(256, activation=\"relu\")(flat)\n",
    "bbox_head2 = Dropout(0.5)(bbox_head1)\n",
    "bbox_head3 = Dense(128, activation=\"relu\")(bbox_head2)\n",
    "bbox_head4 = Dense(64, activation=\"relu\")(bbox_head3)\n",
    "bbox_head5 = Dense(32, activation=\"relu\")(bbox_head4)\n",
    "bbox_head_v4 = Dense(4, activation=\"sigmoid\", name=\"bounding_box\")(bbox_head5)\n",
    "\n",
    "# constructing an output layer to predict class labels\n",
    "class_head1 = Dense(512, activation=\"relu\")(flat)\n",
    "class_head2 = Dropout(0.5)(class_head1)\n",
    "class_head3 = Dense(512, activation=\"relu\")(class_head2)\n",
    "class_head4 = Dropout(0.5)(class_head3)\n",
    "class_head5 = Dense(256, activation=\"relu\")(class_head4)\n",
    "class_head6 = Dropout(0.5)(class_head5)\n",
    "class_head7 = Dense(128, activation=\"relu\")(class_head6)\n",
    "class_head8 = Dropout(0.5)(class_head7)\n",
    "class_head_v4 = Dense(2, activation=\"sigmoid\", name=\"class_label\")(class_head8)\n",
    "\n",
    "# creating a model that accepts an input image and outputs bounding box coordinates and a class label\n",
    "vgg16_model_v4 = Model(inputs=vgg16.input, outputs=(bbox_head_v4, class_head_v4))\n",
    "\n",
    "# creating dictionary for setting loss functions\n",
    "losses = {\"class_label\": \"binary_crossentropy\", \"bounding_box\": \"mean_squared_error\"}\n",
    "\n",
    "# creating dictionry for weighting loss functions\n",
    "loss_weights = {\"class_label\": 1.0, \"bounding_box\": 1.0}\n",
    "\n",
    "# initilizing an \"Adam\" optimizer with learning rate of 0.0001\n",
    "opt = Adam(learning_rate = 0.0001)\n",
    "\n",
    "# compiling model\n",
    "vgg16_model_v4.compile(loss=losses, optimizer=opt, metrics=[\"accuracy\"], loss_weights=loss_weights)\n",
    "print(vgg16_model_v4.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing a dictionary for our training outputs\n",
    "train_targets = {\"class_label\": labels_train, \"bounding_box\": bboxes_train}\n",
    "\n",
    "# constructing a dictionary for our test outputs\n",
    "val_targets = {\"class_label\": labels_val, \"bounding_box\": bboxes_val}\n",
    "\n",
    "# training model for class and bounding box predictions\n",
    "H = vgg16_model_v4.fit(x_train, train_targets, validation_data=(x_val, val_targets), batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "\n",
    "# saving trained model\n",
    "vgg16_model_v4.save('../Models/vgg16_model_v4.h5', save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 1/30\n",
    "# 30/30 [==============================] - 166s 6s/step - loss: 0.7710 - bounding_box_loss: 0.0454 - class_label_loss: 0.7256 - bounding_box_accuracy: 0.5224 - class_label_accuracy: 0.6261 - val_loss: 0.3706 - val_bounding_box_loss: 0.0202 - val_class_label_loss: 0.3503 - val_bounding_box_accuracy: 0.5096 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 2/30\n",
    "# 30/30 [==============================] - 163s 5s/step - loss: 0.4722 - bounding_box_loss: 0.0213 - class_label_loss: 0.4509 - bounding_box_accuracy: 0.5406 - class_label_accuracy: 0.8301 - val_loss: 0.2554 - val_bounding_box_loss: 0.0131 - val_class_label_loss: 0.2424 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 3/30\n",
    "# 30/30 [==============================] - 166s 6s/step - loss: 0.3623 - bounding_box_loss: 0.0170 - class_label_loss: 0.3453 - bounding_box_accuracy: 0.5609 - class_label_accuracy: 0.8835 - val_loss: 0.2157 - val_bounding_box_loss: 0.0121 - val_class_label_loss: 0.2037 - val_bounding_box_accuracy: 0.5769 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 4/30\n",
    "# 30/30 [==============================] - 165s 6s/step - loss: 0.2771 - bounding_box_loss: 0.0154 - class_label_loss: 0.2617 - bounding_box_accuracy: 0.5716 - class_label_accuracy: 0.9338 - val_loss: 0.2148 - val_bounding_box_loss: 0.0117 - val_class_label_loss: 0.2031 - val_bounding_box_accuracy: 0.5962 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 5/30\n",
    "# 30/30 [==============================] - 166s 6s/step - loss: 0.2631 - bounding_box_loss: 0.0140 - class_label_loss: 0.2492 - bounding_box_accuracy: 0.5662 - class_label_accuracy: 0.9316 - val_loss: 0.2142 - val_bounding_box_loss: 0.0107 - val_class_label_loss: 0.2034 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 6/30\n",
    "# 30/30 [==============================] - 166s 6s/step - loss: 0.2223 - bounding_box_loss: 0.0130 - class_label_loss: 0.2092 - bounding_box_accuracy: 0.5855 - class_label_accuracy: 0.9412 - val_loss: 0.2338 - val_bounding_box_loss: 0.0105 - val_class_label_loss: 0.2233 - val_bounding_box_accuracy: 0.5769 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 7/30\n",
    "# 30/30 [==============================] - 165s 5s/step - loss: 0.2140 - bounding_box_loss: 0.0124 - class_label_loss: 0.2017 - bounding_box_accuracy: 0.5588 - class_label_accuracy: 0.9562 - val_loss: 0.2181 - val_bounding_box_loss: 0.0106 - val_class_label_loss: 0.2076 - val_bounding_box_accuracy: 0.6154 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 8/30\n",
    "# 30/30 [==============================] - 165s 6s/step - loss: 0.1852 - bounding_box_loss: 0.0114 - class_label_loss: 0.1738 - bounding_box_accuracy: 0.6079 - class_label_accuracy: 0.9530 - val_loss: 0.2331 - val_bounding_box_loss: 0.0097 - val_class_label_loss: 0.2234 - val_bounding_box_accuracy: 0.6154 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 9/30\n",
    "# 30/30 [==============================] - 164s 5s/step - loss: 0.1777 - bounding_box_loss: 0.0108 - class_label_loss: 0.1669 - bounding_box_accuracy: 0.6271 - class_label_accuracy: 0.9637 - val_loss: 0.2218 - val_bounding_box_loss: 0.0094 - val_class_label_loss: 0.2124 - val_bounding_box_accuracy: 0.5865 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 10/30\n",
    "# 30/30 [==============================] - 162s 5s/step - loss: 0.1875 - bounding_box_loss: 0.0106 - class_label_loss: 0.1769 - bounding_box_accuracy: 0.6314 - class_label_accuracy: 0.9530 - val_loss: 0.2263 - val_bounding_box_loss: 0.0089 - val_class_label_loss: 0.2174 - val_bounding_box_accuracy: 0.6346 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 11/30\n",
    "# 30/30 [==============================] - 163s 5s/step - loss: 0.1642 - bounding_box_loss: 0.0104 - class_label_loss: 0.1538 - bounding_box_accuracy: 0.6368 - class_label_accuracy: 0.9615 - val_loss: 0.2483 - val_bounding_box_loss: 0.0085 - val_class_label_loss: 0.2398 - val_bounding_box_accuracy: 0.6538 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 12/30\n",
    "# 30/30 [==============================] - 166s 6s/step - loss: 0.1455 - bounding_box_loss: 0.0102 - class_label_loss: 0.1352 - bounding_box_accuracy: 0.6613 - class_label_accuracy: 0.9658 - val_loss: 0.2474 - val_bounding_box_loss: 0.0087 - val_class_label_loss: 0.2387 - val_bounding_box_accuracy: 0.6058 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 13/30\n",
    "# 30/30 [==============================] - 164s 5s/step - loss: 0.1345 - bounding_box_loss: 0.0093 - class_label_loss: 0.1252 - bounding_box_accuracy: 0.6923 - class_label_accuracy: 0.9701 - val_loss: 0.2620 - val_bounding_box_loss: 0.0090 - val_class_label_loss: 0.2530 - val_bounding_box_accuracy: 0.6827 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 14/30\n",
    "# 30/30 [==============================] - 164s 5s/step - loss: 0.1271 - bounding_box_loss: 0.0093 - class_label_loss: 0.1178 - bounding_box_accuracy: 0.6795 - class_label_accuracy: 0.9701 - val_loss: 0.2659 - val_bounding_box_loss: 0.0083 - val_class_label_loss: 0.2576 - val_bounding_box_accuracy: 0.6731 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 15/30\n",
    "# 30/30 [==============================] - 165s 5s/step - loss: 0.1188 - bounding_box_loss: 0.0092 - class_label_loss: 0.1096 - bounding_box_accuracy: 0.6538 - class_label_accuracy: 0.9701 - val_loss: 0.2735 - val_bounding_box_loss: 0.0081 - val_class_label_loss: 0.2655 - val_bounding_box_accuracy: 0.6827 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 16/30\n",
    "# 30/30 [==============================] - 163s 5s/step - loss: 0.1061 - bounding_box_loss: 0.0085 - class_label_loss: 0.0976 - bounding_box_accuracy: 0.6838 - class_label_accuracy: 0.9744 - val_loss: 0.2521 - val_bounding_box_loss: 0.0078 - val_class_label_loss: 0.2442 - val_bounding_box_accuracy: 0.6827 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 17/30\n",
    "# 30/30 [==============================] - 161s 5s/step - loss: 0.1110 - bounding_box_loss: 0.0086 - class_label_loss: 0.1025 - bounding_box_accuracy: 0.6891 - class_label_accuracy: 0.9733 - val_loss: 0.3004 - val_bounding_box_loss: 0.0077 - val_class_label_loss: 0.2927 - val_bounding_box_accuracy: 0.6827 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 18/30\n",
    "# 30/30 [==============================] - 160s 5s/step - loss: 0.1124 - bounding_box_loss: 0.0081 - class_label_loss: 0.1043 - bounding_box_accuracy: 0.6859 - class_label_accuracy: 0.9690 - val_loss: 0.3048 - val_bounding_box_loss: 0.0078 - val_class_label_loss: 0.2970 - val_bounding_box_accuracy: 0.6346 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 19/30\n",
    "# 30/30 [==============================] - 160s 5s/step - loss: 0.0834 - bounding_box_loss: 0.0080 - class_label_loss: 0.0754 - bounding_box_accuracy: 0.7041 - class_label_accuracy: 0.9776 - val_loss: 0.3117 - val_bounding_box_loss: 0.0077 - val_class_label_loss: 0.3040 - val_bounding_box_accuracy: 0.6635 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 20/30\n",
    "# 30/30 [==============================] - 161s 5s/step - loss: 0.0839 - bounding_box_loss: 0.0078 - class_label_loss: 0.0761 - bounding_box_accuracy: 0.6902 - class_label_accuracy: 0.9786 - val_loss: 0.3295 - val_bounding_box_loss: 0.0078 - val_class_label_loss: 0.3217 - val_bounding_box_accuracy: 0.6731 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 21/30\n",
    "# 30/30 [==============================] - 163s 5s/step - loss: 0.0709 - bounding_box_loss: 0.0073 - class_label_loss: 0.0636 - bounding_box_accuracy: 0.7169 - class_label_accuracy: 0.9808 - val_loss: 0.3539 - val_bounding_box_loss: 0.0078 - val_class_label_loss: 0.3461 - val_bounding_box_accuracy: 0.7019 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 22/30\n",
    "# 30/30 [==============================] - 164s 5s/step - loss: 0.0665 - bounding_box_loss: 0.0073 - class_label_loss: 0.0592 - bounding_box_accuracy: 0.7062 - class_label_accuracy: 0.9765 - val_loss: 0.3423 - val_bounding_box_loss: 0.0082 - val_class_label_loss: 0.3342 - val_bounding_box_accuracy: 0.7500 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 23/30\n",
    "# 30/30 [==============================] - 169s 6s/step - loss: 0.0600 - bounding_box_loss: 0.0070 - class_label_loss: 0.0530 - bounding_box_accuracy: 0.7062 - class_label_accuracy: 0.9818 - val_loss: 0.4094 - val_bounding_box_loss: 0.0079 - val_class_label_loss: 0.4015 - val_bounding_box_accuracy: 0.7308 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 24/30\n",
    "# 30/30 [==============================] - 179s 6s/step - loss: 0.0660 - bounding_box_loss: 0.0070 - class_label_loss: 0.0590 - bounding_box_accuracy: 0.7083 - class_label_accuracy: 0.9829 - val_loss: 0.4234 - val_bounding_box_loss: 0.0088 - val_class_label_loss: 0.4146 - val_bounding_box_accuracy: 0.7212 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 25/30\n",
    "# 30/30 [==============================] - 178s 6s/step - loss: 0.0577 - bounding_box_loss: 0.0069 - class_label_loss: 0.0508 - bounding_box_accuracy: 0.7212 - class_label_accuracy: 0.9829 - val_loss: 0.4192 - val_bounding_box_loss: 0.0078 - val_class_label_loss: 0.4114 - val_bounding_box_accuracy: 0.6923 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 26/30\n",
    "# 30/30 [==============================] - 179s 6s/step - loss: 0.0642 - bounding_box_loss: 0.0066 - class_label_loss: 0.0576 - bounding_box_accuracy: 0.7415 - class_label_accuracy: 0.9829 - val_loss: 0.3723 - val_bounding_box_loss: 0.0074 - val_class_label_loss: 0.3649 - val_bounding_box_accuracy: 0.7308 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 27/30\n",
    "# 30/30 [==============================] - 183s 6s/step - loss: 0.0759 - bounding_box_loss: 0.0065 - class_label_loss: 0.0694 - bounding_box_accuracy: 0.7457 - class_label_accuracy: 0.9754 - val_loss: 0.4244 - val_bounding_box_loss: 0.0073 - val_class_label_loss: 0.4172 - val_bounding_box_accuracy: 0.7404 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 28/30\n",
    "# 30/30 [==============================] - 181s 6s/step - loss: 0.0606 - bounding_box_loss: 0.0063 - class_label_loss: 0.0543 - bounding_box_accuracy: 0.7532 - class_label_accuracy: 0.9840 - val_loss: 0.4136 - val_bounding_box_loss: 0.0080 - val_class_label_loss: 0.4056 - val_bounding_box_accuracy: 0.7500 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 29/30\n",
    "# 30/30 [==============================] - 176s 6s/step - loss: 0.0604 - bounding_box_loss: 0.0064 - class_label_loss: 0.0540 - bounding_box_accuracy: 0.7703 - class_label_accuracy: 0.9840 - val_loss: 0.3899 - val_bounding_box_loss: 0.0076 - val_class_label_loss: 0.3822 - val_bounding_box_accuracy: 0.7212 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 30/30\n",
    "# 30/30 [==============================] - 178s 6s/step - loss: 0.0486 - bounding_box_loss: 0.0060 - class_label_loss: 0.0426 - bounding_box_accuracy: 0.7553 - class_label_accuracy: 0.9882 - val_loss: 0.4330 - val_bounding_box_loss: 0.0075 - val_class_label_loss: 0.4255 - val_bounding_box_accuracy: 0.7308 - val_class_label_accuracy: 0.9231"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting loss for modeling training\n",
    "plot_loss(model_name='vgg16_model_v4', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training total loss during vgg16_model_v4 training: 0.04861637204885483\n",
    "# Final epoch training class label loss during vgg16_model_v4 training: 0.0426015667617321\n",
    "# Final epoch training bounding box loss during vgg16_model_v4 training: 0.006014810875058174\n",
    "# Final epoch validation total loss during vgg16_model_v4 training: 0.4329741895198822\n",
    "# Final epoch validation class label loss during vgg16_model_v4 training: 0.42547354102134705\n",
    "# Final epoch validation bounding box loss during vgg16_model_v4 training: 0.0075006368570029736"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting accuracy for modeling training\n",
    "plot_accuracy(model_name='vgg16_model_v4', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training class label accurracy during vgg16_model_v4 training: 0.9882478713989258\n",
    "# Final epoch validation class label accurracy during vgg16_model_v4 training: 0.9230769276618958"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating model for mAP metrics and inference time and storing result\n",
    "result_val = evaluate_val(model=vgg16_model_v4, model_name='vgg16_model_v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC PASCAL mAP for vgg16_model_v4 validation data: 0.3273618221282959\n",
    "# VOC PASCAL mAP in all points for vgg16_model_v4 validation data: 0.30979838967323303\n",
    "# COCO mAP for vgg16_model_v4 validation data: 0.5074224472045898\n",
    "# Average inference time for vgg16_model_v4 validation data: 0.15806405131633466"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending results of model evaluation to results dataframe (df_results)\n",
    "df_results_val = df_results_val.append(result_val, ignore_index=True)\n",
    "df_results_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading network but leaving off output layers\n",
    "vgg16 = VGG16(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "# freezing all backbone layers during training so they are not updated\n",
    "vgg16.trainable = False\n",
    "\n",
    "# flattening the max-pooling output of network\n",
    "flatten = vgg16.output\n",
    "flat = Flatten()(flatten)\n",
    "\n",
    "# constructing an output layer to predict bounding box coordinates\n",
    "bbox_head1 = Dense(256, activation=\"relu\")(flat)\n",
    "bbox_head2 = Dropout(0.5)(bbox_head1)\n",
    "bbox_head3 = Dense(128, activation=\"relu\")(bbox_head2)\n",
    "bbox_head4 = Dense(64, activation=\"relu\")(bbox_head3)\n",
    "bbox_head5 = Dense(32, activation=\"relu\")(bbox_head4)\n",
    "bbox_head_v5 = Dense(4, activation=\"sigmoid\", name=\"bounding_box\")(bbox_head5)\n",
    "\n",
    "# constructing an output layer to predict class labels\n",
    "class_head1 = Dense(512, activation=\"relu\")(flat)\n",
    "class_head2 = Dropout(0.5)(class_head1)\n",
    "class_head3 = Dense(256, activation=\"relu\")(class_head2)\n",
    "class_head4 = Dropout(0.5)(class_head3)\n",
    "class_head_v5 = Dense(2, activation=\"sigmoid\", name=\"class_label\")(class_head4)\n",
    "\n",
    "# creating a model that accepts an input image and outputs bounding box coordinates and a class label\n",
    "vgg16_model_v5 = Model(inputs=vgg16.input, outputs=(bbox_head_v5, class_head_v5))\n",
    "\n",
    "# creating dictionary for setting loss functions\n",
    "losses = {\"class_label\": \"binary_crossentropy\", \"bounding_box\": \"mean_squared_error\"}\n",
    "\n",
    "# creating dictionry for weighting loss functions\n",
    "loss_weights = {\"class_label\": 1.0, \"bounding_box\": 1.0}\n",
    "\n",
    "# initilizing an \"Adam\" optimizer with learning rate of 0.0001\n",
    "opt = Adam(learning_rate = 0.0001)\n",
    "\n",
    "# compiling model\n",
    "vgg16_model_v5.compile(loss=losses, optimizer=opt, metrics=[\"accuracy\"], loss_weights=loss_weights)\n",
    "print(vgg16_model_v5.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing a dictionary for our training outputs\n",
    "train_targets = {\"class_label\": labels_train, \"bounding_box\": bboxes_train}\n",
    "\n",
    "# constructing a dictionary for our test outputs\n",
    "val_targets = {\"class_label\": labels_val, \"bounding_box\": bboxes_val}\n",
    "\n",
    "# training model for class and bounding box predictions\n",
    "H = vgg16_model_v5.fit(x_train, train_targets, validation_data=(x_val, val_targets), batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "\n",
    "# saving trained model\n",
    "vgg16_model_v5.save('../Models/vgg16_model_v5.h5', save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 1/30\n",
    "# 30/30 [==============================] - 173s 6s/step - loss: 0.3856 - bounding_box_loss: 0.0342 - class_label_loss: 0.3514 - bounding_box_accuracy: 0.5609 - class_label_accuracy: 0.8846 - val_loss: 0.2320 - val_bounding_box_loss: 0.0157 - val_class_label_loss: 0.2163 - val_bounding_box_accuracy: 0.3846 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 2/30\n",
    "# 30/30 [==============================] - 173s 6s/step - loss: 0.2299 - bounding_box_loss: 0.0195 - class_label_loss: 0.2104 - bounding_box_accuracy: 0.5598 - class_label_accuracy: 0.9444 - val_loss: 0.2328 - val_bounding_box_loss: 0.0130 - val_class_label_loss: 0.2198 - val_bounding_box_accuracy: 0.4423 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 3/30\n",
    "# 30/30 [==============================] - 186s 6s/step - loss: 0.1877 - bounding_box_loss: 0.0153 - class_label_loss: 0.1724 - bounding_box_accuracy: 0.5524 - class_label_accuracy: 0.9573 - val_loss: 0.2285 - val_bounding_box_loss: 0.0128 - val_class_label_loss: 0.2157 - val_bounding_box_accuracy: 0.6442 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 4/30\n",
    "# 30/30 [==============================] - 188s 6s/step - loss: 0.1796 - bounding_box_loss: 0.0137 - class_label_loss: 0.1658 - bounding_box_accuracy: 0.5673 - class_label_accuracy: 0.9551 - val_loss: 0.2454 - val_bounding_box_loss: 0.0106 - val_class_label_loss: 0.2348 - val_bounding_box_accuracy: 0.6058 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 5/30\n",
    "# 30/30 [==============================] - 182s 6s/step - loss: 0.1713 - bounding_box_loss: 0.0122 - class_label_loss: 0.1591 - bounding_box_accuracy: 0.6143 - class_label_accuracy: 0.9573 - val_loss: 0.2230 - val_bounding_box_loss: 0.0103 - val_class_label_loss: 0.2127 - val_bounding_box_accuracy: 0.5962 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 6/30\n",
    "# 30/30 [==============================] - 178s 6s/step - loss: 0.1461 - bounding_box_loss: 0.0113 - class_label_loss: 0.1348 - bounding_box_accuracy: 0.5951 - class_label_accuracy: 0.9637 - val_loss: 0.2378 - val_bounding_box_loss: 0.0102 - val_class_label_loss: 0.2276 - val_bounding_box_accuracy: 0.5673 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 7/30\n",
    "# 30/30 [==============================] - 181s 6s/step - loss: 0.1267 - bounding_box_loss: 0.0107 - class_label_loss: 0.1160 - bounding_box_accuracy: 0.6282 - class_label_accuracy: 0.9733 - val_loss: 0.2530 - val_bounding_box_loss: 0.0096 - val_class_label_loss: 0.2434 - val_bounding_box_accuracy: 0.6346 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 8/30\n",
    "# 30/30 [==============================] - 172s 6s/step - loss: 0.1137 - bounding_box_loss: 0.0104 - class_label_loss: 0.1033 - bounding_box_accuracy: 0.6100 - class_label_accuracy: 0.9669 - val_loss: 0.2527 - val_bounding_box_loss: 0.0087 - val_class_label_loss: 0.2440 - val_bounding_box_accuracy: 0.5962 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 9/30\n",
    "# 30/30 [==============================] - 177s 6s/step - loss: 0.1168 - bounding_box_loss: 0.0098 - class_label_loss: 0.1070 - bounding_box_accuracy: 0.6410 - class_label_accuracy: 0.9679 - val_loss: 0.2564 - val_bounding_box_loss: 0.0093 - val_class_label_loss: 0.2471 - val_bounding_box_accuracy: 0.6635 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 10/30\n",
    "# 30/30 [==============================] - 172s 6s/step - loss: 0.0987 - bounding_box_loss: 0.0090 - class_label_loss: 0.0897 - bounding_box_accuracy: 0.6635 - class_label_accuracy: 0.9754 - val_loss: 0.2832 - val_bounding_box_loss: 0.0083 - val_class_label_loss: 0.2749 - val_bounding_box_accuracy: 0.6058 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 11/30\n",
    "# 30/30 [==============================] - 173s 6s/step - loss: 0.0877 - bounding_box_loss: 0.0088 - class_label_loss: 0.0789 - bounding_box_accuracy: 0.6528 - class_label_accuracy: 0.9797 - val_loss: 0.2772 - val_bounding_box_loss: 0.0081 - val_class_label_loss: 0.2691 - val_bounding_box_accuracy: 0.6442 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 12/30\n",
    "# 30/30 [==============================] - 173s 6s/step - loss: 0.0790 - bounding_box_loss: 0.0081 - class_label_loss: 0.0710 - bounding_box_accuracy: 0.6784 - class_label_accuracy: 0.9765 - val_loss: 0.2821 - val_bounding_box_loss: 0.0080 - val_class_label_loss: 0.2742 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 13/30\n",
    "# 30/30 [==============================] - 175s 6s/step - loss: 0.0785 - bounding_box_loss: 0.0080 - class_label_loss: 0.0705 - bounding_box_accuracy: 0.6934 - class_label_accuracy: 0.9786 - val_loss: 0.2744 - val_bounding_box_loss: 0.0084 - val_class_label_loss: 0.2661 - val_bounding_box_accuracy: 0.6923 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 14/30\n",
    "# 30/30 [==============================] - 162s 5s/step - loss: 0.0767 - bounding_box_loss: 0.0076 - class_label_loss: 0.0691 - bounding_box_accuracy: 0.6720 - class_label_accuracy: 0.9765 - val_loss: 0.3470 - val_bounding_box_loss: 0.0077 - val_class_label_loss: 0.3393 - val_bounding_box_accuracy: 0.6731 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 15/30\n",
    "# 30/30 [==============================] - 160s 5s/step - loss: 0.0682 - bounding_box_loss: 0.0078 - class_label_loss: 0.0604 - bounding_box_accuracy: 0.6902 - class_label_accuracy: 0.9797 - val_loss: 0.3718 - val_bounding_box_loss: 0.0082 - val_class_label_loss: 0.3636 - val_bounding_box_accuracy: 0.6731 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 16/30\n",
    "# 30/30 [==============================] - 159s 5s/step - loss: 0.0566 - bounding_box_loss: 0.0075 - class_label_loss: 0.0490 - bounding_box_accuracy: 0.6891 - class_label_accuracy: 0.9872 - val_loss: 0.3250 - val_bounding_box_loss: 0.0094 - val_class_label_loss: 0.3156 - val_bounding_box_accuracy: 0.6731 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 17/30\n",
    "# 30/30 [==============================] - 161s 5s/step - loss: 0.0587 - bounding_box_loss: 0.0072 - class_label_loss: 0.0515 - bounding_box_accuracy: 0.7041 - class_label_accuracy: 0.9840 - val_loss: 0.3420 - val_bounding_box_loss: 0.0075 - val_class_label_loss: 0.3345 - val_bounding_box_accuracy: 0.7212 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 18/30\n",
    "# 30/30 [==============================] - 166s 6s/step - loss: 0.0533 - bounding_box_loss: 0.0069 - class_label_loss: 0.0464 - bounding_box_accuracy: 0.7094 - class_label_accuracy: 0.9850 - val_loss: 0.4070 - val_bounding_box_loss: 0.0077 - val_class_label_loss: 0.3993 - val_bounding_box_accuracy: 0.6635 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 19/30\n",
    "# 30/30 [==============================] - 145s 5s/step - loss: 0.0517 - bounding_box_loss: 0.0067 - class_label_loss: 0.0450 - bounding_box_accuracy: 0.7083 - class_label_accuracy: 0.9840 - val_loss: 0.3872 - val_bounding_box_loss: 0.0080 - val_class_label_loss: 0.3792 - val_bounding_box_accuracy: 0.7115 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 20/30\n",
    "# 30/30 [==============================] - 143s 5s/step - loss: 0.0641 - bounding_box_loss: 0.0066 - class_label_loss: 0.0574 - bounding_box_accuracy: 0.7179 - class_label_accuracy: 0.9754 - val_loss: 0.3487 - val_bounding_box_loss: 0.0076 - val_class_label_loss: 0.3411 - val_bounding_box_accuracy: 0.7019 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 21/30\n",
    "# 30/30 [==============================] - 139s 5s/step - loss: 0.0478 - bounding_box_loss: 0.0066 - class_label_loss: 0.0412 - bounding_box_accuracy: 0.7286 - class_label_accuracy: 0.9818 - val_loss: 0.3692 - val_bounding_box_loss: 0.0082 - val_class_label_loss: 0.3609 - val_bounding_box_accuracy: 0.7019 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 22/30\n",
    "# 30/30 [==============================] - 140s 5s/step - loss: 0.0534 - bounding_box_loss: 0.0063 - class_label_loss: 0.0472 - bounding_box_accuracy: 0.7030 - class_label_accuracy: 0.9840 - val_loss: 0.3691 - val_bounding_box_loss: 0.0093 - val_class_label_loss: 0.3598 - val_bounding_box_accuracy: 0.7019 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 23/30\n",
    "# 30/30 [==============================] - 141s 5s/step - loss: 0.0451 - bounding_box_loss: 0.0063 - class_label_loss: 0.0388 - bounding_box_accuracy: 0.7308 - class_label_accuracy: 0.9850 - val_loss: 0.4387 - val_bounding_box_loss: 0.0074 - val_class_label_loss: 0.4313 - val_bounding_box_accuracy: 0.6923 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 24/30\n",
    "# 30/30 [==============================] - 144s 5s/step - loss: 0.0419 - bounding_box_loss: 0.0060 - class_label_loss: 0.0360 - bounding_box_accuracy: 0.7425 - class_label_accuracy: 0.9904 - val_loss: 0.4393 - val_bounding_box_loss: 0.0079 - val_class_label_loss: 0.4314 - val_bounding_box_accuracy: 0.7115 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 25/30\n",
    "# 30/30 [==============================] - 155s 5s/step - loss: 0.0450 - bounding_box_loss: 0.0059 - class_label_loss: 0.0390 - bounding_box_accuracy: 0.7222 - class_label_accuracy: 0.9861 - val_loss: 0.4725 - val_bounding_box_loss: 0.0081 - val_class_label_loss: 0.4644 - val_bounding_box_accuracy: 0.6827 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 26/30\n",
    "# 30/30 [==============================] - 155s 5s/step - loss: 0.0438 - bounding_box_loss: 0.0059 - class_label_loss: 0.0378 - bounding_box_accuracy: 0.7415 - class_label_accuracy: 0.9904 - val_loss: 0.5082 - val_bounding_box_loss: 0.0078 - val_class_label_loss: 0.5003 - val_bounding_box_accuracy: 0.7308 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 27/30\n",
    "# 30/30 [==============================] - 153s 5s/step - loss: 0.0398 - bounding_box_loss: 0.0055 - class_label_loss: 0.0343 - bounding_box_accuracy: 0.7436 - class_label_accuracy: 0.9861 - val_loss: 0.4507 - val_bounding_box_loss: 0.0075 - val_class_label_loss: 0.4432 - val_bounding_box_accuracy: 0.7115 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 28/30\n",
    "# 30/30 [==============================] - 167s 6s/step - loss: 0.0465 - bounding_box_loss: 0.0057 - class_label_loss: 0.0408 - bounding_box_accuracy: 0.7425 - class_label_accuracy: 0.9829 - val_loss: 0.4283 - val_bounding_box_loss: 0.0074 - val_class_label_loss: 0.4209 - val_bounding_box_accuracy: 0.7596 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 29/30\n",
    "# 30/30 [==============================] - 163s 5s/step - loss: 0.0431 - bounding_box_loss: 0.0054 - class_label_loss: 0.0376 - bounding_box_accuracy: 0.7543 - class_label_accuracy: 0.9829 - val_loss: 0.4141 - val_bounding_box_loss: 0.0077 - val_class_label_loss: 0.4064 - val_bounding_box_accuracy: 0.7308 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 30/30\n",
    "# 30/30 [==============================] - 164s 5s/step - loss: 0.0372 - bounding_box_loss: 0.0053 - class_label_loss: 0.0319 - bounding_box_accuracy: 0.7543 - class_label_accuracy: 0.9861 - val_loss: 0.4114 - val_bounding_box_loss: 0.0074 - val_class_label_loss: 0.4040 - val_bounding_box_accuracy: 0.7596 - val_class_label_accuracy: 0.9327"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting loss for modeling training\n",
    "plot_loss(model_name='vgg16_model_v5', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training total loss during vgg16_model_v5 training: 0.037232257425785065\n",
    "# Final epoch training class label loss during vgg16_model_v5 training: 0.03188478946685791\n",
    "# Final epoch training bounding box loss during vgg16_model_v5 training: 0.005347466561943293\n",
    "# Final epoch validation total loss during vgg16_model_v5 training: 0.4114198386669159\n",
    "# Final epoch validation class label loss during vgg16_model_v5 training: 0.40399906039237976\n",
    "# Final epoch validation bounding box loss during vgg16_model_v5 training: 0.007420760113745928"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting accuracy for modeling training\n",
    "plot_accuracy(model_name='vgg16_model_v5', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training class label accurracy during vgg16_model_v5 training: 0.9861111044883728\n",
    "# Final epoch validation class label accurracy during vgg16_model_v5 training: 0.932692289352417"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating model for mAP metrics and inference time and storing result\n",
    "result_val = evaluate_val(model=vgg16_model_v5, model_name='vgg16_model_v5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC PASCAL mAP for vgg16_model_v5 validation data: 0.3445416986942291\n",
    "# VOC PASCAL mAP in all points for vgg16_model_v5 validation data: 0.3388655185699463\n",
    "# COCO mAP for vgg16_model_v5 validation data: 0.5221164226531982\n",
    "# Average inference time for vgg16_model_v5 validation data: 0.15669844700739935"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending results of model evaluation to results dataframe (df_results)\n",
    "df_results_val = df_results_val.append(result_val, ignore_index=True)\n",
    "df_results_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading network but leaving off output layers\n",
    "vgg16 = VGG16(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "# freezing all backbone layers during training so they are not updated\n",
    "vgg16.trainable = False\n",
    "\n",
    "# flattening the max-pooling output of network\n",
    "flatten = vgg16.output\n",
    "flat = Flatten()(flatten)\n",
    "\n",
    "# constructing an output layer to predict bounding box coordinates\n",
    "bbox_head1 = Dense(256, activation=\"relu\")(flat)\n",
    "bbox_head2 = Dense(128, activation=\"relu\")(bbox_head1)\n",
    "bbox_head3 = Dense(64, activation=\"relu\")(bbox_head2)\n",
    "bbox_head4 = Dense(32, activation=\"relu\")(bbox_head3)\n",
    "bbox_head_v6 = Dense(4, activation=\"sigmoid\", name=\"bounding_box\")(bbox_head4)\n",
    "\n",
    "# constructing an output layer to predict class labels\n",
    "class_head1 = Dense(256, activation=\"relu\")(flat)\n",
    "class_head2 = Dropout(0.5)(class_head1)\n",
    "class_head3 = Dense(128, activation=\"relu\")(class_head2)\n",
    "class_head4 = Dropout(0.5)(class_head3)\n",
    "class_head_v6 = Dense(2, activation=\"sigmoid\", name=\"class_label\")(class_head4)\n",
    "\n",
    "# creating a model that accepts an input image and outputs bounding box coordinates and a class label\n",
    "vgg16_model_v6 = Model(inputs=vgg16.input, outputs=(bbox_head_v6, class_head_v6))\n",
    "\n",
    "# creating dictionary for setting loss functions\n",
    "losses = {\"class_label\": \"binary_crossentropy\", \"bounding_box\": \"mean_squared_error\"}\n",
    "\n",
    "# creating dictionry for weighting loss functions\n",
    "loss_weights = {\"class_label\": 1.0, \"bounding_box\": 1.0}\n",
    "\n",
    "# initilizing an \"Adam\" optimizer with learning rate of 0.0001\n",
    "opt = Adam(learning_rate = 0.0001)\n",
    "\n",
    "# compiling model\n",
    "vgg16_model_v6.compile(loss=losses, optimizer=opt, metrics=[\"accuracy\"], loss_weights=loss_weights)\n",
    "print(vgg16_model_v6.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing a dictionary for our training outputs\n",
    "train_targets = {\"class_label\": labels_train, \"bounding_box\": bboxes_train}\n",
    "\n",
    "# constructing a dictionary for our test outputs\n",
    "val_targets = {\"class_label\": labels_val, \"bounding_box\": bboxes_val}\n",
    "\n",
    "# training model for class and bounding box predictions\n",
    "H = vgg16_model_v6.fit(x_train, train_targets, validation_data=(x_val, val_targets), batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "\n",
    "# saving trained model\n",
    "vgg16_model_v6.save('../Models/vgg16_model_v6.h5', save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 1/30\n",
    "# 30/30 [==============================] - 176s 6s/step - loss: 0.4034 - bounding_box_loss: 0.0208 - class_label_loss: 0.3826 - bounding_box_accuracy: 0.5267 - class_label_accuracy: 0.8547 - val_loss: 0.2113 - val_bounding_box_loss: 0.0121 - val_class_label_loss: 0.1993 - val_bounding_box_accuracy: 0.5000 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 2/30\n",
    "# 30/30 [==============================] - 194s 6s/step - loss: 0.2345 - bounding_box_loss: 0.0111 - class_label_loss: 0.2234 - bounding_box_accuracy: 0.6229 - class_label_accuracy: 0.9412 - val_loss: 0.2369 - val_bounding_box_loss: 0.0105 - val_class_label_loss: 0.2264 - val_bounding_box_accuracy: 0.5673 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 3/30\n",
    "# 30/30 [==============================] - 197s 7s/step - loss: 0.2119 - bounding_box_loss: 0.0093 - class_label_loss: 0.2026 - bounding_box_accuracy: 0.6763 - class_label_accuracy: 0.9423 - val_loss: 0.2244 - val_bounding_box_loss: 0.0099 - val_class_label_loss: 0.2145 - val_bounding_box_accuracy: 0.6058 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 4/30\n",
    "# 30/30 [==============================] - 195s 7s/step - loss: 0.1949 - bounding_box_loss: 0.0079 - class_label_loss: 0.1869 - bounding_box_accuracy: 0.7083 - class_label_accuracy: 0.9530 - val_loss: 0.2177 - val_bounding_box_loss: 0.0084 - val_class_label_loss: 0.2093 - val_bounding_box_accuracy: 0.6538 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 5/30\n",
    "# 30/30 [==============================] - 197s 7s/step - loss: 0.1733 - bounding_box_loss: 0.0068 - class_label_loss: 0.1665 - bounding_box_accuracy: 0.7511 - class_label_accuracy: 0.9605 - val_loss: 0.2173 - val_bounding_box_loss: 0.0081 - val_class_label_loss: 0.2092 - val_bounding_box_accuracy: 0.6923 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 6/30\n",
    "# 30/30 [==============================] - 197s 7s/step - loss: 0.1713 - bounding_box_loss: 0.0058 - class_label_loss: 0.1655 - bounding_box_accuracy: 0.7874 - class_label_accuracy: 0.9594 - val_loss: 0.2502 - val_bounding_box_loss: 0.0078 - val_class_label_loss: 0.2425 - val_bounding_box_accuracy: 0.7885 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 7/30\n",
    "# 30/30 [==============================] - 196s 7s/step - loss: 0.1507 - bounding_box_loss: 0.0053 - class_label_loss: 0.1453 - bounding_box_accuracy: 0.7724 - class_label_accuracy: 0.9647 - val_loss: 0.2530 - val_bounding_box_loss: 0.0074 - val_class_label_loss: 0.2456 - val_bounding_box_accuracy: 0.7981 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 8/30\n",
    "# 30/30 [==============================] - 194s 6s/step - loss: 0.1283 - bounding_box_loss: 0.0045 - class_label_loss: 0.1239 - bounding_box_accuracy: 0.8237 - class_label_accuracy: 0.9690 - val_loss: 0.2266 - val_bounding_box_loss: 0.0074 - val_class_label_loss: 0.2192 - val_bounding_box_accuracy: 0.7404 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 9/30\n",
    "# 30/30 [==============================] - 194s 6s/step - loss: 0.1315 - bounding_box_loss: 0.0039 - class_label_loss: 0.1276 - bounding_box_accuracy: 0.8301 - class_label_accuracy: 0.9637 - val_loss: 0.2199 - val_bounding_box_loss: 0.0072 - val_class_label_loss: 0.2127 - val_bounding_box_accuracy: 0.7692 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 10/30\n",
    "# 30/30 [==============================] - 193s 6s/step - loss: 0.1225 - bounding_box_loss: 0.0036 - class_label_loss: 0.1189 - bounding_box_accuracy: 0.8333 - class_label_accuracy: 0.9712 - val_loss: 0.2436 - val_bounding_box_loss: 0.0073 - val_class_label_loss: 0.2363 - val_bounding_box_accuracy: 0.7788 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 11/30\n",
    "# 30/30 [==============================] - 193s 6s/step - loss: 0.1169 - bounding_box_loss: 0.0034 - class_label_loss: 0.1135 - bounding_box_accuracy: 0.8440 - class_label_accuracy: 0.9712 - val_loss: 0.2716 - val_bounding_box_loss: 0.0074 - val_class_label_loss: 0.2642 - val_bounding_box_accuracy: 0.7788 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 12/30\n",
    "# 30/30 [==============================] - 193s 6s/step - loss: 0.1046 - bounding_box_loss: 0.0033 - class_label_loss: 0.1013 - bounding_box_accuracy: 0.8429 - class_label_accuracy: 0.9701 - val_loss: 0.2577 - val_bounding_box_loss: 0.0071 - val_class_label_loss: 0.2506 - val_bounding_box_accuracy: 0.7788 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 13/30\n",
    "# 30/30 [==============================] - 193s 6s/step - loss: 0.1009 - bounding_box_loss: 0.0029 - class_label_loss: 0.0980 - bounding_box_accuracy: 0.8494 - class_label_accuracy: 0.9744 - val_loss: 0.3032 - val_bounding_box_loss: 0.0072 - val_class_label_loss: 0.2959 - val_bounding_box_accuracy: 0.7500 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 14/30\n",
    "# 30/30 [==============================] - 191s 6s/step - loss: 0.0837 - bounding_box_loss: 0.0028 - class_label_loss: 0.0808 - bounding_box_accuracy: 0.8387 - class_label_accuracy: 0.9786 - val_loss: 0.3264 - val_bounding_box_loss: 0.0070 - val_class_label_loss: 0.3194 - val_bounding_box_accuracy: 0.7500 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 15/30\n",
    "# 30/30 [==============================] - 191s 6s/step - loss: 0.0873 - bounding_box_loss: 0.0027 - class_label_loss: 0.0846 - bounding_box_accuracy: 0.8547 - class_label_accuracy: 0.9754 - val_loss: 0.3113 - val_bounding_box_loss: 0.0066 - val_class_label_loss: 0.3048 - val_bounding_box_accuracy: 0.7692 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 16/30\n",
    "# 30/30 [==============================] - 191s 6s/step - loss: 0.0723 - bounding_box_loss: 0.0022 - class_label_loss: 0.0701 - bounding_box_accuracy: 0.8729 - class_label_accuracy: 0.9786 - val_loss: 0.2913 - val_bounding_box_loss: 0.0070 - val_class_label_loss: 0.2844 - val_bounding_box_accuracy: 0.8077 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 17/30\n",
    "# 30/30 [==============================] - 188s 6s/step - loss: 0.0659 - bounding_box_loss: 0.0021 - class_label_loss: 0.0638 - bounding_box_accuracy: 0.8835 - class_label_accuracy: 0.9818 - val_loss: 0.3118 - val_bounding_box_loss: 0.0066 - val_class_label_loss: 0.3052 - val_bounding_box_accuracy: 0.7788 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 18/30\n",
    "# 30/30 [==============================] - 185s 6s/step - loss: 0.0690 - bounding_box_loss: 0.0020 - class_label_loss: 0.0670 - bounding_box_accuracy: 0.8878 - class_label_accuracy: 0.9829 - val_loss: 0.3304 - val_bounding_box_loss: 0.0072 - val_class_label_loss: 0.3233 - val_bounding_box_accuracy: 0.7885 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 19/30\n",
    "# 30/30 [==============================] - 185s 6s/step - loss: 0.0590 - bounding_box_loss: 0.0019 - class_label_loss: 0.0571 - bounding_box_accuracy: 0.8878 - class_label_accuracy: 0.9786 - val_loss: 0.3013 - val_bounding_box_loss: 0.0069 - val_class_label_loss: 0.2944 - val_bounding_box_accuracy: 0.7692 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 20/30\n",
    "# 30/30 [==============================] - 183s 6s/step - loss: 0.0518 - bounding_box_loss: 0.0017 - class_label_loss: 0.0501 - bounding_box_accuracy: 0.8932 - class_label_accuracy: 0.9850 - val_loss: 0.3263 - val_bounding_box_loss: 0.0073 - val_class_label_loss: 0.3190 - val_bounding_box_accuracy: 0.8077 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 21/30\n",
    "# 30/30 [==============================] - 183s 6s/step - loss: 0.0527 - bounding_box_loss: 0.0019 - class_label_loss: 0.0508 - bounding_box_accuracy: 0.8974 - class_label_accuracy: 0.9840 - val_loss: 0.3609 - val_bounding_box_loss: 0.0072 - val_class_label_loss: 0.3538 - val_bounding_box_accuracy: 0.7596 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 22/30\n",
    "# 30/30 [==============================] - 183s 6s/step - loss: 0.0596 - bounding_box_loss: 0.0018 - class_label_loss: 0.0577 - bounding_box_accuracy: 0.8964 - class_label_accuracy: 0.9829 - val_loss: 0.3325 - val_bounding_box_loss: 0.0070 - val_class_label_loss: 0.3255 - val_bounding_box_accuracy: 0.7692 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 23/30\n",
    "# 30/30 [==============================] - 181s 6s/step - loss: 0.0451 - bounding_box_loss: 0.0020 - class_label_loss: 0.0431 - bounding_box_accuracy: 0.8985 - class_label_accuracy: 0.9861 - val_loss: 0.3499 - val_bounding_box_loss: 0.0075 - val_class_label_loss: 0.3424 - val_bounding_box_accuracy: 0.7596 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 24/30\n",
    "# 30/30 [==============================] - 182s 6s/step - loss: 0.0502 - bounding_box_loss: 0.0016 - class_label_loss: 0.0487 - bounding_box_accuracy: 0.9006 - class_label_accuracy: 0.9872 - val_loss: 0.3511 - val_bounding_box_loss: 0.0074 - val_class_label_loss: 0.3438 - val_bounding_box_accuracy: 0.7500 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 25/30\n",
    "# 30/30 [==============================] - 182s 6s/step - loss: 0.0503 - bounding_box_loss: 0.0014 - class_label_loss: 0.0488 - bounding_box_accuracy: 0.9049 - class_label_accuracy: 0.9840 - val_loss: 0.3945 - val_bounding_box_loss: 0.0071 - val_class_label_loss: 0.3874 - val_bounding_box_accuracy: 0.7885 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 26/30\n",
    "# 30/30 [==============================] - 180s 6s/step - loss: 0.0471 - bounding_box_loss: 0.0013 - class_label_loss: 0.0458 - bounding_box_accuracy: 0.9167 - class_label_accuracy: 0.9829 - val_loss: 0.3746 - val_bounding_box_loss: 0.0072 - val_class_label_loss: 0.3674 - val_bounding_box_accuracy: 0.7885 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 27/30\n",
    "# 30/30 [==============================] - 180s 6s/step - loss: 0.0425 - bounding_box_loss: 0.0012 - class_label_loss: 0.0413 - bounding_box_accuracy: 0.9113 - class_label_accuracy: 0.9818 - val_loss: 0.3661 - val_bounding_box_loss: 0.0071 - val_class_label_loss: 0.3590 - val_bounding_box_accuracy: 0.7692 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 28/30\n",
    "# 30/30 [==============================] - 179s 6s/step - loss: 0.0458 - bounding_box_loss: 0.0012 - class_label_loss: 0.0446 - bounding_box_accuracy: 0.9092 - class_label_accuracy: 0.9840 - val_loss: 0.4432 - val_bounding_box_loss: 0.0070 - val_class_label_loss: 0.4362 - val_bounding_box_accuracy: 0.7212 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 29/30\n",
    "# 30/30 [==============================] - 179s 6s/step - loss: 0.0416 - bounding_box_loss: 0.0011 - class_label_loss: 0.0404 - bounding_box_accuracy: 0.9103 - class_label_accuracy: 0.9850 - val_loss: 0.4157 - val_bounding_box_loss: 0.0071 - val_class_label_loss: 0.4085 - val_bounding_box_accuracy: 0.7885 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 30/30\n",
    "# 30/30 [==============================] - 180s 6s/step - loss: 0.0337 - bounding_box_loss: 0.0011 - class_label_loss: 0.0326 - bounding_box_accuracy: 0.9156 - class_label_accuracy: 0.9882 - val_loss: 0.4238 - val_bounding_box_loss: 0.0071 - val_class_label_loss: 0.4167 - val_bounding_box_accuracy: 0.7404 - val_class_label_accuracy: 0.9231"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting loss for modeling training\n",
    "plot_loss(model_name='vgg16_model_v6', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training total loss during vgg16_model_v6 training: 0.03366561979055405\n",
    "# Final epoch training class label loss during vgg16_model_v6 training: 0.032574281096458435\n",
    "# Final epoch training bounding box loss during vgg16_model_v6 training: 0.0010913361329585314\n",
    "# Final epoch validation total loss during vgg16_model_v6 training: 0.42383772134780884\n",
    "# Final epoch validation class label loss during vgg16_model_v6 training: 0.41669419407844543\n",
    "# Final epoch validation bounding box loss during vgg16_model_v6 training: 0.007143549155443907"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting accuracy for modeling training\n",
    "plot_accuracy(model_name='vgg16_model_v6', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training class label accurracy during vgg16_model_v6 training: 0.9882478713989258\n",
    "# Final epoch validation class label accurracy during vgg16_model_v6 training: 0.9230769276618958"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating model for mAP metrics and inference time and storing result\n",
    "result_val = evaluate_val(model=vgg16_model_v6, model_name='vgg16_model_v6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC PASCAL mAP for vgg16_model_v6 validation data: 0.4204857051372528\n",
    "# VOC PASCAL mAP in all points for vgg16_model_v6 validation data: 0.4053434729576111\n",
    "# COCO mAP for vgg16_model_v6 validation data: 0.5525089502334595\n",
    "# Average inference time for vgg16_model_v6 validation data: 0.15712501223270708"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending results of model evaluation to results dataframe (df_results)\n",
    "df_results_val = df_results_val.append(result_val, ignore_index=True)\n",
    "df_results_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading network but leaving off output layers\n",
    "vgg16 = VGG16(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "# freezing all backbone layers during training so they are not updated\n",
    "vgg16.trainable = False\n",
    "\n",
    "# flattening the max-pooling output of network\n",
    "flatten = vgg16.output\n",
    "flat = Flatten()(flatten)\n",
    "\n",
    "# constructing an output layer to predict bounding box coordinates\n",
    "bbox_head1 = Dense(128, activation=\"relu\")(flat)\n",
    "bbox_head2 = Dropout(0.5)(bbox_head1)\n",
    "bbox_head3 = Dense(64, activation=\"relu\")(bbox_head2)\n",
    "bbox_head4 = Dense(32, activation=\"relu\")(bbox_head3)\n",
    "bbox_head_v7 = Dense(4, activation=\"sigmoid\", name=\"bounding_box\")(bbox_head4)\n",
    "\n",
    "# constructing an output layer to predict class labels\n",
    "class_head1 = Dense(64, activation=\"relu\")(flat)\n",
    "class_head2 = Dropout(0.5)(class_head1)\n",
    "class_head3 = Dense(32, activation=\"relu\")(class_head2)\n",
    "class_head4 = Dropout(0.5)(class_head3)\n",
    "class_head_v7 = Dense(2, activation=\"sigmoid\", name=\"class_label\")(class_head4)\n",
    "\n",
    "# creating a model that accepts an input image and outputs bounding box coordinates and a class label\n",
    "vgg16_model_v7 = Model(inputs=vgg16.input, outputs=(bbox_head_v7, class_head_v7))\n",
    "\n",
    "# creating dictionary for setting loss functions\n",
    "losses = {\"class_label\": \"binary_crossentropy\", \"bounding_box\": \"mean_squared_error\"}\n",
    "\n",
    "# creating dictionry for weighting loss functions\n",
    "loss_weights = {\"class_label\": 1.0, \"bounding_box\": 1.0}\n",
    "\n",
    "# initilizing an \"Adam\" optimizer with learning rate of 0.0001\n",
    "opt = Adam(learning_rate = 0.0001)\n",
    "\n",
    "# compiling model\n",
    "vgg16_model_v7.compile(loss=losses, optimizer=opt, metrics=[\"accuracy\"], loss_weights=loss_weights)\n",
    "print(vgg16_model_v7.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing a dictionary for our training outputs\n",
    "train_targets = {\"class_label\": labels_train, \"bounding_box\": bboxes_train}\n",
    "\n",
    "# constructing a dictionary for our test outputs\n",
    "val_targets = {\"class_label\": labels_val, \"bounding_box\": bboxes_val}\n",
    "\n",
    "# training model for class and bounding box predictions\n",
    "H = vgg16_model_v7.fit(x_train, train_targets, validation_data=(x_val, val_targets), batch_size=batch_size, epochs=50, verbose=1)\n",
    "\n",
    "# saving trained model\n",
    "vgg16_model_v7.save('../Models/vgg16_model_v7.h5', save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 1/50\n",
    "# 30/30 [==============================] - 159s 5s/step - loss: 0.5458 - bounding_box_loss: 0.0311 - class_label_loss: 0.5148 - bounding_box_accuracy: 0.5288 - class_label_accuracy: 0.7874 - val_loss: 0.2675 - val_bounding_box_loss: 0.0140 - val_class_label_loss: 0.2535 - val_bounding_box_accuracy: 0.3846 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 2/50\n",
    "# 30/30 [==============================] - 158s 5s/step - loss: 0.3884 - bounding_box_loss: 0.0189 - class_label_loss: 0.3695 - bounding_box_accuracy: 0.5406 - class_label_accuracy: 0.8910 - val_loss: 0.2250 - val_bounding_box_loss: 0.0125 - val_class_label_loss: 0.2125 - val_bounding_box_accuracy: 0.4904 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 3/50\n",
    "# 30/30 [==============================] - 159s 5s/step - loss: 0.3791 - bounding_box_loss: 0.0158 - class_label_loss: 0.3634 - bounding_box_accuracy: 0.5470 - class_label_accuracy: 0.9017 - val_loss: 0.2206 - val_bounding_box_loss: 0.0129 - val_class_label_loss: 0.2076 - val_bounding_box_accuracy: 0.4327 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 4/50\n",
    "# 30/30 [==============================] - 160s 5s/step - loss: 0.3209 - bounding_box_loss: 0.0148 - class_label_loss: 0.3062 - bounding_box_accuracy: 0.5577 - class_label_accuracy: 0.9167 - val_loss: 0.2184 - val_bounding_box_loss: 0.0109 - val_class_label_loss: 0.2076 - val_bounding_box_accuracy: 0.5385 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 5/50\n",
    "# 30/30 [==============================] - 162s 5s/step - loss: 0.2955 - bounding_box_loss: 0.0133 - class_label_loss: 0.2822 - bounding_box_accuracy: 0.5598 - class_label_accuracy: 0.9231 - val_loss: 0.2236 - val_bounding_box_loss: 0.0105 - val_class_label_loss: 0.2132 - val_bounding_box_accuracy: 0.4712 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 6/50\n",
    "# 30/30 [==============================] - 159s 5s/step - loss: 0.2941 - bounding_box_loss: 0.0122 - class_label_loss: 0.2819 - bounding_box_accuracy: 0.5716 - class_label_accuracy: 0.9380 - val_loss: 0.2131 - val_bounding_box_loss: 0.0100 - val_class_label_loss: 0.2030 - val_bounding_box_accuracy: 0.5192 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 7/50\n",
    "# 30/30 [==============================] - 160s 5s/step - loss: 0.2521 - bounding_box_loss: 0.0123 - class_label_loss: 0.2398 - bounding_box_accuracy: 0.5951 - class_label_accuracy: 0.9444 - val_loss: 0.2271 - val_bounding_box_loss: 0.0092 - val_class_label_loss: 0.2179 - val_bounding_box_accuracy: 0.6442 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 8/50\n",
    "# 30/30 [==============================] - 160s 5s/step - loss: 0.2808 - bounding_box_loss: 0.0115 - class_label_loss: 0.2693 - bounding_box_accuracy: 0.5641 - class_label_accuracy: 0.9391 - val_loss: 0.2237 - val_bounding_box_loss: 0.0089 - val_class_label_loss: 0.2148 - val_bounding_box_accuracy: 0.6154 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 9/50\n",
    "# 30/30 [==============================] - 157s 5s/step - loss: 0.2494 - bounding_box_loss: 0.0112 - class_label_loss: 0.2382 - bounding_box_accuracy: 0.6058 - class_label_accuracy: 0.9498 - val_loss: 0.2277 - val_bounding_box_loss: 0.0093 - val_class_label_loss: 0.2184 - val_bounding_box_accuracy: 0.5673 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 10/50\n",
    "# 30/30 [==============================] - 158s 5s/step - loss: 0.2571 - bounding_box_loss: 0.0105 - class_label_loss: 0.2466 - bounding_box_accuracy: 0.6122 - class_label_accuracy: 0.9391 - val_loss: 0.2319 - val_bounding_box_loss: 0.0087 - val_class_label_loss: 0.2232 - val_bounding_box_accuracy: 0.6346 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 11/50\n",
    "# 30/30 [==============================] - 156s 5s/step - loss: 0.2348 - bounding_box_loss: 0.0102 - class_label_loss: 0.2247 - bounding_box_accuracy: 0.6175 - class_label_accuracy: 0.9562 - val_loss: 0.2172 - val_bounding_box_loss: 0.0084 - val_class_label_loss: 0.2088 - val_bounding_box_accuracy: 0.6154 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 12/50\n",
    "# 30/30 [==============================] - 156s 5s/step - loss: 0.2232 - bounding_box_loss: 0.0099 - class_label_loss: 0.2133 - bounding_box_accuracy: 0.6453 - class_label_accuracy: 0.9551 - val_loss: 0.2158 - val_bounding_box_loss: 0.0082 - val_class_label_loss: 0.2076 - val_bounding_box_accuracy: 0.6538 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 13/50\n",
    "# 30/30 [==============================] - 155s 5s/step - loss: 0.2103 - bounding_box_loss: 0.0095 - class_label_loss: 0.2008 - bounding_box_accuracy: 0.6197 - class_label_accuracy: 0.9583 - val_loss: 0.2318 - val_bounding_box_loss: 0.0087 - val_class_label_loss: 0.2231 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 14/50\n",
    "# 30/30 [==============================] - 156s 5s/step - loss: 0.2094 - bounding_box_loss: 0.0093 - class_label_loss: 0.2001 - bounding_box_accuracy: 0.6346 - class_label_accuracy: 0.9551 - val_loss: 0.2457 - val_bounding_box_loss: 0.0080 - val_class_label_loss: 0.2377 - val_bounding_box_accuracy: 0.6442 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 15/50\n",
    "# 30/30 [==============================] - 155s 5s/step - loss: 0.1834 - bounding_box_loss: 0.0092 - class_label_loss: 0.1742 - bounding_box_accuracy: 0.6506 - class_label_accuracy: 0.9594 - val_loss: 0.2496 - val_bounding_box_loss: 0.0083 - val_class_label_loss: 0.2414 - val_bounding_box_accuracy: 0.5962 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 16/50\n",
    "# 30/30 [==============================] - 155s 5s/step - loss: 0.1714 - bounding_box_loss: 0.0092 - class_label_loss: 0.1623 - bounding_box_accuracy: 0.6303 - class_label_accuracy: 0.9637 - val_loss: 0.2539 - val_bounding_box_loss: 0.0080 - val_class_label_loss: 0.2459 - val_bounding_box_accuracy: 0.6154 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 17/50\n",
    "# 30/30 [==============================] - 154s 5s/step - loss: 0.1809 - bounding_box_loss: 0.0089 - class_label_loss: 0.1720 - bounding_box_accuracy: 0.6485 - class_label_accuracy: 0.9573 - val_loss: 0.2487 - val_bounding_box_loss: 0.0086 - val_class_label_loss: 0.2402 - val_bounding_box_accuracy: 0.6442 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 18/50\n",
    "# 30/30 [==============================] - 154s 5s/step - loss: 0.1514 - bounding_box_loss: 0.0089 - class_label_loss: 0.1425 - bounding_box_accuracy: 0.6656 - class_label_accuracy: 0.9712 - val_loss: 0.2574 - val_bounding_box_loss: 0.0078 - val_class_label_loss: 0.2496 - val_bounding_box_accuracy: 0.6538 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 19/50\n",
    "# 30/30 [==============================] - 154s 5s/step - loss: 0.1602 - bounding_box_loss: 0.0088 - class_label_loss: 0.1514 - bounding_box_accuracy: 0.6635 - class_label_accuracy: 0.9615 - val_loss: 0.2536 - val_bounding_box_loss: 0.0078 - val_class_label_loss: 0.2458 - val_bounding_box_accuracy: 0.7115 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 20/50\n",
    "# 30/30 [==============================] - 154s 5s/step - loss: 0.1650 - bounding_box_loss: 0.0083 - class_label_loss: 0.1567 - bounding_box_accuracy: 0.6549 - class_label_accuracy: 0.9583 - val_loss: 0.2518 - val_bounding_box_loss: 0.0085 - val_class_label_loss: 0.2433 - val_bounding_box_accuracy: 0.6442 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 21/50\n",
    "# 30/30 [==============================] - 154s 5s/step - loss: 0.1493 - bounding_box_loss: 0.0084 - class_label_loss: 0.1409 - bounding_box_accuracy: 0.6592 - class_label_accuracy: 0.9701 - val_loss: 0.2548 - val_bounding_box_loss: 0.0079 - val_class_label_loss: 0.2468 - val_bounding_box_accuracy: 0.6923 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 22/50\n",
    "# 30/30 [==============================] - 154s 5s/step - loss: 0.1380 - bounding_box_loss: 0.0083 - class_label_loss: 0.1297 - bounding_box_accuracy: 0.6656 - class_label_accuracy: 0.9722 - val_loss: 0.2770 - val_bounding_box_loss: 0.0080 - val_class_label_loss: 0.2690 - val_bounding_box_accuracy: 0.6731 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 23/50\n",
    "# 30/30 [==============================] - 153s 5s/step - loss: 0.1361 - bounding_box_loss: 0.0079 - class_label_loss: 0.1282 - bounding_box_accuracy: 0.6848 - class_label_accuracy: 0.9679 - val_loss: 0.2895 - val_bounding_box_loss: 0.0077 - val_class_label_loss: 0.2818 - val_bounding_box_accuracy: 0.6731 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 24/50\n",
    "# 30/30 [==============================] - 152s 5s/step - loss: 0.1244 - bounding_box_loss: 0.0078 - class_label_loss: 0.1166 - bounding_box_accuracy: 0.6976 - class_label_accuracy: 0.9733 - val_loss: 0.2977 - val_bounding_box_loss: 0.0074 - val_class_label_loss: 0.2902 - val_bounding_box_accuracy: 0.7212 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 25/50\n",
    "# 30/30 [==============================] - 153s 5s/step - loss: 0.1340 - bounding_box_loss: 0.0076 - class_label_loss: 0.1264 - bounding_box_accuracy: 0.7094 - class_label_accuracy: 0.9679 - val_loss: 0.2793 - val_bounding_box_loss: 0.0076 - val_class_label_loss: 0.2718 - val_bounding_box_accuracy: 0.7404 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 26/50\n",
    "# 30/30 [==============================] - 151s 5s/step - loss: 0.1165 - bounding_box_loss: 0.0080 - class_label_loss: 0.1085 - bounding_box_accuracy: 0.7083 - class_label_accuracy: 0.9679 - val_loss: 0.2931 - val_bounding_box_loss: 0.0075 - val_class_label_loss: 0.2857 - val_bounding_box_accuracy: 0.6731 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 27/50\n",
    "# 30/30 [==============================] - 153s 5s/step - loss: 0.1255 - bounding_box_loss: 0.0074 - class_label_loss: 0.1181 - bounding_box_accuracy: 0.7073 - class_label_accuracy: 0.9733 - val_loss: 0.2873 - val_bounding_box_loss: 0.0076 - val_class_label_loss: 0.2797 - val_bounding_box_accuracy: 0.7019 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 28/50\n",
    "# 30/30 [==============================] - 151s 5s/step - loss: 0.1187 - bounding_box_loss: 0.0070 - class_label_loss: 0.1116 - bounding_box_accuracy: 0.7329 - class_label_accuracy: 0.9722 - val_loss: 0.3421 - val_bounding_box_loss: 0.0077 - val_class_label_loss: 0.3344 - val_bounding_box_accuracy: 0.6827 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 29/50\n",
    "# 30/30 [==============================] - 151s 5s/step - loss: 0.0967 - bounding_box_loss: 0.0074 - class_label_loss: 0.0893 - bounding_box_accuracy: 0.6976 - class_label_accuracy: 0.9765 - val_loss: 0.3241 - val_bounding_box_loss: 0.0073 - val_class_label_loss: 0.3168 - val_bounding_box_accuracy: 0.7212 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 30/50\n",
    "# 30/30 [==============================] - 151s 5s/step - loss: 0.1128 - bounding_box_loss: 0.0074 - class_label_loss: 0.1054 - bounding_box_accuracy: 0.6976 - class_label_accuracy: 0.9765 - val_loss: 0.3229 - val_bounding_box_loss: 0.0076 - val_class_label_loss: 0.3153 - val_bounding_box_accuracy: 0.6827 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 31/50\n",
    "# 30/30 [==============================] - 152s 5s/step - loss: 0.1089 - bounding_box_loss: 0.0072 - class_label_loss: 0.1017 - bounding_box_accuracy: 0.6752 - class_label_accuracy: 0.9669 - val_loss: 0.3285 - val_bounding_box_loss: 0.0072 - val_class_label_loss: 0.3213 - val_bounding_box_accuracy: 0.6923 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 32/50\n",
    "# 30/30 [==============================] - 151s 5s/step - loss: 0.1075 - bounding_box_loss: 0.0072 - class_label_loss: 0.1003 - bounding_box_accuracy: 0.7105 - class_label_accuracy: 0.9669 - val_loss: 0.3215 - val_bounding_box_loss: 0.0070 - val_class_label_loss: 0.3145 - val_bounding_box_accuracy: 0.7404 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 33/50\n",
    "# 30/30 [==============================] - 151s 5s/step - loss: 0.1012 - bounding_box_loss: 0.0073 - class_label_loss: 0.0939 - bounding_box_accuracy: 0.7051 - class_label_accuracy: 0.9776 - val_loss: 0.3531 - val_bounding_box_loss: 0.0075 - val_class_label_loss: 0.3457 - val_bounding_box_accuracy: 0.7019 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 34/50\n",
    "# 30/30 [==============================] - 150s 5s/step - loss: 0.0993 - bounding_box_loss: 0.0067 - class_label_loss: 0.0925 - bounding_box_accuracy: 0.7062 - class_label_accuracy: 0.9744 - val_loss: 0.3247 - val_bounding_box_loss: 0.0075 - val_class_label_loss: 0.3172 - val_bounding_box_accuracy: 0.7115 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 35/50\n",
    "# 30/30 [==============================] - 150s 5s/step - loss: 0.0967 - bounding_box_loss: 0.0064 - class_label_loss: 0.0904 - bounding_box_accuracy: 0.7169 - class_label_accuracy: 0.9733 - val_loss: 0.3625 - val_bounding_box_loss: 0.0073 - val_class_label_loss: 0.3552 - val_bounding_box_accuracy: 0.7115 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 36/50\n",
    "# 30/30 [==============================] - 150s 5s/step - loss: 0.0902 - bounding_box_loss: 0.0068 - class_label_loss: 0.0834 - bounding_box_accuracy: 0.7532 - class_label_accuracy: 0.9776 - val_loss: 0.3599 - val_bounding_box_loss: 0.0069 - val_class_label_loss: 0.3530 - val_bounding_box_accuracy: 0.7019 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 37/50\n",
    "# 30/30 [==============================] - 151s 5s/step - loss: 0.0952 - bounding_box_loss: 0.0070 - class_label_loss: 0.0882 - bounding_box_accuracy: 0.7115 - class_label_accuracy: 0.9744 - val_loss: 0.3501 - val_bounding_box_loss: 0.0071 - val_class_label_loss: 0.3430 - val_bounding_box_accuracy: 0.7019 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 38/50\n",
    "# 30/30 [==============================] - 150s 5s/step - loss: 0.0905 - bounding_box_loss: 0.0065 - class_label_loss: 0.0840 - bounding_box_accuracy: 0.7318 - class_label_accuracy: 0.9786 - val_loss: 0.3685 - val_bounding_box_loss: 0.0072 - val_class_label_loss: 0.3613 - val_bounding_box_accuracy: 0.7115 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 39/50\n",
    "# 30/30 [==============================] - 150s 5s/step - loss: 0.0881 - bounding_box_loss: 0.0066 - class_label_loss: 0.0816 - bounding_box_accuracy: 0.7457 - class_label_accuracy: 0.9733 - val_loss: 0.3497 - val_bounding_box_loss: 0.0070 - val_class_label_loss: 0.3426 - val_bounding_box_accuracy: 0.7115 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 40/50\n",
    "# 30/30 [==============================] - 149s 5s/step - loss: 0.0828 - bounding_box_loss: 0.0066 - class_label_loss: 0.0762 - bounding_box_accuracy: 0.7350 - class_label_accuracy: 0.9765 - val_loss: 0.3685 - val_bounding_box_loss: 0.0075 - val_class_label_loss: 0.3611 - val_bounding_box_accuracy: 0.6731 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 41/50\n",
    "# 30/30 [==============================] - 151s 5s/step - loss: 0.0870 - bounding_box_loss: 0.0063 - class_label_loss: 0.0807 - bounding_box_accuracy: 0.7457 - class_label_accuracy: 0.9765 - val_loss: 0.3895 - val_bounding_box_loss: 0.0074 - val_class_label_loss: 0.3821 - val_bounding_box_accuracy: 0.6923 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 42/50\n",
    "# 30/30 [==============================] - 150s 5s/step - loss: 0.0841 - bounding_box_loss: 0.0064 - class_label_loss: 0.0777 - bounding_box_accuracy: 0.7650 - class_label_accuracy: 0.9765 - val_loss: 0.4081 - val_bounding_box_loss: 0.0072 - val_class_label_loss: 0.4008 - val_bounding_box_accuracy: 0.7115 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 43/50\n",
    "# 30/30 [==============================] - 151s 5s/step - loss: 0.0744 - bounding_box_loss: 0.0060 - class_label_loss: 0.0684 - bounding_box_accuracy: 0.7372 - class_label_accuracy: 0.9829 - val_loss: 0.4407 - val_bounding_box_loss: 0.0075 - val_class_label_loss: 0.4333 - val_bounding_box_accuracy: 0.7115 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 44/50\n",
    "# 30/30 [==============================] - 150s 5s/step - loss: 0.0889 - bounding_box_loss: 0.0061 - class_label_loss: 0.0828 - bounding_box_accuracy: 0.7436 - class_label_accuracy: 0.9765 - val_loss: 0.4180 - val_bounding_box_loss: 0.0067 - val_class_label_loss: 0.4113 - val_bounding_box_accuracy: 0.7019 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 45/50\n",
    "# 30/30 [==============================] - 150s 5s/step - loss: 0.0733 - bounding_box_loss: 0.0057 - class_label_loss: 0.0676 - bounding_box_accuracy: 0.7457 - class_label_accuracy: 0.9840 - val_loss: 0.4146 - val_bounding_box_loss: 0.0068 - val_class_label_loss: 0.4078 - val_bounding_box_accuracy: 0.7212 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 46/50\n",
    "# 30/30 [==============================] - 150s 5s/step - loss: 0.0748 - bounding_box_loss: 0.0059 - class_label_loss: 0.0688 - bounding_box_accuracy: 0.7361 - class_label_accuracy: 0.9786 - val_loss: 0.4331 - val_bounding_box_loss: 0.0070 - val_class_label_loss: 0.4261 - val_bounding_box_accuracy: 0.7212 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 47/50\n",
    "# 30/30 [==============================] - 151s 5s/step - loss: 0.0670 - bounding_box_loss: 0.0057 - class_label_loss: 0.0613 - bounding_box_accuracy: 0.7318 - class_label_accuracy: 0.9808 - val_loss: 0.4364 - val_bounding_box_loss: 0.0075 - val_class_label_loss: 0.4289 - val_bounding_box_accuracy: 0.7212 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 48/50\n",
    "# 30/30 [==============================] - 150s 5s/step - loss: 0.0681 - bounding_box_loss: 0.0061 - class_label_loss: 0.0621 - bounding_box_accuracy: 0.7553 - class_label_accuracy: 0.9797 - val_loss: 0.4606 - val_bounding_box_loss: 0.0073 - val_class_label_loss: 0.4532 - val_bounding_box_accuracy: 0.7212 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 49/50\n",
    "# 30/30 [==============================] - 151s 5s/step - loss: 0.0647 - bounding_box_loss: 0.0058 - class_label_loss: 0.0589 - bounding_box_accuracy: 0.7607 - class_label_accuracy: 0.9818 - val_loss: 0.4459 - val_bounding_box_loss: 0.0072 - val_class_label_loss: 0.4387 - val_bounding_box_accuracy: 0.7212 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 50/50\n",
    "# 30/30 [==============================] - 151s 5s/step - loss: 0.0690 - bounding_box_loss: 0.0058 - class_label_loss: 0.0632 - bounding_box_accuracy: 0.7543 - class_label_accuracy: 0.9754 - val_loss: 0.4574 - val_bounding_box_loss: 0.0072 - val_class_label_loss: 0.4502 - val_bounding_box_accuracy: 0.7404 - val_class_label_accuracy: 0.9231"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting loss for modeling training\n",
    "plot_loss(model_name='vgg16_model_v7', num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training total loss during vgg16_model_v7 training: 0.06902801245450974\n",
    "# Final epoch training class label loss during vgg16_model_v7 training: 0.0632222443819046\n",
    "# Final epoch training bounding box loss during vgg16_model_v7 training: 0.005805775988847017\n",
    "# Final epoch validation total loss during vgg16_model_v7 training: 0.45739099383354187\n",
    "# Final epoch validation class label loss during vgg16_model_v7 training: 0.4502268433570862\n",
    "# Final epoch validation bounding box loss during vgg16_model_v7 training: 0.007164067588746548"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting accuracy for modeling training\n",
    "plot_accuracy(model_name='vgg16_model_v7', num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training total loss during vgg16_model_v7 training: 0.06902801245450974\n",
    "# Final epoch training class label loss during vgg16_model_v7 training: 0.0632222443819046\n",
    "# Final epoch training bounding box loss during vgg16_model_v7 training: 0.005805775988847017\n",
    "# Final epoch validation total loss during vgg16_model_v7 training: 0.45739099383354187\n",
    "# Final epoch validation class label loss during vgg16_model_v7 training: 0.4502268433570862\n",
    "# Final epoch validation bounding box loss during vgg16_model_v7 training: 0.007164067588746548"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating model for mAP metrics and inference time and storing result\n",
    "result_val = evaluate_val(model=vgg16_model_v7, model_name='vgg16_model_v7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC PASCAL mAP for vgg16_model_v7 validation data: 0.3361135721206665\n",
    "# VOC PASCAL mAP in all points for vgg16_model_v7 validation data: 0.336334228515625\n",
    "# COCO mAP for vgg16_model_v7 validation data: 0.5313147306442261\n",
    "# Average inference time for vgg16_model_v7 validation data: 0.15242522954940796"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending results of model evaluation to results dataframe (df_results)\n",
    "df_results_val = df_results_val.append(result_val, ignore_index=True)\n",
    "df_results_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet152V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading network but leaving off output layers\n",
    "resnet152v2 = ResNet152V2(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "# freezing all VGG layers during training so they are not updated\n",
    "resnet152v2.trainable = False\n",
    "\n",
    "# flattening the max-pooling output of network\n",
    "flatten = resnet152v2.output\n",
    "flat = Flatten()(flatten)\n",
    "\n",
    "# constructing an output layer to predict bounding box coordinates\n",
    "bbox_head1 = Dense(128, activation=\"relu\")(flat)\n",
    "bbox_head2 = Dense(64, activation=\"relu\")(bbox_head1)\n",
    "bbox_head3 = Dense(32, activation=\"relu\")(bbox_head2)\n",
    "bbox_head_v1 = Dense(4, activation=\"sigmoid\", name=\"bounding_box\")(bbox_head3)\n",
    "\n",
    "# constructing an output layer to predict class labels\n",
    "class_head1 = Dense(512, activation=\"relu\")(flat)\n",
    "class_head2 = Dropout(0.5)(class_head1)\n",
    "class_head3 = Dense(512, activation=\"relu\")(class_head2)\n",
    "class_head4 = Dropout(0.5)(class_head3)\n",
    "class_head_v1 = Dense(2, activation=\"sigmoid\", name=\"class_label\")(class_head4)\n",
    "\n",
    "# creating a model that accepts an input image and outputs bounding box coordinates and a class label\n",
    "resnet152v2_model_v1 = Model(inputs=resnet152v2.input, outputs=(bbox_head_v1, class_head_v1))\n",
    "\n",
    "# creating dictionary for setting loss functions\n",
    "losses = {\"class_label\": \"binary_crossentropy\", \"bounding_box\": \"mean_squared_error\"}\n",
    "\n",
    "# creating dictionry for weighting loss functions\n",
    "loss_weights = {\"class_label\": 1.0, \"bounding_box\": 1.0}\n",
    "\n",
    "# initilizing an \"Adam\" optimizer with learning rate of 0.0001\n",
    "opt = Adam(learning_rate = 0.0001)\n",
    "\n",
    "# compiling model\n",
    "resnet152v2_model_v1.compile(loss=losses, optimizer=opt, metrics=[\"accuracy\"], loss_weights=loss_weights)\n",
    "print(resnet152v2_model_v1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing a dictionary for our training outputs\n",
    "train_targets = {\"class_label\": labels_train, \"bounding_box\": bboxes_train}\n",
    "\n",
    "# constructing a dictionary for our test outputs\n",
    "val_targets = {\"class_label\": labels_val, \"bounding_box\": bboxes_val}\n",
    "\n",
    "# training model for class and bounding box predictions\n",
    "H = resnet152v2_model_v1.fit(x_train, train_targets, validation_data=(x_val, val_targets), batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "\n",
    "# saving trained model\n",
    "resnet152v2_model_v1.save('../Models/resnet152v2_model_v1.h5', save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 1/30\n",
    "# 30/30 [==============================] - 124s 4s/step - loss: 0.4231 - bounding_box_loss: 0.0262 - class_label_loss: 0.3970 - bounding_box_accuracy: 0.6004 - class_label_accuracy: 0.9017 - val_loss: 0.4796 - val_bounding_box_loss: 0.0116 - val_class_label_loss: 0.4680 - val_bounding_box_accuracy: 0.6923 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 2/30\n",
    "# 30/30 [==============================] - 129s 4s/step - loss: 0.2633 - bounding_box_loss: 0.0096 - class_label_loss: 0.2537 - bounding_box_accuracy: 0.7105 - class_label_accuracy: 0.9594 - val_loss: 0.3800 - val_bounding_box_loss: 0.0112 - val_class_label_loss: 0.3687 - val_bounding_box_accuracy: 0.7308 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 3/30\n",
    "# 30/30 [==============================] - 130s 4s/step - loss: 0.1515 - bounding_box_loss: 0.0066 - class_label_loss: 0.1449 - bounding_box_accuracy: 0.7821 - class_label_accuracy: 0.9679 - val_loss: 0.4186 - val_bounding_box_loss: 0.0088 - val_class_label_loss: 0.4098 - val_bounding_box_accuracy: 0.6923 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 4/30\n",
    "# 30/30 [==============================] - 131s 4s/step - loss: 0.1033 - bounding_box_loss: 0.0047 - class_label_loss: 0.0986 - bounding_box_accuracy: 0.8120 - class_label_accuracy: 0.9754 - val_loss: 0.4315 - val_bounding_box_loss: 0.0086 - val_class_label_loss: 0.4229 - val_bounding_box_accuracy: 0.7596 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 5/30\n",
    "# 30/30 [==============================] - 131s 4s/step - loss: 0.0830 - bounding_box_loss: 0.0038 - class_label_loss: 0.0791 - bounding_box_accuracy: 0.8355 - class_label_accuracy: 0.9829 - val_loss: 0.4450 - val_bounding_box_loss: 0.0084 - val_class_label_loss: 0.4366 - val_bounding_box_accuracy: 0.7500 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 6/30\n",
    "# 30/30 [==============================] - 132s 4s/step - loss: 0.0600 - bounding_box_loss: 0.0033 - class_label_loss: 0.0567 - bounding_box_accuracy: 0.8355 - class_label_accuracy: 0.9882 - val_loss: 0.5116 - val_bounding_box_loss: 0.0077 - val_class_label_loss: 0.5039 - val_bounding_box_accuracy: 0.7308 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 7/30\n",
    "# 30/30 [==============================] - 132s 4s/step - loss: 0.0724 - bounding_box_loss: 0.0027 - class_label_loss: 0.0697 - bounding_box_accuracy: 0.8793 - class_label_accuracy: 0.9840 - val_loss: 0.4652 - val_bounding_box_loss: 0.0074 - val_class_label_loss: 0.4578 - val_bounding_box_accuracy: 0.7596 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 8/30\n",
    "# 30/30 [==============================] - 132s 4s/step - loss: 0.0540 - bounding_box_loss: 0.0025 - class_label_loss: 0.0515 - bounding_box_accuracy: 0.8750 - class_label_accuracy: 0.9904 - val_loss: 0.4848 - val_bounding_box_loss: 0.0078 - val_class_label_loss: 0.4770 - val_bounding_box_accuracy: 0.7500 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 9/30\n",
    "# 30/30 [==============================] - 133s 4s/step - loss: 0.0473 - bounding_box_loss: 0.0022 - class_label_loss: 0.0451 - bounding_box_accuracy: 0.8718 - class_label_accuracy: 0.9904 - val_loss: 0.5757 - val_bounding_box_loss: 0.0079 - val_class_label_loss: 0.5678 - val_bounding_box_accuracy: 0.7692 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 10/30\n",
    "# 30/30 [==============================] - 134s 4s/step - loss: 0.0841 - bounding_box_loss: 0.0020 - class_label_loss: 0.0821 - bounding_box_accuracy: 0.8878 - class_label_accuracy: 0.9861 - val_loss: 0.4713 - val_bounding_box_loss: 0.0077 - val_class_label_loss: 0.4637 - val_bounding_box_accuracy: 0.7500 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 11/30\n",
    "# 30/30 [==============================] - 134s 4s/step - loss: 0.0498 - bounding_box_loss: 0.0018 - class_label_loss: 0.0480 - bounding_box_accuracy: 0.9049 - class_label_accuracy: 0.9915 - val_loss: 0.5641 - val_bounding_box_loss: 0.0076 - val_class_label_loss: 0.5566 - val_bounding_box_accuracy: 0.7788 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 12/30\n",
    "# 30/30 [==============================] - 134s 4s/step - loss: 0.0341 - bounding_box_loss: 0.0015 - class_label_loss: 0.0326 - bounding_box_accuracy: 0.9241 - class_label_accuracy: 0.9925 - val_loss: 0.5486 - val_bounding_box_loss: 0.0078 - val_class_label_loss: 0.5408 - val_bounding_box_accuracy: 0.7981 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 13/30\n",
    "# 30/30 [==============================] - 133s 4s/step - loss: 0.0358 - bounding_box_loss: 0.0014 - class_label_loss: 0.0344 - bounding_box_accuracy: 0.9306 - class_label_accuracy: 0.9947 - val_loss: 0.5910 - val_bounding_box_loss: 0.0070 - val_class_label_loss: 0.5840 - val_bounding_box_accuracy: 0.7404 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 14/30\n",
    "# 30/30 [==============================] - 133s 4s/step - loss: 0.0197 - bounding_box_loss: 0.0013 - class_label_loss: 0.0184 - bounding_box_accuracy: 0.9209 - class_label_accuracy: 0.9968 - val_loss: 0.5920 - val_bounding_box_loss: 0.0074 - val_class_label_loss: 0.5846 - val_bounding_box_accuracy: 0.7692 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 15/30\n",
    "# 30/30 [==============================] - 134s 4s/step - loss: 0.0266 - bounding_box_loss: 0.0013 - class_label_loss: 0.0253 - bounding_box_accuracy: 0.9135 - class_label_accuracy: 0.9936 - val_loss: 0.6559 - val_bounding_box_loss: 0.0073 - val_class_label_loss: 0.6485 - val_bounding_box_accuracy: 0.7596 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 16/30\n",
    "# 30/30 [==============================] - 133s 4s/step - loss: 0.0291 - bounding_box_loss: 0.0012 - class_label_loss: 0.0279 - bounding_box_accuracy: 0.9263 - class_label_accuracy: 0.9936 - val_loss: 0.6040 - val_bounding_box_loss: 0.0074 - val_class_label_loss: 0.5966 - val_bounding_box_accuracy: 0.7404 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 17/30\n",
    "# 30/30 [==============================] - 134s 4s/step - loss: 0.0195 - bounding_box_loss: 0.0012 - class_label_loss: 0.0183 - bounding_box_accuracy: 0.9156 - class_label_accuracy: 0.9968 - val_loss: 0.7175 - val_bounding_box_loss: 0.0071 - val_class_label_loss: 0.7103 - val_bounding_box_accuracy: 0.7692 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 18/30\n",
    "# 30/30 [==============================] - 134s 4s/step - loss: 0.0237 - bounding_box_loss: 0.0012 - class_label_loss: 0.0225 - bounding_box_accuracy: 0.9135 - class_label_accuracy: 0.9957 - val_loss: 0.6481 - val_bounding_box_loss: 0.0087 - val_class_label_loss: 0.6394 - val_bounding_box_accuracy: 0.7885 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 19/30\n",
    "# 30/30 [==============================] - 135s 4s/step - loss: 0.0230 - bounding_box_loss: 0.0013 - class_label_loss: 0.0216 - bounding_box_accuracy: 0.9092 - class_label_accuracy: 0.9957 - val_loss: 0.7917 - val_bounding_box_loss: 0.0077 - val_class_label_loss: 0.7840 - val_bounding_box_accuracy: 0.7885 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 20/30\n",
    "# 30/30 [==============================] - 134s 4s/step - loss: 0.0288 - bounding_box_loss: 0.0013 - class_label_loss: 0.0276 - bounding_box_accuracy: 0.9156 - class_label_accuracy: 0.9936 - val_loss: 0.7578 - val_bounding_box_loss: 0.0080 - val_class_label_loss: 0.7498 - val_bounding_box_accuracy: 0.7692 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 21/30\n",
    "# 30/30 [==============================] - 134s 4s/step - loss: 0.0219 - bounding_box_loss: 0.0013 - class_label_loss: 0.0206 - bounding_box_accuracy: 0.9092 - class_label_accuracy: 0.9979 - val_loss: 0.7041 - val_bounding_box_loss: 0.0084 - val_class_label_loss: 0.6957 - val_bounding_box_accuracy: 0.7500 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 22/30\n",
    "# 30/30 [==============================] - 135s 4s/step - loss: 0.0161 - bounding_box_loss: 0.0014 - class_label_loss: 0.0147 - bounding_box_accuracy: 0.9135 - class_label_accuracy: 0.9979 - val_loss: 0.7290 - val_bounding_box_loss: 0.0076 - val_class_label_loss: 0.7215 - val_bounding_box_accuracy: 0.7885 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 23/30\n",
    "# 30/30 [==============================] - 134s 4s/step - loss: 0.0315 - bounding_box_loss: 0.0013 - class_label_loss: 0.0302 - bounding_box_accuracy: 0.9092 - class_label_accuracy: 0.9936 - val_loss: 0.8013 - val_bounding_box_loss: 0.0078 - val_class_label_loss: 0.7934 - val_bounding_box_accuracy: 0.7788 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 24/30\n",
    "# 30/30 [==============================] - 135s 4s/step - loss: 0.0226 - bounding_box_loss: 9.8014e-04 - class_label_loss: 0.0216 - bounding_box_accuracy: 0.9177 - class_label_accuracy: 0.9957 - val_loss: 0.6928 - val_bounding_box_loss: 0.0079 - val_class_label_loss: 0.6849 - val_bounding_box_accuracy: 0.7692 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 25/30\n",
    "# 30/30 [==============================] - 135s 4s/step - loss: 0.0410 - bounding_box_loss: 8.8700e-04 - class_label_loss: 0.0401 - bounding_box_accuracy: 0.9391 - class_label_accuracy: 0.9947 - val_loss: 0.7980 - val_bounding_box_loss: 0.0074 - val_class_label_loss: 0.7905 - val_bounding_box_accuracy: 0.7692 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 26/30\n",
    "# 30/30 [==============================] - 135s 5s/step - loss: 0.0152 - bounding_box_loss: 7.1827e-04 - class_label_loss: 0.0145 - bounding_box_accuracy: 0.9391 - class_label_accuracy: 0.9968 - val_loss: 0.7397 - val_bounding_box_loss: 0.0077 - val_class_label_loss: 0.7319 - val_bounding_box_accuracy: 0.7692 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 27/30\n",
    "# 30/30 [==============================] - 135s 5s/step - loss: 0.0089 - bounding_box_loss: 7.3241e-04 - class_label_loss: 0.0082 - bounding_box_accuracy: 0.9444 - class_label_accuracy: 0.9989 - val_loss: 0.7220 - val_bounding_box_loss: 0.0081 - val_class_label_loss: 0.7139 - val_bounding_box_accuracy: 0.7596 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 28/30\n",
    "# 30/30 [==============================] - 136s 5s/step - loss: 0.0211 - bounding_box_loss: 8.3555e-04 - class_label_loss: 0.0203 - bounding_box_accuracy: 0.9295 - class_label_accuracy: 0.9915 - val_loss: 0.7580 - val_bounding_box_loss: 0.0077 - val_class_label_loss: 0.7503 - val_bounding_box_accuracy: 0.7692 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 29/30\n",
    "# 30/30 [==============================] - 136s 5s/step - loss: 0.0306 - bounding_box_loss: 6.8853e-04 - class_label_loss: 0.0299 - bounding_box_accuracy: 0.9402 - class_label_accuracy: 0.9936 - val_loss: 0.8211 - val_bounding_box_loss: 0.0078 - val_class_label_loss: 0.8134 - val_bounding_box_accuracy: 0.7692 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 30/30\n",
    "# 30/30 [==============================] - 143s 5s/step - loss: 0.0100 - bounding_box_loss: 7.5168e-04 - class_label_loss: 0.0093 - bounding_box_accuracy: 0.9370 - class_label_accuracy: 0.9979 - val_loss: 0.7764 - val_bounding_box_loss: 0.0079 - val_class_label_loss: 0.7685 - val_bounding_box_accuracy: 0.7500 - val_class_label_accuracy: 0.9327"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting loss for modeling training\n",
    "plot_loss(model_name='resnet152v2_model_v1', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training total loss during resnet152v2_model_v1 training: 0.01004087459295988\n",
    "# Final epoch training class label loss during resnet152v2_model_v1 training: 0.009289196692407131\n",
    "# Final epoch training bounding box loss during resnet152v2_model_v1 training: 0.0007516787736676633\n",
    "# Final epoch validation total loss during resnet152v2_model_v1 training: 0.7764177918434143\n",
    "# Final epoch validation class label loss during resnet152v2_model_v1 training: 0.7685229182243347\n",
    "# Final epoch validation bounding box loss during resnet152v2_model_v1 training: 0.007894890382885933"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting accuracy for modeling training\n",
    "plot_accuracy(model_name='resnet152v2_model_v1', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training class label accurracy during resnet152v2_model_v1 training: 0.997863233089447\n",
    "# Final epoch validation class label accurracy during resnet152v2_model_v1 training: 0.932692289352417"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating model for mAP metrics and inference time and storing result\n",
    "result_val = evaluate_val(model=resnet152v2_model_v1, model_name='resnet152v2_model_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC PASCAL mAP for resnet152v2_model_v1 validation data: 0.42556232213974\n",
    "# VOC PASCAL mAP in all points for resnet152v2_model_v1 validation data: 0.42415618896484375\n",
    "# COCO mAP for resnet152v2_model_v1 validation data: 0.49584507942199707\n",
    "# Average inference time for resnet152v2_model_v1 validation data: 0.2587215854571416"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending results of model evaluation to results dataframe (df_results)\n",
    "df_results_val = df_results_val.append(result_val, ignore_index=True)\n",
    "df_results_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading network but leaving off output layers\n",
    "resnet152v2 = ResNet152V2(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "# freezing all VGG layers during training so they are not updated\n",
    "resnet152v2.trainable = False\n",
    "\n",
    "# flattening the max-pooling output of network\n",
    "flatten = resnet152v2.output\n",
    "flat = Flatten()(flatten)\n",
    "\n",
    "# constructing an output layer to predict bounding box coordinates\n",
    "bbox_head1 = Dense(128, activation=\"relu\")(flat)\n",
    "bbox_head2 = Dropout(0.5)(bbox_head1)\n",
    "bbox_head3 = Dense(64, activation=\"relu\")(bbox_head2)\n",
    "bbox_head4 = Dropout(0.5)(bbox_head3)\n",
    "bbox_head5 = Dense(32, activation=\"relu\")(bbox_head4)\n",
    "bbox_head6 = Dropout(0.5)(bbox_head5)\n",
    "bbox_head_v2 = Dense(4, activation=\"sigmoid\", name=\"bounding_box\")(bbox_head6)\n",
    "\n",
    "# constructing an output layer to predict class labels\n",
    "class_head1 = Dense(512, activation=\"relu\")(flat)\n",
    "class_head2 = Dropout(0.5)(class_head1)\n",
    "class_head3 = Dense(512, activation=\"relu\")(class_head2)\n",
    "class_head4 = Dropout(0.5)(class_head3)\n",
    "class_head5 = Dense(512, activation=\"relu\")(class_head4)\n",
    "class_head6 = Dropout(0.5)(class_head5)\n",
    "class_head_v2 = Dense(2, activation=\"sigmoid\", name=\"class_label\")(class_head6)\n",
    "\n",
    "# creating a model that accepts an input image and outputs bounding box coordinates and a class label\n",
    "resnet152v2_model_v2 = Model(inputs=resnet152v2.input, outputs=(bbox_head_v2, class_head_v2))\n",
    "\n",
    "# creating dictionary for setting loss functions\n",
    "losses = {\"class_label\": \"binary_crossentropy\", \"bounding_box\": \"mean_squared_error\"}\n",
    "\n",
    "# creating dictionry for weighting loss functions\n",
    "loss_weights = {\"class_label\": 1.0, \"bounding_box\": 1.0}\n",
    "\n",
    "# initilizing an \"Adam\" optimizer with learning rate of 0.0001\n",
    "opt = Adam(learning_rate = 0.0001)\n",
    "\n",
    "# compiling model\n",
    "resnet152v2_model_v2.compile(loss=losses, optimizer=opt, metrics=[\"accuracy\"], loss_weights=loss_weights)\n",
    "print(resnet152v2_model_v2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing a dictionary for our training outputs\n",
    "train_targets = {\"class_label\": labels_train, \"bounding_box\": bboxes_train}\n",
    "\n",
    "# constructing a dictionary for our test outputs\n",
    "val_targets = {\"class_label\": labels_val, \"bounding_box\": bboxes_val}\n",
    "\n",
    "# training model for class and bounding box predictions\n",
    "H = resnet152v2_model_v2.fit(x_train, train_targets, validation_data=(x_val, val_targets), batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "\n",
    "# saving trained model\n",
    "resnet152v2_model_v2.save('../Models/resnet152v2_model_v2.h5', save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 1/30\n",
    "# 30/30 [==============================] - 131s 4s/step - loss: 0.7379 - bounding_box_loss: 0.1659 - class_label_loss: 0.5720 - bounding_box_accuracy: 0.4509 - class_label_accuracy: 0.8259 - val_loss: 0.3808 - val_bounding_box_loss: 0.0486 - val_class_label_loss: 0.3322 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 2/30\n",
    "# 30/30 [==============================] - 136s 5s/step - loss: 0.4447 - bounding_box_loss: 0.1471 - class_label_loss: 0.2976 - bounding_box_accuracy: 0.4840 - class_label_accuracy: 0.9466 - val_loss: 0.3888 - val_bounding_box_loss: 0.0672 - val_class_label_loss: 0.3216 - val_bounding_box_accuracy: 0.6154 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 3/30\n",
    "# 30/30 [==============================] - 134s 4s/step - loss: 0.3142 - bounding_box_loss: 0.1138 - class_label_loss: 0.2004 - bounding_box_accuracy: 0.4701 - class_label_accuracy: 0.9637 - val_loss: 0.3903 - val_bounding_box_loss: 0.0544 - val_class_label_loss: 0.3359 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 4/30\n",
    "# 30/30 [==============================] - 136s 5s/step - loss: 0.2630 - bounding_box_loss: 0.1095 - class_label_loss: 0.1535 - bounding_box_accuracy: 0.5011 - class_label_accuracy: 0.9679 - val_loss: 0.3839 - val_bounding_box_loss: 0.0570 - val_class_label_loss: 0.3269 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 5/30\n",
    "# 30/30 [==============================] - 135s 4s/step - loss: 0.2486 - bounding_box_loss: 0.1005 - class_label_loss: 0.1481 - bounding_box_accuracy: 0.5182 - class_label_accuracy: 0.9669 - val_loss: 0.4019 - val_bounding_box_loss: 0.0465 - val_class_label_loss: 0.3555 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 6/30\n",
    "# 30/30 [==============================] - 137s 5s/step - loss: 0.2041 - bounding_box_loss: 0.0929 - class_label_loss: 0.1112 - bounding_box_accuracy: 0.5577 - class_label_accuracy: 0.9744 - val_loss: 0.4185 - val_bounding_box_loss: 0.0440 - val_class_label_loss: 0.3745 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 7/30\n",
    "# 30/30 [==============================] - 134s 4s/step - loss: 0.2032 - bounding_box_loss: 0.0898 - class_label_loss: 0.1134 - bounding_box_accuracy: 0.5363 - class_label_accuracy: 0.9786 - val_loss: 0.4445 - val_bounding_box_loss: 0.0312 - val_class_label_loss: 0.4133 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 8/30\n",
    "# 30/30 [==============================] - 135s 4s/step - loss: 0.1701 - bounding_box_loss: 0.0817 - class_label_loss: 0.0884 - bounding_box_accuracy: 0.5395 - class_label_accuracy: 0.9797 - val_loss: 0.4694 - val_bounding_box_loss: 0.0366 - val_class_label_loss: 0.4328 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 9/30\n",
    "# 30/30 [==============================] - 134s 4s/step - loss: 0.1516 - bounding_box_loss: 0.0771 - class_label_loss: 0.0746 - bounding_box_accuracy: 0.5630 - class_label_accuracy: 0.9882 - val_loss: 0.4976 - val_bounding_box_loss: 0.0286 - val_class_label_loss: 0.4690 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 10/30\n",
    "# 30/30 [==============================] - 134s 4s/step - loss: 0.1352 - bounding_box_loss: 0.0747 - class_label_loss: 0.0606 - bounding_box_accuracy: 0.5673 - class_label_accuracy: 0.9861 - val_loss: 0.5068 - val_bounding_box_loss: 0.0246 - val_class_label_loss: 0.4821 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 11/30\n",
    "# 30/30 [==============================] - 134s 4s/step - loss: 0.1375 - bounding_box_loss: 0.0749 - class_label_loss: 0.0627 - bounding_box_accuracy: 0.5694 - class_label_accuracy: 0.9850 - val_loss: 0.5512 - val_bounding_box_loss: 0.0205 - val_class_label_loss: 0.5306 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 12/30\n",
    "# 30/30 [==============================] - 138s 5s/step - loss: 0.0979 - bounding_box_loss: 0.0659 - class_label_loss: 0.0320 - bounding_box_accuracy: 0.5726 - class_label_accuracy: 0.9904 - val_loss: 0.6076 - val_bounding_box_loss: 0.0158 - val_class_label_loss: 0.5918 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 13/30\n",
    "# 30/30 [==============================] - 135s 4s/step - loss: 0.1010 - bounding_box_loss: 0.0632 - class_label_loss: 0.0378 - bounding_box_accuracy: 0.5630 - class_label_accuracy: 0.9925 - val_loss: 0.5863 - val_bounding_box_loss: 0.0185 - val_class_label_loss: 0.5678 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 14/30\n",
    "# 30/30 [==============================] - 135s 4s/step - loss: 0.1174 - bounding_box_loss: 0.0621 - class_label_loss: 0.0552 - bounding_box_accuracy: 0.5673 - class_label_accuracy: 0.9861 - val_loss: 0.5775 - val_bounding_box_loss: 0.0186 - val_class_label_loss: 0.5589 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 15/30\n",
    "# 30/30 [==============================] - 135s 5s/step - loss: 0.0832 - bounding_box_loss: 0.0564 - class_label_loss: 0.0268 - bounding_box_accuracy: 0.5705 - class_label_accuracy: 0.9925 - val_loss: 0.6219 - val_bounding_box_loss: 0.0179 - val_class_label_loss: 0.6040 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 16/30\n",
    "# 30/30 [==============================] - 137s 5s/step - loss: 0.0831 - bounding_box_loss: 0.0571 - class_label_loss: 0.0260 - bounding_box_accuracy: 0.5780 - class_label_accuracy: 0.9936 - val_loss: 0.6910 - val_bounding_box_loss: 0.0177 - val_class_label_loss: 0.6733 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 17/30\n",
    "# 30/30 [==============================] - 137s 5s/step - loss: 0.0827 - bounding_box_loss: 0.0534 - class_label_loss: 0.0293 - bounding_box_accuracy: 0.5833 - class_label_accuracy: 0.9915 - val_loss: 0.7351 - val_bounding_box_loss: 0.0146 - val_class_label_loss: 0.7205 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 18/30\n",
    "# 30/30 [==============================] - 137s 5s/step - loss: 0.0828 - bounding_box_loss: 0.0489 - class_label_loss: 0.0339 - bounding_box_accuracy: 0.5726 - class_label_accuracy: 0.9882 - val_loss: 0.7737 - val_bounding_box_loss: 0.0160 - val_class_label_loss: 0.7577 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 19/30\n",
    "# 30/30 [==============================] - 137s 5s/step - loss: 0.0953 - bounding_box_loss: 0.0504 - class_label_loss: 0.0449 - bounding_box_accuracy: 0.5812 - class_label_accuracy: 0.9882 - val_loss: 0.6536 - val_bounding_box_loss: 0.0156 - val_class_label_loss: 0.6380 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 20/30\n",
    "# 30/30 [==============================] - 136s 5s/step - loss: 0.0795 - bounding_box_loss: 0.0480 - class_label_loss: 0.0315 - bounding_box_accuracy: 0.5406 - class_label_accuracy: 0.9925 - val_loss: 0.6222 - val_bounding_box_loss: 0.0164 - val_class_label_loss: 0.6058 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 21/30\n",
    "# 30/30 [==============================] - 136s 5s/step - loss: 0.0792 - bounding_box_loss: 0.0446 - class_label_loss: 0.0346 - bounding_box_accuracy: 0.5694 - class_label_accuracy: 0.9915 - val_loss: 0.6589 - val_bounding_box_loss: 0.0147 - val_class_label_loss: 0.6442 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 22/30\n",
    "# 30/30 [==============================] - 136s 5s/step - loss: 0.0862 - bounding_box_loss: 0.0461 - class_label_loss: 0.0402 - bounding_box_accuracy: 0.5694 - class_label_accuracy: 0.9893 - val_loss: 0.7153 - val_bounding_box_loss: 0.0141 - val_class_label_loss: 0.7012 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 23/30\n",
    "# 30/30 [==============================] - 136s 5s/step - loss: 0.0594 - bounding_box_loss: 0.0429 - class_label_loss: 0.0166 - bounding_box_accuracy: 0.5801 - class_label_accuracy: 0.9968 - val_loss: 0.7364 - val_bounding_box_loss: 0.0148 - val_class_label_loss: 0.7215 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 24/30\n",
    "# 30/30 [==============================] - 135s 5s/step - loss: 0.0532 - bounding_box_loss: 0.0437 - class_label_loss: 0.0095 - bounding_box_accuracy: 0.5684 - class_label_accuracy: 0.9979 - val_loss: 0.8070 - val_bounding_box_loss: 0.0140 - val_class_label_loss: 0.7930 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 25/30\n",
    "# 30/30 [==============================] - 139s 5s/step - loss: 0.0750 - bounding_box_loss: 0.0414 - class_label_loss: 0.0336 - bounding_box_accuracy: 0.5502 - class_label_accuracy: 0.9936 - val_loss: 0.7918 - val_bounding_box_loss: 0.0142 - val_class_label_loss: 0.7776 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 26/30\n",
    "# 30/30 [==============================] - 137s 5s/step - loss: 0.0877 - bounding_box_loss: 0.0438 - class_label_loss: 0.0439 - bounding_box_accuracy: 0.5801 - class_label_accuracy: 0.9915 - val_loss: 0.8821 - val_bounding_box_loss: 0.0136 - val_class_label_loss: 0.8684 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 27/30\n",
    "# 30/30 [==============================] - 137s 5s/step - loss: 0.0627 - bounding_box_loss: 0.0396 - class_label_loss: 0.0231 - bounding_box_accuracy: 0.5897 - class_label_accuracy: 0.9936 - val_loss: 0.8499 - val_bounding_box_loss: 0.0142 - val_class_label_loss: 0.8357 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 28/30\n",
    "# 30/30 [==============================] - 137s 5s/step - loss: 0.0600 - bounding_box_loss: 0.0368 - class_label_loss: 0.0232 - bounding_box_accuracy: 0.5972 - class_label_accuracy: 0.9957 - val_loss: 0.9243 - val_bounding_box_loss: 0.0138 - val_class_label_loss: 0.9105 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 29/30\n",
    "# 30/30 [==============================] - 136s 5s/step - loss: 0.0638 - bounding_box_loss: 0.0363 - class_label_loss: 0.0276 - bounding_box_accuracy: 0.5833 - class_label_accuracy: 0.9947 - val_loss: 0.9275 - val_bounding_box_loss: 0.0145 - val_class_label_loss: 0.9129 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 30/30\n",
    "# 30/30 [==============================] - 136s 5s/step - loss: 0.0553 - bounding_box_loss: 0.0372 - class_label_loss: 0.0181 - bounding_box_accuracy: 0.5812 - class_label_accuracy: 0.9925 - val_loss: 0.9198 - val_bounding_box_loss: 0.0141 - val_class_label_loss: 0.9057 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9231"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting loss for modeling training\n",
    "plot_loss(model_name='resnet152v2_model_v2', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training total loss during resnet152v2_model_v2 training: 0.055264152586460114\n",
    "# Final epoch training class label loss during resnet152v2_model_v2 training: 0.018091373145580292\n",
    "# Final epoch training bounding box loss during resnet152v2_model_v2 training: 0.03717277944087982\n",
    "# Final epoch validation total loss during resnet152v2_model_v2 training: 0.9197996854782104\n",
    "# Final epoch validation class label loss during resnet152v2_model_v2 training: 0.9057289958000183\n",
    "# Final epoch validation bounding box loss during resnet152v2_model_v2 training: 0.014070725999772549"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting accuracy for modeling training\n",
    "plot_accuracy(model_name='resnet152v2_model_v2', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training class label accurracy during resnet152v2_model_v2 training: 0.992521345615387\n",
    "# Final epoch validation class label accurracy during resnet152v2_model_v2 training: 0.9230769276618958"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating model for mAP metrics and inference time and storing result\n",
    "result_val = evaluate_val(model=resnet152v2_model_v2, model_name='resnet152v2_model_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC PASCAL mAP for resnet152v2_model_v2 validation data: 0.1384262591600418\n",
    "# VOC PASCAL mAP in all points for resnet152v2_model_v2 validation data: 0.0756792202591896\n",
    "# COCO mAP for resnet152v2_model_v2 validation data: 0.338842511177063\n",
    "# Average inference time for resnet152v2_model_v2 validation data: 0.24269587718523467"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending results of model evaluation to results dataframe (df_results)\n",
    "df_results_val = df_results_val.append(result_val, ignore_index=True)\n",
    "df_results_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InceptionResNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading network but leaving off output layers\n",
    "inceptionresnetv2 = InceptionResNetV2(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "# freezing all VGG layers during training so they are not updated\n",
    "inceptionresnetv2.trainable = False\n",
    "\n",
    "# flattening the max-pooling output of network\n",
    "flatten = inceptionresnetv2.output\n",
    "flat = Flatten()(flatten)\n",
    "\n",
    "# constructing an output layer to predict bounding box coordinates\n",
    "bbox_head1 = Dense(128, activation=\"relu\")(flat)\n",
    "bbox_head2 = Dense(64, activation=\"relu\")(bbox_head1)\n",
    "bbox_head3 = Dense(32, activation=\"relu\")(bbox_head2)\n",
    "bbox_head_v1 = Dense(4, activation=\"sigmoid\", name=\"bounding_box\")(bbox_head3)\n",
    "\n",
    "# constructing an output layer to predict class labels\n",
    "class_head1 = Dense(512, activation=\"relu\")(flat)\n",
    "class_head2 = Dropout(0.5)(class_head1)\n",
    "class_head3 = Dense(512, activation=\"relu\")(class_head2)\n",
    "class_head4 = Dropout(0.5)(class_head3)\n",
    "class_head_v1 = Dense(2, activation=\"sigmoid\", name=\"class_label\")(class_head4)\n",
    "\n",
    "# creating a model that accepts an input image and outputs bounding box coordinates and a class label\n",
    "inceptionresnetv2_model_v1 = Model(inputs=inceptionresnetv2.input, outputs=(bbox_head_v1, class_head_v1))\n",
    "\n",
    "# creating dictionary for setting loss functions\n",
    "losses = {\"class_label\": \"binary_crossentropy\", \"bounding_box\": \"mean_squared_error\"}\n",
    "\n",
    "# creating dictionry for weighting loss functions\n",
    "loss_weights = {\"class_label\": 1.0, \"bounding_box\": 1.0}\n",
    "\n",
    "# initilizing an \"Adam\" optimizer with learning rate of 0.0001\n",
    "opt = Adam(learning_rate = 0.0001)\n",
    "\n",
    "# compiling model\n",
    "inceptionresnetv2_model_v1.compile(loss=losses, optimizer=opt, metrics=[\"accuracy\"], loss_weights=loss_weights)\n",
    "print(inceptionresnetv2_model_v1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing a dictionary for our training outputs\n",
    "train_targets = {\"class_label\": labels_train, \"bounding_box\": bboxes_train}\n",
    "\n",
    "# constructing a dictionary for our test outputs\n",
    "val_targets = {\"class_label\": labels_val, \"bounding_box\": bboxes_val}\n",
    "\n",
    "# training model for class and bounding box predictions\n",
    "H = inceptionresnetv2_model_v1.fit(x_train, train_targets, validation_data=(x_val, val_targets), batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "\n",
    "# saving trained model\n",
    "inceptionresnetv2_model_v1.save('../Models/inceptionresnetv2_model_v1.h5', save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 1/30\n",
    "# 30/30 [==============================] - 70s 2s/step - loss: 0.4446 - bounding_box_loss: 0.0325 - class_label_loss: 0.4121 - bounding_box_accuracy: 0.5897 - class_label_accuracy: 0.9006 - val_loss: 0.4371 - val_bounding_box_loss: 0.0160 - val_class_label_loss: 0.4211 - val_bounding_box_accuracy: 0.5865 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 2/30\n",
    "# 30/30 [==============================] - 68s 2s/step - loss: 0.2678 - bounding_box_loss: 0.0141 - class_label_loss: 0.2537 - bounding_box_accuracy: 0.6303 - class_label_accuracy: 0.9551 - val_loss: 0.3700 - val_bounding_box_loss: 0.0104 - val_class_label_loss: 0.3596 - val_bounding_box_accuracy: 0.5962 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 3/30\n",
    "# 30/30 [==============================] - 77s 3s/step - loss: 0.1809 - bounding_box_loss: 0.0090 - class_label_loss: 0.1719 - bounding_box_accuracy: 0.6976 - class_label_accuracy: 0.9626 - val_loss: 0.4461 - val_bounding_box_loss: 0.0087 - val_class_label_loss: 0.4374 - val_bounding_box_accuracy: 0.6538 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 4/30\n",
    "# 30/30 [==============================] - 78s 3s/step - loss: 0.1844 - bounding_box_loss: 0.0069 - class_label_loss: 0.1775 - bounding_box_accuracy: 0.7724 - class_label_accuracy: 0.9594 - val_loss: 0.3280 - val_bounding_box_loss: 0.0083 - val_class_label_loss: 0.3197 - val_bounding_box_accuracy: 0.6731 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 5/30\n",
    "# 30/30 [==============================] - 81s 3s/step - loss: 0.1391 - bounding_box_loss: 0.0060 - class_label_loss: 0.1331 - bounding_box_accuracy: 0.7767 - class_label_accuracy: 0.9679 - val_loss: 0.3518 - val_bounding_box_loss: 0.0083 - val_class_label_loss: 0.3435 - val_bounding_box_accuracy: 0.7019 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 6/30\n",
    "# 30/30 [==============================] - 81s 3s/step - loss: 0.1185 - bounding_box_loss: 0.0052 - class_label_loss: 0.1133 - bounding_box_accuracy: 0.7821 - class_label_accuracy: 0.9744 - val_loss: 0.3622 - val_bounding_box_loss: 0.0074 - val_class_label_loss: 0.3547 - val_bounding_box_accuracy: 0.6923 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 7/30\n",
    "# 30/30 [==============================] - 78s 3s/step - loss: 0.1088 - bounding_box_loss: 0.0049 - class_label_loss: 0.1040 - bounding_box_accuracy: 0.8002 - class_label_accuracy: 0.9786 - val_loss: 0.3656 - val_bounding_box_loss: 0.0077 - val_class_label_loss: 0.3579 - val_bounding_box_accuracy: 0.6635 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 8/30\n",
    "# 30/30 [==============================] - 76s 3s/step - loss: 0.0979 - bounding_box_loss: 0.0041 - class_label_loss: 0.0939 - bounding_box_accuracy: 0.7885 - class_label_accuracy: 0.9754 - val_loss: 0.3466 - val_bounding_box_loss: 0.0083 - val_class_label_loss: 0.3383 - val_bounding_box_accuracy: 0.7981 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 9/30\n",
    "# 30/30 [==============================] - 76s 3s/step - loss: 0.0926 - bounding_box_loss: 0.0040 - class_label_loss: 0.0886 - bounding_box_accuracy: 0.8013 - class_label_accuracy: 0.9776 - val_loss: 0.3661 - val_bounding_box_loss: 0.0067 - val_class_label_loss: 0.3594 - val_bounding_box_accuracy: 0.7404 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 10/30\n",
    "# 30/30 [==============================] - 77s 3s/step - loss: 0.0782 - bounding_box_loss: 0.0035 - class_label_loss: 0.0747 - bounding_box_accuracy: 0.8045 - class_label_accuracy: 0.9786 - val_loss: 0.4238 - val_bounding_box_loss: 0.0073 - val_class_label_loss: 0.4164 - val_bounding_box_accuracy: 0.7404 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 11/30\n",
    "# 30/30 [==============================] - 77s 3s/step - loss: 0.0743 - bounding_box_loss: 0.0035 - class_label_loss: 0.0708 - bounding_box_accuracy: 0.8066 - class_label_accuracy: 0.9786 - val_loss: 0.4391 - val_bounding_box_loss: 0.0068 - val_class_label_loss: 0.4323 - val_bounding_box_accuracy: 0.6731 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 12/30\n",
    "# 30/30 [==============================] - 79s 3s/step - loss: 0.0784 - bounding_box_loss: 0.0029 - class_label_loss: 0.0755 - bounding_box_accuracy: 0.8419 - class_label_accuracy: 0.9754 - val_loss: 0.3964 - val_bounding_box_loss: 0.0072 - val_class_label_loss: 0.3893 - val_bounding_box_accuracy: 0.7212 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 13/30\n",
    "# 30/30 [==============================] - 78s 3s/step - loss: 0.0610 - bounding_box_loss: 0.0026 - class_label_loss: 0.0584 - bounding_box_accuracy: 0.8355 - class_label_accuracy: 0.9808 - val_loss: 0.3930 - val_bounding_box_loss: 0.0071 - val_class_label_loss: 0.3859 - val_bounding_box_accuracy: 0.6827 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 14/30\n",
    "# 30/30 [==============================] - 77s 3s/step - loss: 0.0609 - bounding_box_loss: 0.0026 - class_label_loss: 0.0583 - bounding_box_accuracy: 0.8408 - class_label_accuracy: 0.9808 - val_loss: 0.4446 - val_bounding_box_loss: 0.0068 - val_class_label_loss: 0.4378 - val_bounding_box_accuracy: 0.6731 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 15/30\n",
    "# 30/30 [==============================] - 76s 3s/step - loss: 0.0535 - bounding_box_loss: 0.0025 - class_label_loss: 0.0510 - bounding_box_accuracy: 0.8088 - class_label_accuracy: 0.9829 - val_loss: 0.4624 - val_bounding_box_loss: 0.0064 - val_class_label_loss: 0.4560 - val_bounding_box_accuracy: 0.7115 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 16/30\n",
    "# 30/30 [==============================] - 77s 3s/step - loss: 0.0481 - bounding_box_loss: 0.0022 - class_label_loss: 0.0459 - bounding_box_accuracy: 0.8750 - class_label_accuracy: 0.9818 - val_loss: 0.4677 - val_bounding_box_loss: 0.0064 - val_class_label_loss: 0.4613 - val_bounding_box_accuracy: 0.7212 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 17/30\n",
    "# 30/30 [==============================] - 77s 3s/step - loss: 0.0567 - bounding_box_loss: 0.0019 - class_label_loss: 0.0548 - bounding_box_accuracy: 0.8547 - class_label_accuracy: 0.9818 - val_loss: 0.4493 - val_bounding_box_loss: 0.0071 - val_class_label_loss: 0.4422 - val_bounding_box_accuracy: 0.7115 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 18/30\n",
    "# 30/30 [==============================] - 78s 3s/step - loss: 0.0582 - bounding_box_loss: 0.0017 - class_label_loss: 0.0565 - bounding_box_accuracy: 0.8835 - class_label_accuracy: 0.9829 - val_loss: 0.4007 - val_bounding_box_loss: 0.0066 - val_class_label_loss: 0.3941 - val_bounding_box_accuracy: 0.7500 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 19/30\n",
    "# 30/30 [==============================] - 78s 3s/step - loss: 0.0579 - bounding_box_loss: 0.0015 - class_label_loss: 0.0564 - bounding_box_accuracy: 0.8793 - class_label_accuracy: 0.9829 - val_loss: 0.4479 - val_bounding_box_loss: 0.0067 - val_class_label_loss: 0.4413 - val_bounding_box_accuracy: 0.7404 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 20/30\n",
    "# 30/30 [==============================] - 78s 3s/step - loss: 0.0533 - bounding_box_loss: 0.0015 - class_label_loss: 0.0518 - bounding_box_accuracy: 0.8868 - class_label_accuracy: 0.9861 - val_loss: 0.4461 - val_bounding_box_loss: 0.0063 - val_class_label_loss: 0.4399 - val_bounding_box_accuracy: 0.7500 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 21/30\n",
    "# 30/30 [==============================] - 77s 3s/step - loss: 0.0543 - bounding_box_loss: 0.0015 - class_label_loss: 0.0528 - bounding_box_accuracy: 0.8846 - class_label_accuracy: 0.9840 - val_loss: 0.4572 - val_bounding_box_loss: 0.0064 - val_class_label_loss: 0.4508 - val_bounding_box_accuracy: 0.7212 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 22/30\n",
    "# 30/30 [==============================] - 78s 3s/step - loss: 0.0637 - bounding_box_loss: 0.0013 - class_label_loss: 0.0624 - bounding_box_accuracy: 0.8942 - class_label_accuracy: 0.9829 - val_loss: 0.4765 - val_bounding_box_loss: 0.0064 - val_class_label_loss: 0.4701 - val_bounding_box_accuracy: 0.7788 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 23/30\n",
    "# 30/30 [==============================] - 79s 3s/step - loss: 0.0491 - bounding_box_loss: 0.0012 - class_label_loss: 0.0479 - bounding_box_accuracy: 0.8996 - class_label_accuracy: 0.9850 - val_loss: 0.5830 - val_bounding_box_loss: 0.0063 - val_class_label_loss: 0.5767 - val_bounding_box_accuracy: 0.7308 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 24/30\n",
    "# 30/30 [==============================] - 78s 3s/step - loss: 0.0437 - bounding_box_loss: 0.0010 - class_label_loss: 0.0427 - bounding_box_accuracy: 0.9156 - class_label_accuracy: 0.9861 - val_loss: 0.5498 - val_bounding_box_loss: 0.0065 - val_class_label_loss: 0.5433 - val_bounding_box_accuracy: 0.7404 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 25/30\n",
    "# 30/30 [==============================] - 79s 3s/step - loss: 0.0420 - bounding_box_loss: 0.0012 - class_label_loss: 0.0408 - bounding_box_accuracy: 0.8857 - class_label_accuracy: 0.9893 - val_loss: 0.5624 - val_bounding_box_loss: 0.0064 - val_class_label_loss: 0.5560 - val_bounding_box_accuracy: 0.7596 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 26/30\n",
    "# 30/30 [==============================] - 78s 3s/step - loss: 0.0337 - bounding_box_loss: 0.0010 - class_label_loss: 0.0326 - bounding_box_accuracy: 0.9060 - class_label_accuracy: 0.9893 - val_loss: 0.5415 - val_bounding_box_loss: 0.0067 - val_class_label_loss: 0.5348 - val_bounding_box_accuracy: 0.7308 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 27/30\n",
    "# 30/30 [==============================] - 80s 3s/step - loss: 0.0348 - bounding_box_loss: 0.0011 - class_label_loss: 0.0337 - bounding_box_accuracy: 0.8921 - class_label_accuracy: 0.9850 - val_loss: 0.5395 - val_bounding_box_loss: 0.0063 - val_class_label_loss: 0.5332 - val_bounding_box_accuracy: 0.7404 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 28/30\n",
    "# 30/30 [==============================] - 80s 3s/step - loss: 0.0249 - bounding_box_loss: 8.8311e-04 - class_label_loss: 0.0241 - bounding_box_accuracy: 0.9071 - class_label_accuracy: 0.9915 - val_loss: 0.6172 - val_bounding_box_loss: 0.0062 - val_class_label_loss: 0.6109 - val_bounding_box_accuracy: 0.7212 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 29/30\n",
    "# 30/30 [==============================] - 80s 3s/step - loss: 0.0258 - bounding_box_loss: 8.7189e-04 - class_label_loss: 0.0250 - bounding_box_accuracy: 0.9081 - class_label_accuracy: 0.9904 - val_loss: 0.6146 - val_bounding_box_loss: 0.0063 - val_class_label_loss: 0.6084 - val_bounding_box_accuracy: 0.7308 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 30/30\n",
    "# 30/30 [==============================] - 80s 3s/step - loss: 0.0227 - bounding_box_loss: 8.1574e-04 - class_label_loss: 0.0219 - bounding_box_accuracy: 0.8964 - class_label_accuracy: 0.9925 - val_loss: 0.7013 - val_bounding_box_loss: 0.0067 - val_class_label_loss: 0.6947 - val_bounding_box_accuracy: 0.6827 - val_class_label_accuracy: 0.9231"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting loss for modeling training\n",
    "plot_loss(model_name='inceptionresnetv2_model_v1', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training total loss during inceptionresnetv2_model_v1 training: 0.022688426077365875\n",
    "# Final epoch training class label loss during inceptionresnetv2_model_v1 training: 0.021872680634260178\n",
    "# Final epoch training bounding box loss during inceptionresnetv2_model_v1 training: 0.0008157437550835311\n",
    "# Final epoch validation total loss during inceptionresnetv2_model_v1 training: 0.7013420462608337\n",
    "# Final epoch validation class label loss during inceptionresnetv2_model_v1 training: 0.6946655511856079\n",
    "# Final epoch validation bounding box loss during inceptionresnetv2_model_v1 training: 0.006676511839032173"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting accuracy for modeling training\n",
    "plot_accuracy(model_name='inceptionresnetv2_model_v1', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training class label accurracy during inceptionresnetv2_model_v1 training: 0.992521345615387\n",
    "# Final epoch validation class label accurracy during inceptionresnetv2_model_v1 training: 0.9230769276618958"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating model for mAP metrics and inference time and storing result\n",
    "result_val = evaluate_val(model=inceptionresnetv2_model_v1, model_name='inceptionresnetv2_model_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC PASCAL mAP for inceptionresnetv2_model_v1 validation data: 0.367081880569458\n",
    "# VOC PASCAL mAP in all points for inceptionresnetv2_model_v1 validation data: 0.37707239389419556\n",
    "# COCO mAP for inceptionresnetv2_model_v1 validation data: 0.47645172476768494\n",
    "# Average inference time for inceptionresnetv2_model_v1 validation data: 0.14871560151760393"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending results of model evaluation to results dataframe (df_results)\n",
    "df_results_val = df_results_val.append(result_val, ignore_index=True)\n",
    "df_results_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading network but leaving off output layers\n",
    "inceptionresnetv2 = InceptionResNetV2(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "# freezing all network layers during training so they are not updated\n",
    "inceptionresnetv2.trainable = False\n",
    "\n",
    "# flattening the max-pooling output of network\n",
    "flatten = inceptionresnetv2.output\n",
    "flat = Flatten()(flatten)\n",
    "\n",
    "# constructing an output layer to predict bounding box coordinates\n",
    "bbox_head1 = Dense(128, activation=\"relu\")(flat)\n",
    "bbox_head2 = Dropout(0.5)(bbox_head1)\n",
    "bbox_head3 = Dense(64, activation=\"relu\")(bbox_head2)\n",
    "bbox_head4 = Dropout(0.5)(bbox_head3)\n",
    "bbox_head5 = Dense(32, activation=\"relu\")(bbox_head4)\n",
    "bbox_head6 = Dropout(0.5)(bbox_head5)\n",
    "bbox_head_v2 = Dense(4, activation=\"sigmoid\", name=\"bounding_box\")(bbox_head6)\n",
    "\n",
    "# constructing an output layer to predict class labels\n",
    "class_head1 = Dense(512, activation=\"relu\")(flat)\n",
    "class_head2 = Dropout(0.5)(class_head1)\n",
    "class_head3 = Dense(512, activation=\"relu\")(class_head2)\n",
    "class_head4 = Dropout(0.5)(class_head3)\n",
    "class_head5 = Dense(512, activation=\"relu\")(class_head4)\n",
    "class_head6 = Dropout(0.5)(class_head5)\n",
    "class_head_v2 = Dense(2, activation=\"sigmoid\", name=\"class_label\")(class_head6)\n",
    "\n",
    "# creating a model that accepts an input image and outputs bounding box coordinates and a class label\n",
    "inceptionresnetv2_model_v2 = Model(inputs=inceptionresnetv2.input, outputs=(bbox_head_v2, class_head_v2))\n",
    "\n",
    "# creating dictionary for setting loss functions\n",
    "losses = {\"class_label\": \"binary_crossentropy\", \"bounding_box\": \"mean_squared_error\"}\n",
    "\n",
    "# creating dictionry for weighting loss functions\n",
    "loss_weights = {\"class_label\": 1.0, \"bounding_box\": 1.0}\n",
    "\n",
    "# initilizing an \"Adam\" optimizer with learning rate of 0.0001\n",
    "opt = Adam(learning_rate = 0.0001)\n",
    "\n",
    "# compiling model\n",
    "inceptionresnetv2_model_v2.compile(loss=losses, optimizer=opt, metrics=[\"accuracy\"], loss_weights=loss_weights)\n",
    "print(inceptionresnetv2_model_v2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# constructing a dictionary for our training outputs\n",
    "train_targets = {\"class_label\": labels_train, \"bounding_box\": bboxes_train}\n",
    "\n",
    "# constructing a dictionary for our test outputs\n",
    "val_targets = {\"class_label\": labels_val, \"bounding_box\": bboxes_val}\n",
    "\n",
    "# training model for class and bounding box predictions\n",
    "H = inceptionresnetv2_model_v2.fit(x_train, train_targets, validation_data=(x_val, val_targets), batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "\n",
    "# saving trained model\n",
    "inceptionresnetv2_model_v2.save('../Models/inceptionresnetv2_model_v2.h5', save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 1/30\n",
    "# 30/30 [==============================] - 70s 2s/step - loss: 0.6978 - bounding_box_loss: 0.1333 - class_label_loss: 0.5645 - bounding_box_accuracy: 0.3953 - class_label_accuracy: 0.8269 - val_loss: 0.3945 - val_bounding_box_loss: 0.0227 - val_class_label_loss: 0.3718 - val_bounding_box_accuracy: 0.5865 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 2/30\n",
    "# 30/30 [==============================] - 86s 3s/step - loss: 0.4020 - bounding_box_loss: 0.0993 - class_label_loss: 0.3027 - bounding_box_accuracy: 0.4583 - class_label_accuracy: 0.9348 - val_loss: 0.4044 - val_bounding_box_loss: 0.0206 - val_class_label_loss: 0.3838 - val_bounding_box_accuracy: 0.5192 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 3/30\n",
    "# 30/30 [==============================] - 88s 3s/step - loss: 0.3222 - bounding_box_loss: 0.0844 - class_label_loss: 0.2378 - bounding_box_accuracy: 0.4444 - class_label_accuracy: 0.9444 - val_loss: 0.3643 - val_bounding_box_loss: 0.0294 - val_class_label_loss: 0.3349 - val_bounding_box_accuracy: 0.5288 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 4/30\n",
    "# 30/30 [==============================] - 83s 3s/step - loss: 0.2673 - bounding_box_loss: 0.0750 - class_label_loss: 0.1923 - bounding_box_accuracy: 0.4594 - class_label_accuracy: 0.9637 - val_loss: 0.4043 - val_bounding_box_loss: 0.0330 - val_class_label_loss: 0.3713 - val_bounding_box_accuracy: 0.4808 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 5/30\n",
    "# 30/30 [==============================] - 82s 3s/step - loss: 0.2368 - bounding_box_loss: 0.0697 - class_label_loss: 0.1671 - bounding_box_accuracy: 0.4647 - class_label_accuracy: 0.9669 - val_loss: 0.3790 - val_bounding_box_loss: 0.0296 - val_class_label_loss: 0.3494 - val_bounding_box_accuracy: 0.5385 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 6/30\n",
    "# 30/30 [==============================] - 82s 3s/step - loss: 0.2357 - bounding_box_loss: 0.0665 - class_label_loss: 0.1692 - bounding_box_accuracy: 0.4530 - class_label_accuracy: 0.9679 - val_loss: 0.3498 - val_bounding_box_loss: 0.0197 - val_class_label_loss: 0.3301 - val_bounding_box_accuracy: 0.5865 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 7/30\n",
    "# 30/30 [==============================] - 82s 3s/step - loss: 0.2099 - bounding_box_loss: 0.0633 - class_label_loss: 0.1467 - bounding_box_accuracy: 0.4904 - class_label_accuracy: 0.9690 - val_loss: 0.3519 - val_bounding_box_loss: 0.0180 - val_class_label_loss: 0.3340 - val_bounding_box_accuracy: 0.4904 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 8/30\n",
    "# 30/30 [==============================] - 86s 3s/step - loss: 0.1892 - bounding_box_loss: 0.0591 - class_label_loss: 0.1302 - bounding_box_accuracy: 0.4893 - class_label_accuracy: 0.9669 - val_loss: 0.3649 - val_bounding_box_loss: 0.0174 - val_class_label_loss: 0.3475 - val_bounding_box_accuracy: 0.5769 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 9/30\n",
    "# 30/30 [==============================] - 79s 3s/step - loss: 0.1848 - bounding_box_loss: 0.0565 - class_label_loss: 0.1283 - bounding_box_accuracy: 0.4722 - class_label_accuracy: 0.9658 - val_loss: 0.3247 - val_bounding_box_loss: 0.0148 - val_class_label_loss: 0.3098 - val_bounding_box_accuracy: 0.3654 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 10/30\n",
    "# 30/30 [==============================] - 75s 3s/step - loss: 0.1638 - bounding_box_loss: 0.0547 - class_label_loss: 0.1091 - bounding_box_accuracy: 0.4701 - class_label_accuracy: 0.9744 - val_loss: 0.4193 - val_bounding_box_loss: 0.0210 - val_class_label_loss: 0.3983 - val_bounding_box_accuracy: 0.5385 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 11/30\n",
    "# 30/30 [==============================] - 73s 2s/step - loss: 0.1511 - bounding_box_loss: 0.0521 - class_label_loss: 0.0990 - bounding_box_accuracy: 0.5075 - class_label_accuracy: 0.9776 - val_loss: 0.3559 - val_bounding_box_loss: 0.0132 - val_class_label_loss: 0.3426 - val_bounding_box_accuracy: 0.3942 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 12/30\n",
    "# 30/30 [==============================] - 63s 2s/step - loss: 0.1375 - bounding_box_loss: 0.0519 - class_label_loss: 0.0856 - bounding_box_accuracy: 0.4936 - class_label_accuracy: 0.9786 - val_loss: 0.4316 - val_bounding_box_loss: 0.0171 - val_class_label_loss: 0.4145 - val_bounding_box_accuracy: 0.5481 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 13/30\n",
    "# 30/30 [==============================] - 65s 2s/step - loss: 0.1347 - bounding_box_loss: 0.0489 - class_label_loss: 0.0857 - bounding_box_accuracy: 0.4989 - class_label_accuracy: 0.9765 - val_loss: 0.4035 - val_bounding_box_loss: 0.0102 - val_class_label_loss: 0.3933 - val_bounding_box_accuracy: 0.6058 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 14/30\n",
    "# 30/30 [==============================] - 64s 2s/step - loss: 0.1277 - bounding_box_loss: 0.0446 - class_label_loss: 0.0831 - bounding_box_accuracy: 0.4818 - class_label_accuracy: 0.9776 - val_loss: 0.4290 - val_bounding_box_loss: 0.0128 - val_class_label_loss: 0.4161 - val_bounding_box_accuracy: 0.6442 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 15/30\n",
    "# 30/30 [==============================] - 65s 2s/step - loss: 0.1149 - bounding_box_loss: 0.0431 - class_label_loss: 0.0717 - bounding_box_accuracy: 0.4733 - class_label_accuracy: 0.9818 - val_loss: 0.4016 - val_bounding_box_loss: 0.0121 - val_class_label_loss: 0.3895 - val_bounding_box_accuracy: 0.4423 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 16/30\n",
    "# 30/30 [==============================] - 66s 2s/step - loss: 0.1001 - bounding_box_loss: 0.0429 - class_label_loss: 0.0571 - bounding_box_accuracy: 0.4989 - class_label_accuracy: 0.9786 - val_loss: 0.4521 - val_bounding_box_loss: 0.0105 - val_class_label_loss: 0.4415 - val_bounding_box_accuracy: 0.4712 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 17/30\n",
    "# 30/30 [==============================] - 66s 2s/step - loss: 0.0949 - bounding_box_loss: 0.0396 - class_label_loss: 0.0553 - bounding_box_accuracy: 0.4989 - class_label_accuracy: 0.9808 - val_loss: 0.4195 - val_bounding_box_loss: 0.0118 - val_class_label_loss: 0.4076 - val_bounding_box_accuracy: 0.5769 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 18/30\n",
    "# 30/30 [==============================] - 67s 2s/step - loss: 0.0835 - bounding_box_loss: 0.0382 - class_label_loss: 0.0453 - bounding_box_accuracy: 0.5118 - class_label_accuracy: 0.9840 - val_loss: 0.4359 - val_bounding_box_loss: 0.0126 - val_class_label_loss: 0.4233 - val_bounding_box_accuracy: 0.5192 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 19/30\n",
    "# 30/30 [==============================] - 73s 2s/step - loss: 0.0929 - bounding_box_loss: 0.0375 - class_label_loss: 0.0554 - bounding_box_accuracy: 0.5096 - class_label_accuracy: 0.9840 - val_loss: 0.4716 - val_bounding_box_loss: 0.0109 - val_class_label_loss: 0.4608 - val_bounding_box_accuracy: 0.5000 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 20/30\n",
    "# 30/30 [==============================] - 76s 3s/step - loss: 0.0875 - bounding_box_loss: 0.0391 - class_label_loss: 0.0484 - bounding_box_accuracy: 0.5118 - class_label_accuracy: 0.9861 - val_loss: 0.4430 - val_bounding_box_loss: 0.0117 - val_class_label_loss: 0.4312 - val_bounding_box_accuracy: 0.5962 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 21/30\n",
    "# 30/30 [==============================] - 80s 3s/step - loss: 0.0906 - bounding_box_loss: 0.0372 - class_label_loss: 0.0533 - bounding_box_accuracy: 0.5321 - class_label_accuracy: 0.9818 - val_loss: 0.4407 - val_bounding_box_loss: 0.0100 - val_class_label_loss: 0.4307 - val_bounding_box_accuracy: 0.6058 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 22/30\n",
    "# 30/30 [==============================] - 85s 3s/step - loss: 0.1017 - bounding_box_loss: 0.0347 - class_label_loss: 0.0670 - bounding_box_accuracy: 0.4722 - class_label_accuracy: 0.9786 - val_loss: 0.4504 - val_bounding_box_loss: 0.0112 - val_class_label_loss: 0.4392 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 23/30\n",
    "# 30/30 [==============================] - 86s 3s/step - loss: 0.0874 - bounding_box_loss: 0.0355 - class_label_loss: 0.0519 - bounding_box_accuracy: 0.4968 - class_label_accuracy: 0.9829 - val_loss: 0.4414 - val_bounding_box_loss: 0.0116 - val_class_label_loss: 0.4298 - val_bounding_box_accuracy: 0.4904 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 24/30\n",
    "# 30/30 [==============================] - 88s 3s/step - loss: 0.0833 - bounding_box_loss: 0.0331 - class_label_loss: 0.0501 - bounding_box_accuracy: 0.4850 - class_label_accuracy: 0.9829 - val_loss: 0.5240 - val_bounding_box_loss: 0.0104 - val_class_label_loss: 0.5136 - val_bounding_box_accuracy: 0.6058 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 25/30\n",
    "# 30/30 [==============================] - 91s 3s/step - loss: 0.0913 - bounding_box_loss: 0.0325 - class_label_loss: 0.0588 - bounding_box_accuracy: 0.5043 - class_label_accuracy: 0.9797 - val_loss: 0.5315 - val_bounding_box_loss: 0.0111 - val_class_label_loss: 0.5204 - val_bounding_box_accuracy: 0.5385 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 26/30\n",
    "# 30/30 [==============================] - 99s 3s/step - loss: 0.0704 - bounding_box_loss: 0.0312 - class_label_loss: 0.0393 - bounding_box_accuracy: 0.4936 - class_label_accuracy: 0.9872 - val_loss: 0.5451 - val_bounding_box_loss: 0.0126 - val_class_label_loss: 0.5325 - val_bounding_box_accuracy: 0.5962 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 27/30\n",
    "# 30/30 [==============================] - 104s 3s/step - loss: 0.0717 - bounding_box_loss: 0.0310 - class_label_loss: 0.0407 - bounding_box_accuracy: 0.5235 - class_label_accuracy: 0.9840 - val_loss: 0.4952 - val_bounding_box_loss: 0.0118 - val_class_label_loss: 0.4834 - val_bounding_box_accuracy: 0.6154 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 28/30\n",
    "# 30/30 [==============================] - 106s 4s/step - loss: 0.0764 - bounding_box_loss: 0.0314 - class_label_loss: 0.0450 - bounding_box_accuracy: 0.5288 - class_label_accuracy: 0.9850 - val_loss: 0.5355 - val_bounding_box_loss: 0.0120 - val_class_label_loss: 0.5235 - val_bounding_box_accuracy: 0.5962 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 29/30\n",
    "# 30/30 [==============================] - 79s 3s/step - loss: 0.0603 - bounding_box_loss: 0.0291 - class_label_loss: 0.0312 - bounding_box_accuracy: 0.5160 - class_label_accuracy: 0.9882 - val_loss: 0.5360 - val_bounding_box_loss: 0.0114 - val_class_label_loss: 0.5246 - val_bounding_box_accuracy: 0.6346 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 30/30\n",
    "# 30/30 [==============================] - 69s 2s/step - loss: 0.0588 - bounding_box_loss: 0.0320 - class_label_loss: 0.0268 - bounding_box_accuracy: 0.5288 - class_label_accuracy: 0.9915 - val_loss: 0.5937 - val_bounding_box_loss: 0.0124 - val_class_label_loss: 0.5813 - val_bounding_box_accuracy: 0.6635 - val_class_label_accuracy: 0.9231"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting loss for modeling training\n",
    "plot_loss(model_name='inceptionresnetv2_model_v2', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training total loss during inceptionresnetv2_model_v2 training: 0.058783989399671555\n",
    "# Final epoch training class label loss during inceptionresnetv2_model_v2 training: 0.026819711551070213\n",
    "# Final epoch training bounding box loss during inceptionresnetv2_model_v2 training: 0.031964272260665894\n",
    "# Final epoch validation total loss during inceptionresnetv2_model_v2 training: 0.593679666519165\n",
    "# Final epoch validation class label loss during inceptionresnetv2_model_v2 training: 0.5813268423080444\n",
    "# Final epoch validation bounding box loss during inceptionresnetv2_model_v2 training: 0.012352792546153069"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting accuracy for modeling training\n",
    "plot_accuracy(model_name='inceptionresnetv2_model_v2', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training class label accurracy during inceptionresnetv2_model_v2 training: 0.9914529919624329\n",
    "# Final epoch validation class label accurracy during inceptionresnetv2_model_v2 training: 0.9230769276618958"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating model for mAP metrics and inference time and storing result\n",
    "result_val = evaluate_val(model=inceptionresnetv2_model_v2, model_name='inceptionresnetv2_model_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC PASCAL mAP for inceptionresnetv2_model_v2 validation data: 0.2339620590209961\n",
    "# VOC PASCAL mAP in all points for inceptionresnetv2_model_v2 validation data: 0.19751280546188354\n",
    "# COCO mAP for inceptionresnetv2_model_v2 validation data: 0.40174946188926697\n",
    "# Average inference time for inceptionresnetv2_model_v2 validation data: 0.15381198892226586"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending results of model evaluation to results dataframe (df_results)\n",
    "df_results_val = df_results_val.append(result_val, ignore_index=True)\n",
    "df_results_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading network but leaving off output layers\n",
    "mobilenetv2 = MobileNetV2(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "# freezing all VGG layers during training so they are not updated\n",
    "mobilenetv2.trainable = False\n",
    "\n",
    "# flattening the max-pooling output of network\n",
    "flatten = mobilenetv2.output\n",
    "flat = Flatten()(flatten)\n",
    "\n",
    "# constructing an output layer to predict bounding box coordinates\n",
    "bbox_head1 = Dense(128, activation=\"relu\")(flat)\n",
    "bbox_head2 = Dense(64, activation=\"relu\")(bbox_head1)\n",
    "bbox_head3 = Dense(32, activation=\"relu\")(bbox_head2)\n",
    "bbox_head_v1 = Dense(4, activation=\"sigmoid\", name=\"bounding_box\")(bbox_head3)\n",
    "\n",
    "# constructing an output layer to predict class labels\n",
    "class_head1 = Dense(512, activation=\"relu\")(flat)\n",
    "class_head2 = Dropout(0.5)(class_head1)\n",
    "class_head3 = Dense(512, activation=\"relu\")(class_head2)\n",
    "class_head4 = Dropout(0.5)(class_head3)\n",
    "class_head_v1 = Dense(2, activation=\"sigmoid\", name=\"class_label\")(class_head4)\n",
    "\n",
    "# creating a model that accepts an input image and outputs bounding box coordinates and a class label\n",
    "mobilenetv2_model_v1 = Model(inputs=mobilenetv2.input, outputs=(bbox_head_v1, class_head_v1))\n",
    "\n",
    "# creating dictionary for setting loss functions\n",
    "losses = {\"class_label\": \"binary_crossentropy\", \"bounding_box\": \"mean_squared_error\"}\n",
    "\n",
    "# creating dictionry for weighting loss functions\n",
    "loss_weights = {\"class_label\": 1.0, \"bounding_box\": 1.0}\n",
    "\n",
    "# initilizing an \"Adam\" optimizer with learning rate of 0.0001\n",
    "opt = Adam(learning_rate = 0.0001)\n",
    "\n",
    "# compiling model\n",
    "mobilenetv2_model_v1.compile(loss=losses, optimizer=opt, metrics=[\"accuracy\"], loss_weights=loss_weights)\n",
    "print(mobilenetv2_model_v1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing a dictionary for our training outputs\n",
    "train_targets = {\"class_label\": labels_train, \"bounding_box\": bboxes_train}\n",
    "\n",
    "# constructing a dictionary for our test outputs\n",
    "val_targets = {\"class_label\": labels_val, \"bounding_box\": bboxes_val}\n",
    "\n",
    "# training model for class and bounding box predictions\n",
    "H = mobilenetv2_model_v1.fit(x_train, train_targets, validation_data=(x_val, val_targets), batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "\n",
    "# saving trained model\n",
    "mobilenetv2_model_v1.save('../Models/mobilenetv2_model_v1.h5', save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 1/30\n",
    "# 30/30 [==============================] - 20s 682ms/step - loss: 0.5270 - bounding_box_loss: 0.0433 - class_label_loss: 0.4836 - bounding_box_accuracy: 0.5459 - class_label_accuracy: 0.9092 - val_loss: 0.5651 - val_bounding_box_loss: 0.0149 - val_class_label_loss: 0.5502 - val_bounding_box_accuracy: 0.6154 - val_class_label_accuracy: 0.9135\n",
    "# Epoch 2/30\n",
    "# 30/30 [==============================] - 21s 716ms/step - loss: 0.3097 - bounding_box_loss: 0.0110 - class_label_loss: 0.2987 - bounding_box_accuracy: 0.6699 - class_label_accuracy: 0.9530 - val_loss: 0.4537 - val_bounding_box_loss: 0.0108 - val_class_label_loss: 0.4429 - val_bounding_box_accuracy: 0.6538 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 3/30\n",
    "# 30/30 [==============================] - 22s 744ms/step - loss: 0.2122 - bounding_box_loss: 0.0063 - class_label_loss: 0.2059 - bounding_box_accuracy: 0.7318 - class_label_accuracy: 0.9679 - val_loss: 0.3348 - val_bounding_box_loss: 0.0086 - val_class_label_loss: 0.3262 - val_bounding_box_accuracy: 0.6731 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 4/30\n",
    "# 30/30 [==============================] - 26s 854ms/step - loss: 0.1234 - bounding_box_loss: 0.0042 - class_label_loss: 0.1191 - bounding_box_accuracy: 0.8066 - class_label_accuracy: 0.9733 - val_loss: 0.3423 - val_bounding_box_loss: 0.0082 - val_class_label_loss: 0.3341 - val_bounding_box_accuracy: 0.7019 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 5/30\n",
    "# 30/30 [==============================] - 32s 1s/step - loss: 0.1182 - bounding_box_loss: 0.0031 - class_label_loss: 0.1151 - bounding_box_accuracy: 0.8130 - class_label_accuracy: 0.9722 - val_loss: 0.3094 - val_bounding_box_loss: 0.0092 - val_class_label_loss: 0.3002 - val_bounding_box_accuracy: 0.7019 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 6/30\n",
    "# 30/30 [==============================] - 35s 1s/step - loss: 0.1276 - bounding_box_loss: 0.0036 - class_label_loss: 0.1240 - bounding_box_accuracy: 0.8397 - class_label_accuracy: 0.9744 - val_loss: 0.3490 - val_bounding_box_loss: 0.0081 - val_class_label_loss: 0.3409 - val_bounding_box_accuracy: 0.7019 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 7/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.0925 - bounding_box_loss: 0.0028 - class_label_loss: 0.0897 - bounding_box_accuracy: 0.8536 - class_label_accuracy: 0.9818 - val_loss: 0.4355 - val_bounding_box_loss: 0.0080 - val_class_label_loss: 0.4275 - val_bounding_box_accuracy: 0.6923 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 8/30\n",
    "# 30/30 [==============================] - 29s 980ms/step - loss: 0.0776 - bounding_box_loss: 0.0024 - class_label_loss: 0.0752 - bounding_box_accuracy: 0.8611 - class_label_accuracy: 0.9808 - val_loss: 0.4996 - val_bounding_box_loss: 0.0087 - val_class_label_loss: 0.4909 - val_bounding_box_accuracy: 0.6731 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 9/30\n",
    "# 30/30 [==============================] - 28s 919ms/step - loss: 0.0683 - bounding_box_loss: 0.0020 - class_label_loss: 0.0662 - bounding_box_accuracy: 0.8547 - class_label_accuracy: 0.9850 - val_loss: 0.5862 - val_bounding_box_loss: 0.0070 - val_class_label_loss: 0.5792 - val_bounding_box_accuracy: 0.7212 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 10/30\n",
    "# 30/30 [==============================] - 28s 918ms/step - loss: 0.0838 - bounding_box_loss: 0.0015 - class_label_loss: 0.0822 - bounding_box_accuracy: 0.8793 - class_label_accuracy: 0.9797 - val_loss: 0.4612 - val_bounding_box_loss: 0.0082 - val_class_label_loss: 0.4530 - val_bounding_box_accuracy: 0.7308 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 11/30\n",
    "# 30/30 [==============================] - 28s 918ms/step - loss: 0.0793 - bounding_box_loss: 0.0015 - class_label_loss: 0.0778 - bounding_box_accuracy: 0.8654 - class_label_accuracy: 0.9829 - val_loss: 0.4444 - val_bounding_box_loss: 0.0074 - val_class_label_loss: 0.4370 - val_bounding_box_accuracy: 0.7212 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 12/30\n",
    "# 30/30 [==============================] - 29s 956ms/step - loss: 0.0799 - bounding_box_loss: 0.0012 - class_label_loss: 0.0788 - bounding_box_accuracy: 0.8814 - class_label_accuracy: 0.9829 - val_loss: 0.4565 - val_bounding_box_loss: 0.0077 - val_class_label_loss: 0.4487 - val_bounding_box_accuracy: 0.7788 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 13/30\n",
    "# 30/30 [==============================] - 29s 983ms/step - loss: 0.0513 - bounding_box_loss: 0.0013 - class_label_loss: 0.0500 - bounding_box_accuracy: 0.8942 - class_label_accuracy: 0.9872 - val_loss: 0.4783 - val_bounding_box_loss: 0.0074 - val_class_label_loss: 0.4709 - val_bounding_box_accuracy: 0.7115 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 14/30\n",
    "# 30/30 [==============================] - 30s 989ms/step - loss: 0.0484 - bounding_box_loss: 0.0011 - class_label_loss: 0.0472 - bounding_box_accuracy: 0.8974 - class_label_accuracy: 0.9872 - val_loss: 0.4696 - val_bounding_box_loss: 0.0078 - val_class_label_loss: 0.4618 - val_bounding_box_accuracy: 0.7404 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 15/30\n",
    "# 30/30 [==============================] - 31s 1s/step - loss: 0.0369 - bounding_box_loss: 0.0011 - class_label_loss: 0.0358 - bounding_box_accuracy: 0.9071 - class_label_accuracy: 0.9893 - val_loss: 0.4891 - val_bounding_box_loss: 0.0070 - val_class_label_loss: 0.4821 - val_bounding_box_accuracy: 0.7981 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 16/30\n",
    "# 30/30 [==============================] - 30s 1s/step - loss: 0.0402 - bounding_box_loss: 8.7831e-04 - class_label_loss: 0.0393 - bounding_box_accuracy: 0.9060 - class_label_accuracy: 0.9882 - val_loss: 0.4551 - val_bounding_box_loss: 0.0071 - val_class_label_loss: 0.4481 - val_bounding_box_accuracy: 0.7596 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 17/30\n",
    "# 30/30 [==============================] - 30s 990ms/step - loss: 0.0550 - bounding_box_loss: 0.0013 - class_label_loss: 0.0538 - bounding_box_accuracy: 0.8643 - class_label_accuracy: 0.9861 - val_loss: 0.4571 - val_bounding_box_loss: 0.0072 - val_class_label_loss: 0.4499 - val_bounding_box_accuracy: 0.6731 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 18/30\n",
    "# 30/30 [==============================] - 30s 989ms/step - loss: 0.0431 - bounding_box_loss: 0.0013 - class_label_loss: 0.0419 - bounding_box_accuracy: 0.8996 - class_label_accuracy: 0.9904 - val_loss: 0.5841 - val_bounding_box_loss: 0.0073 - val_class_label_loss: 0.5768 - val_bounding_box_accuracy: 0.7692 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 19/30\n",
    "# 30/30 [==============================] - 29s 976ms/step - loss: 0.0546 - bounding_box_loss: 0.0016 - class_label_loss: 0.0530 - bounding_box_accuracy: 0.8707 - class_label_accuracy: 0.9872 - val_loss: 0.4793 - val_bounding_box_loss: 0.0076 - val_class_label_loss: 0.4717 - val_bounding_box_accuracy: 0.7212 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 20/30\n",
    "# 30/30 [==============================] - 29s 967ms/step - loss: 0.0321 - bounding_box_loss: 0.0018 - class_label_loss: 0.0304 - bounding_box_accuracy: 0.8494 - class_label_accuracy: 0.9957 - val_loss: 0.5020 - val_bounding_box_loss: 0.0070 - val_class_label_loss: 0.4950 - val_bounding_box_accuracy: 0.7788 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 21/30\n",
    "# 30/30 [==============================] - 29s 963ms/step - loss: 0.0262 - bounding_box_loss: 0.0015 - class_label_loss: 0.0247 - bounding_box_accuracy: 0.8547 - class_label_accuracy: 0.9957 - val_loss: 0.5266 - val_bounding_box_loss: 0.0076 - val_class_label_loss: 0.5190 - val_bounding_box_accuracy: 0.6923 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 22/30\n",
    "# 30/30 [==============================] - 29s 966ms/step - loss: 0.0301 - bounding_box_loss: 0.0015 - class_label_loss: 0.0287 - bounding_box_accuracy: 0.8900 - class_label_accuracy: 0.9936 - val_loss: 0.6461 - val_bounding_box_loss: 0.0070 - val_class_label_loss: 0.6390 - val_bounding_box_accuracy: 0.7596 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 23/30\n",
    "# 30/30 [==============================] - 29s 973ms/step - loss: 0.0294 - bounding_box_loss: 0.0013 - class_label_loss: 0.0280 - bounding_box_accuracy: 0.8942 - class_label_accuracy: 0.9957 - val_loss: 0.6294 - val_bounding_box_loss: 0.0078 - val_class_label_loss: 0.6216 - val_bounding_box_accuracy: 0.7692 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 24/30\n",
    "# 30/30 [==============================] - 29s 965ms/step - loss: 0.0274 - bounding_box_loss: 0.0014 - class_label_loss: 0.0260 - bounding_box_accuracy: 0.8665 - class_label_accuracy: 0.9936 - val_loss: 0.6336 - val_bounding_box_loss: 0.0069 - val_class_label_loss: 0.6268 - val_bounding_box_accuracy: 0.7404 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 25/30\n",
    "# 30/30 [==============================] - 28s 950ms/step - loss: 0.0302 - bounding_box_loss: 0.0013 - class_label_loss: 0.0289 - bounding_box_accuracy: 0.8921 - class_label_accuracy: 0.9947 - val_loss: 0.5426 - val_bounding_box_loss: 0.0075 - val_class_label_loss: 0.5351 - val_bounding_box_accuracy: 0.7500 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 26/30\n",
    "# 30/30 [==============================] - 28s 944ms/step - loss: 0.0297 - bounding_box_loss: 0.0013 - class_label_loss: 0.0284 - bounding_box_accuracy: 0.8825 - class_label_accuracy: 0.9957 - val_loss: 0.6719 - val_bounding_box_loss: 0.0078 - val_class_label_loss: 0.6641 - val_bounding_box_accuracy: 0.7404 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 27/30\n",
    "# 30/30 [==============================] - 29s 955ms/step - loss: 0.0444 - bounding_box_loss: 0.0012 - class_label_loss: 0.0432 - bounding_box_accuracy: 0.9049 - class_label_accuracy: 0.9893 - val_loss: 0.6176 - val_bounding_box_loss: 0.0075 - val_class_label_loss: 0.6101 - val_bounding_box_accuracy: 0.7692 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 28/30\n",
    "# 30/30 [==============================] - 28s 944ms/step - loss: 0.0235 - bounding_box_loss: 0.0012 - class_label_loss: 0.0223 - bounding_box_accuracy: 0.8889 - class_label_accuracy: 0.9957 - val_loss: 0.5565 - val_bounding_box_loss: 0.0071 - val_class_label_loss: 0.5494 - val_bounding_box_accuracy: 0.7692 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 29/30\n",
    "# 30/30 [==============================] - 28s 948ms/step - loss: 0.0339 - bounding_box_loss: 0.0012 - class_label_loss: 0.0327 - bounding_box_accuracy: 0.8974 - class_label_accuracy: 0.9915 - val_loss: 0.6206 - val_bounding_box_loss: 0.0073 - val_class_label_loss: 0.6133 - val_bounding_box_accuracy: 0.7500 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 30/30\n",
    "# 30/30 [==============================] - 28s 926ms/step - loss: 0.0318 - bounding_box_loss: 0.0013 - class_label_loss: 0.0305 - bounding_box_accuracy: 0.8964 - class_label_accuracy: 0.9968 - val_loss: 0.6006 - val_bounding_box_loss: 0.0072 - val_class_label_loss: 0.5934 - val_bounding_box_accuracy: 0.7885 - val_class_label_accuracy: 0.9327"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting loss for modeling training\n",
    "plot_loss(model_name='mobilenetv2_model_v1', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training total loss during mobilenetv2_model_v1 training: 0.03179658204317093\n",
    "# Final epoch training class label loss during mobilenetv2_model_v1 training: 0.03053557500243187\n",
    "# Final epoch training bounding box loss during mobilenetv2_model_v1 training: 0.0012610058765858412\n",
    "# Final epoch validation total loss during mobilenetv2_model_v1 training: 0.600569486618042\n",
    "# Final epoch validation class label loss during mobilenetv2_model_v1 training: 0.5934180021286011\n",
    "# Final epoch validation bounding box loss during mobilenetv2_model_v1 training: 0.007151525001972914"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting accuracy for modeling training\n",
    "plot_accuracy(model_name='mobilenetv2_model_v1', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training class label accurracy during mobilenetv2_model_v1 training: 0.9967948794364929\n",
    "# Final epoch validation class label accurracy during mobilenetv2_model_v1 training: 0.932692289352417"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating model for mAP metrics and inference time and storing result\n",
    "result_val = evaluate_val(model=mobilenetv2_model_v1, model_name='mobilenetv2_model_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC PASCAL mAP for mobilenetv2_model_v1 validation data: 0.35191255807876587\n",
    "# VOC PASCAL mAP in all points for mobilenetv2_model_v1 validation data: 0.3447115421295166\n",
    "# COCO mAP for mobilenetv2_model_v1 validation data: 0.5264539122581482\n",
    "# Average inference time for mobilenetv2_model_v1 validation data: 0.07291219096917373"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending results of model evaluation to results dataframe (df_results)\n",
    "df_results_val = df_results_val.append(result_val, ignore_index=True)\n",
    "df_results_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading network but leaving off output layers\n",
    "mobilenetv2 = MobileNetV2(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "# freezing all VGG layers during training so they are not updated\n",
    "mobilenetv2.trainable = False\n",
    "\n",
    "# flattening the max-pooling output of network\n",
    "flatten = mobilenetv2.output\n",
    "flat = Flatten()(flatten)\n",
    "\n",
    "# constructing an output layer to predict bounding box coordinates\n",
    "bbox_head1 = Dense(128, activation=\"relu\")(flat)\n",
    "bbox_head2 = Dropout(0.5)(bbox_head1)\n",
    "bbox_head3 = Dense(64, activation=\"relu\")(bbox_head2)\n",
    "bbox_head4 = Dropout(0.5)(bbox_head3)\n",
    "bbox_head5 = Dense(32, activation=\"relu\")(bbox_head4)\n",
    "bbox_head6 = Dropout(0.5)(bbox_head5)\n",
    "bbox_head_v2 = Dense(4, activation=\"sigmoid\", name=\"bounding_box\")(bbox_head6)\n",
    "\n",
    "# constructing an output layer to predict class labels\n",
    "class_head1 = Dense(512, activation=\"relu\")(flat)\n",
    "class_head2 = Dropout(0.5)(class_head1)\n",
    "class_head3 = Dense(512, activation=\"relu\")(class_head2)\n",
    "class_head4 = Dropout(0.5)(class_head3)\n",
    "class_head5 = Dense(512, activation=\"relu\")(class_head4)\n",
    "class_head6 = Dropout(0.5)(class_head5)\n",
    "class_head_v2 = Dense(2, activation=\"sigmoid\", name=\"class_label\")(class_head6)\n",
    "\n",
    "# creating a model that accepts an input image and outputs bounding box coordinates and a class label\n",
    "mobilenetv2_model_v2 = Model(inputs=mobilenetv2.input, outputs=(bbox_head_v2, class_head_v2))\n",
    "\n",
    "# creating dictionary for setting loss functions\n",
    "losses = {\"class_label\": \"binary_crossentropy\", \"bounding_box\": \"mean_squared_error\"}\n",
    "\n",
    "# creating dictionry for weighting loss functions\n",
    "loss_weights = {\"class_label\": 1.0, \"bounding_box\": 1.0}\n",
    "\n",
    "# initilizing an \"Adam\" optimizer with learning rate of 0.0001\n",
    "opt = Adam(learning_rate = 0.0001)\n",
    "\n",
    "# compiling model\n",
    "mobilenetv2_model_v2.compile(loss=losses, optimizer=opt, metrics=[\"accuracy\"], loss_weights=loss_weights)\n",
    "print(mobilenetv2_model_v2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing a dictionary for our training outputs\n",
    "train_targets = {\"class_label\": labels_train, \"bounding_box\": bboxes_train}\n",
    "\n",
    "# constructing a dictionary for our test outputs\n",
    "val_targets = {\"class_label\": labels_val, \"bounding_box\": bboxes_val}\n",
    "\n",
    "# training model for class and bounding box predictions\n",
    "H = mobilenetv2_model_v2.fit(x_train, train_targets, validation_data=(x_val, val_targets), batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "\n",
    "# saving trained model\n",
    "mobilenetv2_model_v2.save('../Models/mobilenetv2_model_v2.h5', save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 1/30\n",
    "# 30/30 [==============================] - 20s 679ms/step - loss: 0.7065 - bounding_box_loss: 0.1091 - class_label_loss: 0.5974 - bounding_box_accuracy: 0.4402 - class_label_accuracy: 0.8355 - val_loss: 0.3859 - val_bounding_box_loss: 0.0471 - val_class_label_loss: 0.3388 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 2/30\n",
    "# 30/30 [==============================] - 21s 700ms/step - loss: 0.3969 - bounding_box_loss: 0.0927 - class_label_loss: 0.3042 - bounding_box_accuracy: 0.4412 - class_label_accuracy: 0.9423 - val_loss: 0.4305 - val_bounding_box_loss: 0.0474 - val_class_label_loss: 0.3831 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 3/30\n",
    "# 30/30 [==============================] - 22s 736ms/step - loss: 0.3858 - bounding_box_loss: 0.0891 - class_label_loss: 0.2967 - bounding_box_accuracy: 0.4605 - class_label_accuracy: 0.9498 - val_loss: 0.3441 - val_bounding_box_loss: 0.0475 - val_class_label_loss: 0.2966 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 4/30\n",
    "# 30/30 [==============================] - 22s 747ms/step - loss: 0.2832 - bounding_box_loss: 0.0821 - class_label_loss: 0.2011 - bounding_box_accuracy: 0.4936 - class_label_accuracy: 0.9519 - val_loss: 0.3727 - val_bounding_box_loss: 0.0455 - val_class_label_loss: 0.3272 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 5/30\n",
    "# 30/30 [==============================] - 26s 855ms/step - loss: 0.2148 - bounding_box_loss: 0.0784 - class_label_loss: 0.1364 - bounding_box_accuracy: 0.4626 - class_label_accuracy: 0.9701 - val_loss: 0.3710 - val_bounding_box_loss: 0.0463 - val_class_label_loss: 0.3246 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 6/30\n",
    "# 30/30 [==============================] - 31s 1s/step - loss: 0.2081 - bounding_box_loss: 0.0768 - class_label_loss: 0.1313 - bounding_box_accuracy: 0.4733 - class_label_accuracy: 0.9722 - val_loss: 0.4310 - val_bounding_box_loss: 0.0437 - val_class_label_loss: 0.3874 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 7/30\n",
    "# 30/30 [==============================] - 32s 1s/step - loss: 0.2088 - bounding_box_loss: 0.0731 - class_label_loss: 0.1356 - bounding_box_accuracy: 0.4701 - class_label_accuracy: 0.9701 - val_loss: 0.4096 - val_bounding_box_loss: 0.0362 - val_class_label_loss: 0.3734 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 8/30\n",
    "# 30/30 [==============================] - 30s 1s/step - loss: 0.1661 - bounding_box_loss: 0.0643 - class_label_loss: 0.1018 - bounding_box_accuracy: 0.4765 - class_label_accuracy: 0.9765 - val_loss: 0.3826 - val_bounding_box_loss: 0.0347 - val_class_label_loss: 0.3479 - val_bounding_box_accuracy: 0.3654 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 9/30\n",
    "# 30/30 [==============================] - 28s 922ms/step - loss: 0.1397 - bounding_box_loss: 0.0560 - class_label_loss: 0.0837 - bounding_box_accuracy: 0.4679 - class_label_accuracy: 0.9744 - val_loss: 0.3557 - val_bounding_box_loss: 0.0235 - val_class_label_loss: 0.3322 - val_bounding_box_accuracy: 0.3654 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 10/30\n",
    "# 30/30 [==============================] - 26s 882ms/step - loss: 0.1690 - bounding_box_loss: 0.0593 - class_label_loss: 0.1097 - bounding_box_accuracy: 0.4295 - class_label_accuracy: 0.9722 - val_loss: 0.4106 - val_bounding_box_loss: 0.0214 - val_class_label_loss: 0.3891 - val_bounding_box_accuracy: 0.4135 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 11/30\n",
    "# 30/30 [==============================] - 26s 881ms/step - loss: 0.1317 - bounding_box_loss: 0.0526 - class_label_loss: 0.0791 - bounding_box_accuracy: 0.4487 - class_label_accuracy: 0.9786 - val_loss: 0.4077 - val_bounding_box_loss: 0.0203 - val_class_label_loss: 0.3874 - val_bounding_box_accuracy: 0.3942 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 12/30\n",
    "# 30/30 [==============================] - 26s 881ms/step - loss: 0.1299 - bounding_box_loss: 0.0466 - class_label_loss: 0.0833 - bounding_box_accuracy: 0.4850 - class_label_accuracy: 0.9829 - val_loss: 0.4519 - val_bounding_box_loss: 0.0173 - val_class_label_loss: 0.4345 - val_bounding_box_accuracy: 0.3654 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 13/30\n",
    "# 30/30 [==============================] - 27s 892ms/step - loss: 0.1158 - bounding_box_loss: 0.0452 - class_label_loss: 0.0706 - bounding_box_accuracy: 0.4626 - class_label_accuracy: 0.9808 - val_loss: 0.4471 - val_bounding_box_loss: 0.0162 - val_class_label_loss: 0.4308 - val_bounding_box_accuracy: 0.3846 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 14/30\n",
    "# 30/30 [==============================] - 28s 917ms/step - loss: 0.1074 - bounding_box_loss: 0.0450 - class_label_loss: 0.0624 - bounding_box_accuracy: 0.4412 - class_label_accuracy: 0.9850 - val_loss: 0.4660 - val_bounding_box_loss: 0.0172 - val_class_label_loss: 0.4489 - val_bounding_box_accuracy: 0.3846 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 15/30\n",
    "# 30/30 [==============================] - 28s 920ms/step - loss: 0.1062 - bounding_box_loss: 0.0420 - class_label_loss: 0.0642 - bounding_box_accuracy: 0.4637 - class_label_accuracy: 0.9818 - val_loss: 0.3883 - val_bounding_box_loss: 0.0171 - val_class_label_loss: 0.3712 - val_bounding_box_accuracy: 0.3846 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 16/30\n",
    "# 30/30 [==============================] - 27s 908ms/step - loss: 0.0967 - bounding_box_loss: 0.0392 - class_label_loss: 0.0575 - bounding_box_accuracy: 0.5107 - class_label_accuracy: 0.9840 - val_loss: 0.5317 - val_bounding_box_loss: 0.0170 - val_class_label_loss: 0.5147 - val_bounding_box_accuracy: 0.4135 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 17/30\n",
    "# 30/30 [==============================] - 27s 910ms/step - loss: 0.0974 - bounding_box_loss: 0.0437 - class_label_loss: 0.0537 - bounding_box_accuracy: 0.4829 - class_label_accuracy: 0.9861 - val_loss: 0.4724 - val_bounding_box_loss: 0.0160 - val_class_label_loss: 0.4564 - val_bounding_box_accuracy: 0.4038 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 18/30\n",
    "# 30/30 [==============================] - 28s 917ms/step - loss: 0.0708 - bounding_box_loss: 0.0392 - class_label_loss: 0.0315 - bounding_box_accuracy: 0.4701 - class_label_accuracy: 0.9947 - val_loss: 0.5897 - val_bounding_box_loss: 0.0169 - val_class_label_loss: 0.5728 - val_bounding_box_accuracy: 0.3654 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 19/30\n",
    "# 30/30 [==============================] - 27s 900ms/step - loss: 0.0546 - bounding_box_loss: 0.0362 - class_label_loss: 0.0184 - bounding_box_accuracy: 0.4818 - class_label_accuracy: 0.9925 - val_loss: 0.5027 - val_bounding_box_loss: 0.0170 - val_class_label_loss: 0.4857 - val_bounding_box_accuracy: 0.3654 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 20/30\n",
    "# 30/30 [==============================] - 28s 933ms/step - loss: 0.0752 - bounding_box_loss: 0.0373 - class_label_loss: 0.0379 - bounding_box_accuracy: 0.4605 - class_label_accuracy: 0.9957 - val_loss: 0.5599 - val_bounding_box_loss: 0.0166 - val_class_label_loss: 0.5433 - val_bounding_box_accuracy: 0.3846 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 21/30\n",
    "# 30/30 [==============================] - 28s 917ms/step - loss: 0.0858 - bounding_box_loss: 0.0378 - class_label_loss: 0.0479 - bounding_box_accuracy: 0.4861 - class_label_accuracy: 0.9882 - val_loss: 0.5187 - val_bounding_box_loss: 0.0174 - val_class_label_loss: 0.5013 - val_bounding_box_accuracy: 0.3654 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 22/30\n",
    "# 30/30 [==============================] - 27s 906ms/step - loss: 0.0684 - bounding_box_loss: 0.0366 - class_label_loss: 0.0318 - bounding_box_accuracy: 0.4915 - class_label_accuracy: 0.9925 - val_loss: 0.5219 - val_bounding_box_loss: 0.0160 - val_class_label_loss: 0.5059 - val_bounding_box_accuracy: 0.4135 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 23/30\n",
    "# 30/30 [==============================] - 27s 891ms/step - loss: 0.0710 - bounding_box_loss: 0.0357 - class_label_loss: 0.0353 - bounding_box_accuracy: 0.4904 - class_label_accuracy: 0.9936 - val_loss: 0.6025 - val_bounding_box_loss: 0.0176 - val_class_label_loss: 0.5850 - val_bounding_box_accuracy: 0.3654 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 24/30\n",
    "# 30/30 [==============================] - 27s 915ms/step - loss: 0.0682 - bounding_box_loss: 0.0325 - class_label_loss: 0.0357 - bounding_box_accuracy: 0.4541 - class_label_accuracy: 0.9861 - val_loss: 0.4939 - val_bounding_box_loss: 0.0187 - val_class_label_loss: 0.4752 - val_bounding_box_accuracy: 0.3654 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 25/30\n",
    "# 30/30 [==============================] - 27s 889ms/step - loss: 0.0598 - bounding_box_loss: 0.0339 - class_label_loss: 0.0259 - bounding_box_accuracy: 0.4850 - class_label_accuracy: 0.9936 - val_loss: 0.5678 - val_bounding_box_loss: 0.0175 - val_class_label_loss: 0.5503 - val_bounding_box_accuracy: 0.4135 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 26/30\n",
    "# 30/30 [==============================] - 26s 875ms/step - loss: 0.0590 - bounding_box_loss: 0.0304 - class_label_loss: 0.0286 - bounding_box_accuracy: 0.4968 - class_label_accuracy: 0.9957 - val_loss: 0.6365 - val_bounding_box_loss: 0.0154 - val_class_label_loss: 0.6210 - val_bounding_box_accuracy: 0.4327 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 27/30\n",
    "# 30/30 [==============================] - 27s 885ms/step - loss: 0.0579 - bounding_box_loss: 0.0323 - class_label_loss: 0.0256 - bounding_box_accuracy: 0.4872 - class_label_accuracy: 0.9904 - val_loss: 0.5616 - val_bounding_box_loss: 0.0157 - val_class_label_loss: 0.5459 - val_bounding_box_accuracy: 0.3654 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 28/30\n",
    "# 30/30 [==============================] - 26s 881ms/step - loss: 0.0676 - bounding_box_loss: 0.0324 - class_label_loss: 0.0352 - bounding_box_accuracy: 0.4712 - class_label_accuracy: 0.9904 - val_loss: 0.5045 - val_bounding_box_loss: 0.0169 - val_class_label_loss: 0.4876 - val_bounding_box_accuracy: 0.4327 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 29/30\n",
    "# 30/30 [==============================] - 26s 875ms/step - loss: 0.0532 - bounding_box_loss: 0.0313 - class_label_loss: 0.0219 - bounding_box_accuracy: 0.4594 - class_label_accuracy: 0.9925 - val_loss: 0.5328 - val_bounding_box_loss: 0.0167 - val_class_label_loss: 0.5161 - val_bounding_box_accuracy: 0.4135 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 30/30\n",
    "# 30/30 [==============================] - 26s 879ms/step - loss: 0.0905 - bounding_box_loss: 0.0303 - class_label_loss: 0.0603 - bounding_box_accuracy: 0.4850 - class_label_accuracy: 0.9925 - val_loss: 0.5345 - val_bounding_box_loss: 0.0161 - val_class_label_loss: 0.5184 - val_bounding_box_accuracy: 0.4423 - val_class_label_accuracy: 0.9231"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting loss for modeling training\n",
    "plot_loss(model_name='mobilenetv2_model_v2', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training total loss during mobilenetv2_model_v2 training: 0.09050777554512024\n",
    "# Final epoch training class label loss during mobilenetv2_model_v2 training: 0.06025640293955803\n",
    "# Final epoch training bounding box loss during mobilenetv2_model_v2 training: 0.03025139681994915\n",
    "# Final epoch validation total loss during mobilenetv2_model_v2 training: 0.5345425009727478\n",
    "# Final epoch validation class label loss during mobilenetv2_model_v2 training: 0.5183947086334229\n",
    "# Final epoch validation bounding box loss during mobilenetv2_model_v2 training: 0.01614779233932495"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting accuracy for modeling training\n",
    "plot_accuracy(model_name='mobilenetv2_model_v2', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training class label accurracy during mobilenetv2_model_v2 training: 0.992521345615387\n",
    "# Final epoch validation class label accurracy during mobilenetv2_model_v2 training: 0.9230769276618958"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating model for mAP metrics and inference time and storing result\n",
    "result_val = evaluate_val(model=mobilenetv2_model_v2, model_name='mobilenetv2_model_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC PASCAL mAP for mobilenetv2_model_v2 validation data: 0.13082066178321838\n",
    "# VOC PASCAL mAP in all points for mobilenetv2_model_v2 validation data: 0.07348012179136276\n",
    "# COCO mAP for mobilenetv2_model_v2 validation data: 0.33873993158340454\n",
    "# Average inference time for mobilenetv2_model_v2 validation data: 0.07101701773129977"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending results of model evaluation to results dataframe (df_results)\n",
    "df_results_val = df_results_val.append(result_val, ignore_index=True)\n",
    "df_results_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading network but leaving off output layers\n",
    "mobilenetv2 = MobileNetV2(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "# freezing all backbone layers during training so they are not updated\n",
    "mobilenetv2.trainable = False\n",
    "\n",
    "# flattening the max-pooling output of network\n",
    "flatten = mobilenetv2.output\n",
    "flat = Flatten()(flatten)\n",
    "\n",
    "# constructing an output layer to predict bounding box coordinates\n",
    "bbox_head1 = Dense(128, activation=\"relu\")(flat)\n",
    "bbox_head2 = Dropout(0.5)(bbox_head1)\n",
    "bbox_head3 = Dense(64, activation=\"relu\")(bbox_head2)\n",
    "bbox_head4 = Dense(32, activation=\"relu\")(bbox_head3)\n",
    "bbox_head_v3 = Dense(4, activation=\"sigmoid\", name=\"bounding_box\")(bbox_head4)\n",
    "\n",
    "# constructing an output layer to predict class labels\n",
    "class_head1 = Dense(512, activation=\"relu\")(flat)\n",
    "class_head2 = Dropout(0.5)(class_head1)\n",
    "class_head3 = Dense(512, activation=\"relu\")(class_head2)\n",
    "class_head4 = Dropout(0.5)(class_head3)\n",
    "class_head5 = Dense(512, activation=\"relu\")(class_head4)\n",
    "class_head6 = Dropout(0.5)(class_head5)\n",
    "class_head7 = Dense(512, activation=\"relu\")(class_head6)\n",
    "class_head8 = Dropout(0.5)(class_head7)\n",
    "class_head_v3 = Dense(2, activation=\"sigmoid\", name=\"class_label\")(class_head8)\n",
    "\n",
    "# creating a model that accepts an input image and outputs bounding box coordinates and a class label\n",
    "mobilenetv2_model_v3 = Model(inputs=mobilenetv2.input, outputs=(bbox_head_v3, class_head_v3))\n",
    "\n",
    "# creating dictionary for setting loss functions\n",
    "losses = {\"class_label\": \"binary_crossentropy\", \"bounding_box\": \"mean_squared_error\"}\n",
    "\n",
    "# creating dictionry for weighting loss functions\n",
    "loss_weights = {\"class_label\": 1.0, \"bounding_box\": 1.0}\n",
    "\n",
    "# initilizing an \"Adam\" optimizer with learning rate of 0.0001\n",
    "opt = Adam(learning_rate = 0.0001)\n",
    "\n",
    "# compiling model\n",
    "mobilenetv2_model_v3.compile(loss=losses, optimizer=opt, metrics=[\"accuracy\"], loss_weights=loss_weights)\n",
    "print(mobilenetv2_model_v3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing a dictionary for our training outputs\n",
    "train_targets = {\"class_label\": labels_train, \"bounding_box\": bboxes_train}\n",
    "\n",
    "# constructing a dictionary for our test outputs\n",
    "val_targets = {\"class_label\": labels_val, \"bounding_box\": bboxes_val}\n",
    "\n",
    "# training model for class and bounding box predictions\n",
    "H = mobilenetv2_model_v3.fit(x_train, train_targets, validation_data=(x_val, val_targets), batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "\n",
    "# saving trained model\n",
    "mobilenetv2_model_v3.save('../Models/mobilenetv2_model_v3.h5', save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 1/30\n",
    "# 30/30 [==============================] - 21s 707ms/step - loss: 0.9543 - bounding_box_loss: 0.0460 - class_label_loss: 0.9083 - bounding_box_accuracy: 0.5673 - class_label_accuracy: 0.7222 - val_loss: 0.3002 - val_bounding_box_loss: 0.0313 - val_class_label_loss: 0.2689 - val_bounding_box_accuracy: 0.4423 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 2/30\n",
    "# 30/30 [==============================] - 22s 730ms/step - loss: 0.4953 - bounding_box_loss: 0.0361 - class_label_loss: 0.4592 - bounding_box_accuracy: 0.4840 - class_label_accuracy: 0.8889 - val_loss: 0.3039 - val_bounding_box_loss: 0.0193 - val_class_label_loss: 0.2846 - val_bounding_box_accuracy: 0.5577 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 3/30\n",
    "# 30/30 [==============================] - 26s 863ms/step - loss: 0.3431 - bounding_box_loss: 0.0249 - class_label_loss: 0.3182 - bounding_box_accuracy: 0.5363 - class_label_accuracy: 0.9231 - val_loss: 0.2455 - val_bounding_box_loss: 0.0165 - val_class_label_loss: 0.2290 - val_bounding_box_accuracy: 0.5865 - val_class_label_accuracy: 0.9615\n",
    "# Epoch 4/30\n",
    "# 30/30 [==============================] - 27s 904ms/step - loss: 0.2674 - bounding_box_loss: 0.0202 - class_label_loss: 0.2472 - bounding_box_accuracy: 0.5620 - class_label_accuracy: 0.9487 - val_loss: 0.2537 - val_bounding_box_loss: 0.0115 - val_class_label_loss: 0.2422 - val_bounding_box_accuracy: 0.5962 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 5/30\n",
    "# 30/30 [==============================] - 28s 924ms/step - loss: 0.2455 - bounding_box_loss: 0.0181 - class_label_loss: 0.2273 - bounding_box_accuracy: 0.5737 - class_label_accuracy: 0.9509 - val_loss: 0.3044 - val_bounding_box_loss: 0.0122 - val_class_label_loss: 0.2922 - val_bounding_box_accuracy: 0.5865 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 6/30\n",
    "# 30/30 [==============================] - 26s 870ms/step - loss: 0.1944 - bounding_box_loss: 0.0172 - class_label_loss: 0.1772 - bounding_box_accuracy: 0.5427 - class_label_accuracy: 0.9626 - val_loss: 0.3002 - val_bounding_box_loss: 0.0116 - val_class_label_loss: 0.2886 - val_bounding_box_accuracy: 0.5769 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 7/30\n",
    "# 30/30 [==============================] - 25s 848ms/step - loss: 0.1836 - bounding_box_loss: 0.0164 - class_label_loss: 0.1672 - bounding_box_accuracy: 0.5577 - class_label_accuracy: 0.9733 - val_loss: 0.3117 - val_bounding_box_loss: 0.0113 - val_class_label_loss: 0.3004 - val_bounding_box_accuracy: 0.5481 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 8/30\n",
    "# 30/30 [==============================] - 24s 810ms/step - loss: 0.1630 - bounding_box_loss: 0.0165 - class_label_loss: 0.1465 - bounding_box_accuracy: 0.5588 - class_label_accuracy: 0.9679 - val_loss: 0.2824 - val_bounding_box_loss: 0.0100 - val_class_label_loss: 0.2724 - val_bounding_box_accuracy: 0.5865 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 9/30\n",
    "# 30/30 [==============================] - 23s 765ms/step - loss: 0.1717 - bounding_box_loss: 0.0158 - class_label_loss: 0.1559 - bounding_box_accuracy: 0.5908 - class_label_accuracy: 0.9626 - val_loss: 0.3500 - val_bounding_box_loss: 0.0105 - val_class_label_loss: 0.3395 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 10/30\n",
    "# 30/30 [==============================] - 23s 769ms/step - loss: 0.1217 - bounding_box_loss: 0.0141 - class_label_loss: 0.1077 - bounding_box_accuracy: 0.6100 - class_label_accuracy: 0.9701 - val_loss: 0.3516 - val_bounding_box_loss: 0.0131 - val_class_label_loss: 0.3385 - val_bounding_box_accuracy: 0.6635 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 11/30\n",
    "# 30/30 [==============================] - 23s 783ms/step - loss: 0.1086 - bounding_box_loss: 0.0145 - class_label_loss: 0.0941 - bounding_box_accuracy: 0.5812 - class_label_accuracy: 0.9829 - val_loss: 0.2862 - val_bounding_box_loss: 0.0109 - val_class_label_loss: 0.2753 - val_bounding_box_accuracy: 0.5962 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 12/30\n",
    "# 30/30 [==============================] - 24s 793ms/step - loss: 0.0925 - bounding_box_loss: 0.0138 - class_label_loss: 0.0788 - bounding_box_accuracy: 0.6368 - class_label_accuracy: 0.9776 - val_loss: 0.2796 - val_bounding_box_loss: 0.0104 - val_class_label_loss: 0.2692 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 13/30\n",
    "# 30/30 [==============================] - 23s 777ms/step - loss: 0.1156 - bounding_box_loss: 0.0137 - class_label_loss: 0.1018 - bounding_box_accuracy: 0.6111 - class_label_accuracy: 0.9744 - val_loss: 0.3646 - val_bounding_box_loss: 0.0112 - val_class_label_loss: 0.3534 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 14/30\n",
    "# 30/30 [==============================] - 23s 777ms/step - loss: 0.0852 - bounding_box_loss: 0.0140 - class_label_loss: 0.0712 - bounding_box_accuracy: 0.5833 - class_label_accuracy: 0.9840 - val_loss: 0.3735 - val_bounding_box_loss: 0.0102 - val_class_label_loss: 0.3632 - val_bounding_box_accuracy: 0.6058 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 15/30\n",
    "# 30/30 [==============================] - 24s 797ms/step - loss: 0.1133 - bounding_box_loss: 0.0136 - class_label_loss: 0.0996 - bounding_box_accuracy: 0.6004 - class_label_accuracy: 0.9722 - val_loss: 0.3896 - val_bounding_box_loss: 0.0118 - val_class_label_loss: 0.3778 - val_bounding_box_accuracy: 0.6731 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 16/30\n",
    "# 30/30 [==============================] - 25s 820ms/step - loss: 0.0849 - bounding_box_loss: 0.0136 - class_label_loss: 0.0713 - bounding_box_accuracy: 0.6132 - class_label_accuracy: 0.9808 - val_loss: 0.3253 - val_bounding_box_loss: 0.0137 - val_class_label_loss: 0.3116 - val_bounding_box_accuracy: 0.6635 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 17/30\n",
    "# 30/30 [==============================] - 24s 803ms/step - loss: 0.0959 - bounding_box_loss: 0.0127 - class_label_loss: 0.0832 - bounding_box_accuracy: 0.6346 - class_label_accuracy: 0.9786 - val_loss: 0.4136 - val_bounding_box_loss: 0.0116 - val_class_label_loss: 0.4020 - val_bounding_box_accuracy: 0.6154 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 18/30\n",
    "# 30/30 [==============================] - 24s 809ms/step - loss: 0.0555 - bounding_box_loss: 0.0123 - class_label_loss: 0.0431 - bounding_box_accuracy: 0.6207 - class_label_accuracy: 0.9872 - val_loss: 0.4234 - val_bounding_box_loss: 0.0100 - val_class_label_loss: 0.4134 - val_bounding_box_accuracy: 0.6346 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 19/30\n",
    "# 30/30 [==============================] - 25s 847ms/step - loss: 0.0643 - bounding_box_loss: 0.0120 - class_label_loss: 0.0523 - bounding_box_accuracy: 0.5951 - class_label_accuracy: 0.9893 - val_loss: 0.4665 - val_bounding_box_loss: 0.0125 - val_class_label_loss: 0.4540 - val_bounding_box_accuracy: 0.5865 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 20/30\n",
    "# 30/30 [==============================] - 26s 867ms/step - loss: 0.0632 - bounding_box_loss: 0.0123 - class_label_loss: 0.0509 - bounding_box_accuracy: 0.6464 - class_label_accuracy: 0.9840 - val_loss: 0.3546 - val_bounding_box_loss: 0.0126 - val_class_label_loss: 0.3420 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 21/30\n",
    "# 30/30 [==============================] - 28s 931ms/step - loss: 0.0694 - bounding_box_loss: 0.0121 - class_label_loss: 0.0572 - bounding_box_accuracy: 0.6197 - class_label_accuracy: 0.9861 - val_loss: 0.4634 - val_bounding_box_loss: 0.0139 - val_class_label_loss: 0.4495 - val_bounding_box_accuracy: 0.6442 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 22/30\n",
    "# 30/30 [==============================] - 26s 858ms/step - loss: 0.0645 - bounding_box_loss: 0.0126 - class_label_loss: 0.0519 - bounding_box_accuracy: 0.5940 - class_label_accuracy: 0.9840 - val_loss: 0.3895 - val_bounding_box_loss: 0.0146 - val_class_label_loss: 0.3749 - val_bounding_box_accuracy: 0.6058 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 23/30\n",
    "# 30/30 [==============================] - 25s 834ms/step - loss: 0.0508 - bounding_box_loss: 0.0119 - class_label_loss: 0.0389 - bounding_box_accuracy: 0.6122 - class_label_accuracy: 0.9882 - val_loss: 0.4889 - val_bounding_box_loss: 0.0138 - val_class_label_loss: 0.4751 - val_bounding_box_accuracy: 0.6154 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 24/30\n",
    "# 30/30 [==============================] - 25s 847ms/step - loss: 0.0550 - bounding_box_loss: 0.0118 - class_label_loss: 0.0431 - bounding_box_accuracy: 0.6143 - class_label_accuracy: 0.9904 - val_loss: 0.5206 - val_bounding_box_loss: 0.0138 - val_class_label_loss: 0.5068 - val_bounding_box_accuracy: 0.6058 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 25/30\n",
    "# 30/30 [==============================] - 25s 831ms/step - loss: 0.0605 - bounding_box_loss: 0.0121 - class_label_loss: 0.0483 - bounding_box_accuracy: 0.6154 - class_label_accuracy: 0.9850 - val_loss: 0.6069 - val_bounding_box_loss: 0.0101 - val_class_label_loss: 0.5968 - val_bounding_box_accuracy: 0.6154 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 26/30\n",
    "# 30/30 [==============================] - 25s 830ms/step - loss: 0.0793 - bounding_box_loss: 0.0120 - class_label_loss: 0.0674 - bounding_box_accuracy: 0.5769 - class_label_accuracy: 0.9872 - val_loss: 0.5324 - val_bounding_box_loss: 0.0138 - val_class_label_loss: 0.5186 - val_bounding_box_accuracy: 0.6442 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 27/30\n",
    "# 30/30 [==============================] - 24s 806ms/step - loss: 0.0459 - bounding_box_loss: 0.0114 - class_label_loss: 0.0345 - bounding_box_accuracy: 0.5887 - class_label_accuracy: 0.9915 - val_loss: 0.4937 - val_bounding_box_loss: 0.0129 - val_class_label_loss: 0.4809 - val_bounding_box_accuracy: 0.5962 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 28/30\n",
    "# 30/30 [==============================] - 25s 821ms/step - loss: 0.0332 - bounding_box_loss: 0.0112 - class_label_loss: 0.0220 - bounding_box_accuracy: 0.6271 - class_label_accuracy: 0.9947 - val_loss: 0.5145 - val_bounding_box_loss: 0.0128 - val_class_label_loss: 0.5017 - val_bounding_box_accuracy: 0.5769 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 29/30\n",
    "# 30/30 [==============================] - 30s 986ms/step - loss: 0.0442 - bounding_box_loss: 0.0107 - class_label_loss: 0.0335 - bounding_box_accuracy: 0.6100 - class_label_accuracy: 0.9893 - val_loss: 0.6103 - val_bounding_box_loss: 0.0124 - val_class_label_loss: 0.5978 - val_bounding_box_accuracy: 0.6538 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 30/30\n",
    "# 30/30 [==============================] - 25s 836ms/step - loss: 0.0421 - bounding_box_loss: 0.0099 - class_label_loss: 0.0322 - bounding_box_accuracy: 0.6464 - class_label_accuracy: 0.9915 - val_loss: 0.6597 - val_bounding_box_loss: 0.0143 - val_class_label_loss: 0.6454 - val_bounding_box_accuracy: 0.6538 - val_class_label_accuracy: 0.9327"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting loss for modeling training\n",
    "plot_loss(model_name='mobilenetv2_model_v3', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training total loss during mobilenetv2_model_v3 training: 0.0421028696000576\n",
    "# Final epoch training class label loss during mobilenetv2_model_v3 training: 0.03217374533414841\n",
    "# Final epoch training bounding box loss during mobilenetv2_model_v3 training: 0.009929127991199493\n",
    "# Final epoch validation total loss during mobilenetv2_model_v3 training: 0.6597028374671936\n",
    "# Final epoch validation class label loss during mobilenetv2_model_v3 training: 0.6453897953033447\n",
    "# Final epoch validation bounding box loss during mobilenetv2_model_v3 training: 0.014313031919300556"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting accuracy for modeling training\n",
    "plot_accuracy(model_name='mobilenetv2_model_v3', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training class label accurracy during mobilenetv2_model_v3 training: 0.9914529919624329\n",
    "# Final epoch validation class label accurracy during mobilenetv2_model_v3 training: 0.932692289352417"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating model for mAP metrics and inference time and storing result\n",
    "result_val = evaluate_val(model=mobilenetv2_model_v3, model_name='mobilenetv2_model_v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC PASCAL mAP for mobilenetv2_model_v3 validation data: 0.22940725088119507\n",
    "# VOC PASCAL mAP in all points for mobilenetv2_model_v3 validation data: 0.1736449897289276\n",
    "# COCO mAP for mobilenetv2_model_v3 validation data: 0.2964179515838623\n",
    "# Average inference time for mobilenetv2_model_v3 validation data: 0.07161321777563828"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending results of model evaluation to results dataframe (df_results)\n",
    "df_results_val = df_results_val.append(result_val, ignore_index=True)\n",
    "df_results_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading network but leaving off output layers\n",
    "densenet201 = DenseNet201(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "# freezing all network layers during training so they are not updated\n",
    "densenet201.trainable = False\n",
    "\n",
    "# flattening the max-pooling output of network\n",
    "flatten = densenet201.output\n",
    "flat = Flatten()(flatten)\n",
    "\n",
    "# constructing an output layer to predict bounding box coordinates\n",
    "bbox_head1 = Dense(128, activation=\"relu\")(flat)\n",
    "bbox_head2 = Dense(64, activation=\"relu\")(bbox_head1)\n",
    "bbox_head3 = Dense(32, activation=\"relu\")(bbox_head2)\n",
    "bbox_head_v1 = Dense(4, activation=\"sigmoid\", name=\"bounding_box\")(bbox_head3)\n",
    "\n",
    "# constructing an output layer to predict class labels\n",
    "class_head1 = Dense(512, activation=\"relu\")(flat)\n",
    "class_head2 = Dropout(0.5)(class_head1)\n",
    "class_head3 = Dense(512, activation=\"relu\")(class_head2)\n",
    "class_head4 = Dropout(0.5)(class_head3)\n",
    "class_head_v1 = Dense(2, activation=\"sigmoid\", name=\"class_label\")(class_head4)\n",
    "\n",
    "# creating a model that accepts an input image and outputs bounding box coordinates and a class label\n",
    "densenet201_model_v1 = Model(inputs=densenet201.input, outputs=(bbox_head_v1, class_head_v1))\n",
    "\n",
    "# creating dictionary for setting loss functions\n",
    "losses = {\"class_label\": \"binary_crossentropy\", \"bounding_box\": \"mean_squared_error\"}\n",
    "\n",
    "# creating dictionry for weighting loss functions\n",
    "loss_weights = {\"class_label\": 1.0, \"bounding_box\": 1.0}\n",
    "\n",
    "# initilizing an \"Adam\" optimizer with learning rate of 0.0001\n",
    "opt = Adam(learning_rate = 0.0001)\n",
    "\n",
    "# compiling model\n",
    "densenet201_model_v1.compile(loss=losses, optimizer=opt, metrics=[\"accuracy\"], loss_weights=loss_weights)\n",
    "print(densenet201_model_v1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing a dictionary for our training outputs\n",
    "train_targets = {\"class_label\": labels_train, \"bounding_box\": bboxes_train}\n",
    "\n",
    "# constructing a dictionary for our test outputs\n",
    "val_targets = {\"class_label\": labels_val, \"bounding_box\": bboxes_val}\n",
    "\n",
    "# training model for class and bounding box predictions\n",
    "H = densenet201_model_v1.fit(x_train, train_targets, validation_data=(x_val, val_targets), batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "\n",
    "# saving trained model\n",
    "densenet201_model_v1.save('../Models/densenet201_model_v1.h5', save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 1/30\n",
    "# 30/30 [==============================] - 123s 4s/step - loss: 0.5188 - bounding_box_loss: 0.0368 - class_label_loss: 0.4820 - bounding_box_accuracy: 0.6015 - class_label_accuracy: 0.8675 - val_loss: 0.3886 - val_bounding_box_loss: 0.0147 - val_class_label_loss: 0.3739 - val_bounding_box_accuracy: 0.5192 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 2/30\n",
    "# 30/30 [==============================] - 122s 4s/step - loss: 0.2415 - bounding_box_loss: 0.0116 - class_label_loss: 0.2300 - bounding_box_accuracy: 0.6111 - class_label_accuracy: 0.9551 - val_loss: 0.3120 - val_bounding_box_loss: 0.0089 - val_class_label_loss: 0.3031 - val_bounding_box_accuracy: 0.6442 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 3/30\n",
    "# 30/30 [==============================] - 119s 4s/step - loss: 0.1492 - bounding_box_loss: 0.0072 - class_label_loss: 0.1421 - bounding_box_accuracy: 0.7212 - class_label_accuracy: 0.9712 - val_loss: 0.3135 - val_bounding_box_loss: 0.0079 - val_class_label_loss: 0.3056 - val_bounding_box_accuracy: 0.7404 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 4/30\n",
    "# 30/30 [==============================] - 120s 4s/step - loss: 0.1163 - bounding_box_loss: 0.0053 - class_label_loss: 0.1110 - bounding_box_accuracy: 0.7853 - class_label_accuracy: 0.9744 - val_loss: 0.3316 - val_bounding_box_loss: 0.0065 - val_class_label_loss: 0.3251 - val_bounding_box_accuracy: 0.6827 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 5/30\n",
    "# 30/30 [==============================] - 119s 4s/step - loss: 0.1165 - bounding_box_loss: 0.0042 - class_label_loss: 0.1123 - bounding_box_accuracy: 0.8034 - class_label_accuracy: 0.9754 - val_loss: 0.3144 - val_bounding_box_loss: 0.0083 - val_class_label_loss: 0.3061 - val_bounding_box_accuracy: 0.7115 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 6/30\n",
    "# 30/30 [==============================] - 118s 4s/step - loss: 0.0864 - bounding_box_loss: 0.0044 - class_label_loss: 0.0819 - bounding_box_accuracy: 0.7810 - class_label_accuracy: 0.9818 - val_loss: 0.3417 - val_bounding_box_loss: 0.0071 - val_class_label_loss: 0.3346 - val_bounding_box_accuracy: 0.7212 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 7/30\n",
    "# 30/30 [==============================] - 118s 4s/step - loss: 0.1014 - bounding_box_loss: 0.0035 - class_label_loss: 0.0979 - bounding_box_accuracy: 0.8013 - class_label_accuracy: 0.9765 - val_loss: 0.3074 - val_bounding_box_loss: 0.0070 - val_class_label_loss: 0.3004 - val_bounding_box_accuracy: 0.7308 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 8/30\n",
    "# 30/30 [==============================] - 115s 4s/step - loss: 0.0770 - bounding_box_loss: 0.0028 - class_label_loss: 0.0741 - bounding_box_accuracy: 0.8259 - class_label_accuracy: 0.9818 - val_loss: 0.3509 - val_bounding_box_loss: 0.0074 - val_class_label_loss: 0.3434 - val_bounding_box_accuracy: 0.7212 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 9/30\n",
    "# 30/30 [==============================] - 112s 4s/step - loss: 0.0586 - bounding_box_loss: 0.0023 - class_label_loss: 0.0563 - bounding_box_accuracy: 0.8590 - class_label_accuracy: 0.9861 - val_loss: 0.3935 - val_bounding_box_loss: 0.0074 - val_class_label_loss: 0.3861 - val_bounding_box_accuracy: 0.7115 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 10/30\n",
    "# 30/30 [==============================] - 111s 4s/step - loss: 0.0650 - bounding_box_loss: 0.0020 - class_label_loss: 0.0630 - bounding_box_accuracy: 0.8558 - class_label_accuracy: 0.9829 - val_loss: 0.3827 - val_bounding_box_loss: 0.0078 - val_class_label_loss: 0.3749 - val_bounding_box_accuracy: 0.7212 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 11/30\n",
    "# 30/30 [==============================] - 112s 4s/step - loss: 0.0533 - bounding_box_loss: 0.0019 - class_label_loss: 0.0514 - bounding_box_accuracy: 0.8697 - class_label_accuracy: 0.9818 - val_loss: 0.3960 - val_bounding_box_loss: 0.0073 - val_class_label_loss: 0.3887 - val_bounding_box_accuracy: 0.7788 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 12/30\n",
    "# 30/30 [==============================] - 111s 4s/step - loss: 0.0577 - bounding_box_loss: 0.0015 - class_label_loss: 0.0561 - bounding_box_accuracy: 0.8707 - class_label_accuracy: 0.9850 - val_loss: 0.3594 - val_bounding_box_loss: 0.0069 - val_class_label_loss: 0.3525 - val_bounding_box_accuracy: 0.7308 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 13/30\n",
    "# 30/30 [==============================] - 113s 4s/step - loss: 0.0711 - bounding_box_loss: 0.0014 - class_label_loss: 0.0697 - bounding_box_accuracy: 0.8793 - class_label_accuracy: 0.9808 - val_loss: 0.4937 - val_bounding_box_loss: 0.0067 - val_class_label_loss: 0.4870 - val_bounding_box_accuracy: 0.7500 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 14/30\n",
    "# 30/30 [==============================] - 112s 4s/step - loss: 0.0368 - bounding_box_loss: 0.0012 - class_label_loss: 0.0356 - bounding_box_accuracy: 0.9049 - class_label_accuracy: 0.9936 - val_loss: 0.4228 - val_bounding_box_loss: 0.0071 - val_class_label_loss: 0.4156 - val_bounding_box_accuracy: 0.7596 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 15/30\n",
    "# 30/30 [==============================] - 114s 4s/step - loss: 0.0306 - bounding_box_loss: 0.0011 - class_label_loss: 0.0295 - bounding_box_accuracy: 0.8974 - class_label_accuracy: 0.9925 - val_loss: 0.4013 - val_bounding_box_loss: 0.0069 - val_class_label_loss: 0.3944 - val_bounding_box_accuracy: 0.7788 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 16/30\n",
    "# 30/30 [==============================] - 113s 4s/step - loss: 0.0322 - bounding_box_loss: 0.0010 - class_label_loss: 0.0312 - bounding_box_accuracy: 0.8964 - class_label_accuracy: 0.9915 - val_loss: 0.4171 - val_bounding_box_loss: 0.0070 - val_class_label_loss: 0.4101 - val_bounding_box_accuracy: 0.7788 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 17/30\n",
    "# 30/30 [==============================] - 114s 4s/step - loss: 0.0429 - bounding_box_loss: 9.6024e-04 - class_label_loss: 0.0419 - bounding_box_accuracy: 0.9071 - class_label_accuracy: 0.9872 - val_loss: 0.4595 - val_bounding_box_loss: 0.0073 - val_class_label_loss: 0.4522 - val_bounding_box_accuracy: 0.7885 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 18/30\n",
    "# 30/30 [==============================] - 115s 4s/step - loss: 0.0458 - bounding_box_loss: 9.1747e-04 - class_label_loss: 0.0448 - bounding_box_accuracy: 0.9092 - class_label_accuracy: 0.9882 - val_loss: 0.3989 - val_bounding_box_loss: 0.0074 - val_class_label_loss: 0.3915 - val_bounding_box_accuracy: 0.7596 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 19/30\n",
    "# 30/30 [==============================] - 113s 4s/step - loss: 0.0556 - bounding_box_loss: 9.4522e-04 - class_label_loss: 0.0546 - bounding_box_accuracy: 0.8953 - class_label_accuracy: 0.9850 - val_loss: 0.4119 - val_bounding_box_loss: 0.0072 - val_class_label_loss: 0.4047 - val_bounding_box_accuracy: 0.7404 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 20/30\n",
    "# 30/30 [==============================] - 115s 4s/step - loss: 0.0259 - bounding_box_loss: 8.5457e-04 - class_label_loss: 0.0251 - bounding_box_accuracy: 0.9092 - class_label_accuracy: 0.9915 - val_loss: 0.5823 - val_bounding_box_loss: 0.0077 - val_class_label_loss: 0.5746 - val_bounding_box_accuracy: 0.7596 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 21/30\n",
    "# 30/30 [==============================] - 114s 4s/step - loss: 0.0323 - bounding_box_loss: 8.1299e-04 - class_label_loss: 0.0315 - bounding_box_accuracy: 0.9006 - class_label_accuracy: 0.9915 - val_loss: 0.4670 - val_bounding_box_loss: 0.0073 - val_class_label_loss: 0.4597 - val_bounding_box_accuracy: 0.7404 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 22/30\n",
    "# 30/30 [==============================] - 110s 4s/step - loss: 0.0279 - bounding_box_loss: 8.0106e-04 - class_label_loss: 0.0271 - bounding_box_accuracy: 0.9252 - class_label_accuracy: 0.9947 - val_loss: 0.4745 - val_bounding_box_loss: 0.0071 - val_class_label_loss: 0.4673 - val_bounding_box_accuracy: 0.7788 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 23/30\n",
    "# 30/30 [==============================] - 110s 4s/step - loss: 0.0236 - bounding_box_loss: 6.8378e-04 - class_label_loss: 0.0229 - bounding_box_accuracy: 0.9113 - class_label_accuracy: 0.9947 - val_loss: 0.5457 - val_bounding_box_loss: 0.0077 - val_class_label_loss: 0.5380 - val_bounding_box_accuracy: 0.7788 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 24/30\n",
    "# 30/30 [==============================] - 110s 4s/step - loss: 0.0164 - bounding_box_loss: 8.5619e-04 - class_label_loss: 0.0156 - bounding_box_accuracy: 0.8996 - class_label_accuracy: 0.9957 - val_loss: 0.5963 - val_bounding_box_loss: 0.0074 - val_class_label_loss: 0.5889 - val_bounding_box_accuracy: 0.7692 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 25/30\n",
    "# 30/30 [==============================] - 110s 4s/step - loss: 0.0271 - bounding_box_loss: 9.6959e-04 - class_label_loss: 0.0261 - bounding_box_accuracy: 0.8964 - class_label_accuracy: 0.9904 - val_loss: 0.4800 - val_bounding_box_loss: 0.0076 - val_class_label_loss: 0.4724 - val_bounding_box_accuracy: 0.7308 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 26/30\n",
    "# 30/30 [==============================] - 110s 4s/step - loss: 0.0209 - bounding_box_loss: 0.0012 - class_label_loss: 0.0197 - bounding_box_accuracy: 0.8771 - class_label_accuracy: 0.9936 - val_loss: 0.5101 - val_bounding_box_loss: 0.0075 - val_class_label_loss: 0.5026 - val_bounding_box_accuracy: 0.8077 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 27/30\n",
    "# 30/30 [==============================] - 110s 4s/step - loss: 0.0308 - bounding_box_loss: 0.0011 - class_label_loss: 0.0296 - bounding_box_accuracy: 0.8974 - class_label_accuracy: 0.9947 - val_loss: 0.5589 - val_bounding_box_loss: 0.0081 - val_class_label_loss: 0.5508 - val_bounding_box_accuracy: 0.7404 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 28/30\n",
    "# 30/30 [==============================] - 111s 4s/step - loss: 0.0152 - bounding_box_loss: 0.0010 - class_label_loss: 0.0142 - bounding_box_accuracy: 0.8996 - class_label_accuracy: 0.9968 - val_loss: 0.4876 - val_bounding_box_loss: 0.0078 - val_class_label_loss: 0.4799 - val_bounding_box_accuracy: 0.7308 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 29/30\n",
    "# 30/30 [==============================] - 111s 4s/step - loss: 0.0301 - bounding_box_loss: 9.9308e-04 - class_label_loss: 0.0291 - bounding_box_accuracy: 0.8889 - class_label_accuracy: 0.9925 - val_loss: 0.6361 - val_bounding_box_loss: 0.0079 - val_class_label_loss: 0.6283 - val_bounding_box_accuracy: 0.7692 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 30/30\n",
    "# 30/30 [==============================] - 112s 4s/step - loss: 0.0302 - bounding_box_loss: 8.0017e-04 - class_label_loss: 0.0294 - bounding_box_accuracy: 0.9113 - class_label_accuracy: 0.9936 - val_loss: 0.5671 - val_bounding_box_loss: 0.0077 - val_class_label_loss: 0.5594 - val_bounding_box_accuracy: 0.7115 - val_class_label_accuracy: 0.9327"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting loss for modeling training\n",
    "plot_loss(model_name='densenet201_model_v1', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training total loss during densenet201_model_v1 training: 0.030231695622205734\n",
    "# Final epoch training class label loss during densenet201_model_v1 training: 0.029431521892547607\n",
    "# Final epoch training bounding box loss during densenet201_model_v1 training: 0.0008001707028597593\n",
    "# Final epoch validation total loss during densenet201_model_v1 training: 0.5670788288116455\n",
    "# Final epoch validation class label loss during densenet201_model_v1 training: 0.5594009160995483\n",
    "# Final epoch validation bounding box loss during densenet201_model_v1 training: 0.007677962072193623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting accuracy for modeling training\n",
    "plot_accuracy(model_name='densenet201_model_v1', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training class label accurracy during densenet201_model_v1 training: 0.9935897588729858\n",
    "# Final epoch validation class label accurracy during densenet201_model_v1 training: 0.932692289352417"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating model for mAP metrics and inference time and storing result\n",
    "result_val = evaluate_val(model=densenet201_model_v1, model_name='densenet201_model_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC PASCAL mAP for densenet201_model_v1 validation data: 0.39148420095443726\n",
    "# VOC PASCAL mAP in all points for densenet201_model_v1 validation data: 0.3665178120136261\n",
    "# COCO mAP for densenet201_model_v1 validation data: 0.5001807808876038\n",
    "# Average inference time for densenet201_model_v1 validation data: 0.2110929236962245"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending results of model evaluation to results dataframe (df_results)\n",
    "df_results_val = df_results_val.append(result_val, ignore_index=True)\n",
    "df_results_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading network but leaving off output layers\n",
    "densenet201 = DenseNet201(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "# freezing all VGG layers during training so they are not updated\n",
    "densenet201.trainable = False\n",
    "\n",
    "# flattening the max-pooling output of network\n",
    "flatten = densenet201.output\n",
    "flat = Flatten()(flatten)\n",
    "\n",
    "# constructing an output layer to predict bounding box coordinates\n",
    "bbox_head1 = Dense(128, activation=\"relu\")(flat)\n",
    "bbox_head2 = Dropout(0.5)(bbox_head1)\n",
    "bbox_head3 = Dense(64, activation=\"relu\")(bbox_head2)\n",
    "bbox_head4 = Dropout(0.5)(bbox_head3)\n",
    "bbox_head5 = Dense(32, activation=\"relu\")(bbox_head4)\n",
    "bbox_head6 = Dropout(0.5)(bbox_head5)\n",
    "bbox_head_v2 = Dense(4, activation=\"sigmoid\", name=\"bounding_box\")(bbox_head6)\n",
    "\n",
    "# constructing an output layer to predict class labels\n",
    "class_head1 = Dense(512, activation=\"relu\")(flat)\n",
    "class_head2 = Dropout(0.5)(class_head1)\n",
    "class_head3 = Dense(512, activation=\"relu\")(class_head2)\n",
    "class_head4 = Dropout(0.5)(class_head3)\n",
    "class_head5 = Dense(512, activation=\"relu\")(class_head4)\n",
    "class_head6 = Dropout(0.5)(class_head5)\n",
    "class_head_v2 = Dense(2, activation=\"sigmoid\", name=\"class_label\")(class_head6)\n",
    "\n",
    "# creating a model that accepts an input image and outputs bounding box coordinates and a class label\n",
    "densenet201_model_v2 = Model(inputs=densenet201.input, outputs=(bbox_head_v2, class_head_v2))\n",
    "\n",
    "# creating dictionary for setting loss functions\n",
    "losses = {\"class_label\": \"binary_crossentropy\", \"bounding_box\": \"mean_squared_error\"}\n",
    "\n",
    "# creating dictionry for weighting loss functions\n",
    "loss_weights = {\"class_label\": 1.0, \"bounding_box\": 1.0}\n",
    "\n",
    "# initilizing an \"Adam\" optimizer with learning rate of 0.0001\n",
    "opt = Adam(learning_rate = 0.0001)\n",
    "\n",
    "# compiling model\n",
    "densenet201_model_v2.compile(loss=losses, optimizer=opt, metrics=[\"accuracy\"], loss_weights=loss_weights)\n",
    "print(densenet201_model_v2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing a dictionary for our training outputs\n",
    "train_targets = {\"class_label\": labels_train, \"bounding_box\": bboxes_train}\n",
    "\n",
    "# constructing a dictionary for our test outputs\n",
    "val_targets = {\"class_label\": labels_val, \"bounding_box\": bboxes_val}\n",
    "\n",
    "# training model for class and bounding box predictions\n",
    "H = densenet201_model_v2.fit(x_train, train_targets, validation_data=(x_val, val_targets), batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "\n",
    "# saving trained model\n",
    "densenet201_model_v2.save('../Models/densenet201_model_v2.h5', save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 1/30\n",
    "# 30/30 [==============================] - 116s 4s/step - loss: 0.7059 - bounding_box_loss: 0.1380 - class_label_loss: 0.5679 - bounding_box_accuracy: 0.4263 - class_label_accuracy: 0.8226 - val_loss: 0.3324 - val_bounding_box_loss: 0.0226 - val_class_label_loss: 0.3098 - val_bounding_box_accuracy: 0.4808 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 2/30\n",
    "# 30/30 [==============================] - 109s 4s/step - loss: 0.3369 - bounding_box_loss: 0.1105 - class_label_loss: 0.2264 - bounding_box_accuracy: 0.4701 - class_label_accuracy: 0.9530 - val_loss: 0.3886 - val_bounding_box_loss: 0.0476 - val_class_label_loss: 0.3410 - val_bounding_box_accuracy: 0.4519 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 3/30\n",
    "# 30/30 [==============================] - 108s 4s/step - loss: 0.2795 - bounding_box_loss: 0.0952 - class_label_loss: 0.1844 - bounding_box_accuracy: 0.4583 - class_label_accuracy: 0.9658 - val_loss: 0.3229 - val_bounding_box_loss: 0.0586 - val_class_label_loss: 0.2643 - val_bounding_box_accuracy: 0.4615 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 4/30\n",
    "# 30/30 [==============================] - 108s 4s/step - loss: 0.2867 - bounding_box_loss: 0.0907 - class_label_loss: 0.1960 - bounding_box_accuracy: 0.4744 - class_label_accuracy: 0.9658 - val_loss: 0.3112 - val_bounding_box_loss: 0.0535 - val_class_label_loss: 0.2577 - val_bounding_box_accuracy: 0.5769 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 5/30\n",
    "# 30/30 [==============================] - 108s 4s/step - loss: 0.2164 - bounding_box_loss: 0.0842 - class_label_loss: 0.1322 - bounding_box_accuracy: 0.4968 - class_label_accuracy: 0.9690 - val_loss: 0.3362 - val_bounding_box_loss: 0.0452 - val_class_label_loss: 0.2910 - val_bounding_box_accuracy: 0.4904 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 6/30\n",
    "# 30/30 [==============================] - 107s 4s/step - loss: 0.2044 - bounding_box_loss: 0.0790 - class_label_loss: 0.1255 - bounding_box_accuracy: 0.4818 - class_label_accuracy: 0.9722 - val_loss: 0.3036 - val_bounding_box_loss: 0.0307 - val_class_label_loss: 0.2729 - val_bounding_box_accuracy: 0.5288 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 7/30\n",
    "# 30/30 [==============================] - 108s 4s/step - loss: 0.1915 - bounding_box_loss: 0.0779 - class_label_loss: 0.1136 - bounding_box_accuracy: 0.4818 - class_label_accuracy: 0.9765 - val_loss: 0.3434 - val_bounding_box_loss: 0.0235 - val_class_label_loss: 0.3200 - val_bounding_box_accuracy: 0.3942 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 8/30\n",
    "# 30/30 [==============================] - 108s 4s/step - loss: 0.1785 - bounding_box_loss: 0.0722 - class_label_loss: 0.1063 - bounding_box_accuracy: 0.5395 - class_label_accuracy: 0.9722 - val_loss: 0.3152 - val_bounding_box_loss: 0.0274 - val_class_label_loss: 0.2878 - val_bounding_box_accuracy: 0.4423 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 9/30\n",
    "# 30/30 [==============================] - 108s 4s/step - loss: 0.1602 - bounding_box_loss: 0.0664 - class_label_loss: 0.0938 - bounding_box_accuracy: 0.4765 - class_label_accuracy: 0.9744 - val_loss: 0.3637 - val_bounding_box_loss: 0.0220 - val_class_label_loss: 0.3416 - val_bounding_box_accuracy: 0.3942 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 10/30\n",
    "# 30/30 [==============================] - 108s 4s/step - loss: 0.1542 - bounding_box_loss: 0.0634 - class_label_loss: 0.0908 - bounding_box_accuracy: 0.4690 - class_label_accuracy: 0.9744 - val_loss: 0.3652 - val_bounding_box_loss: 0.0251 - val_class_label_loss: 0.3401 - val_bounding_box_accuracy: 0.4135 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 11/30\n",
    "# 30/30 [==============================] - 108s 4s/step - loss: 0.1210 - bounding_box_loss: 0.0603 - class_label_loss: 0.0607 - bounding_box_accuracy: 0.4936 - class_label_accuracy: 0.9840 - val_loss: 0.3900 - val_bounding_box_loss: 0.0234 - val_class_label_loss: 0.3666 - val_bounding_box_accuracy: 0.4423 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 12/30\n",
    "# 30/30 [==============================] - 108s 4s/step - loss: 0.1253 - bounding_box_loss: 0.0584 - class_label_loss: 0.0669 - bounding_box_accuracy: 0.4733 - class_label_accuracy: 0.9754 - val_loss: 0.4364 - val_bounding_box_loss: 0.0181 - val_class_label_loss: 0.4183 - val_bounding_box_accuracy: 0.4327 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 13/30\n",
    "# 30/30 [==============================] - 108s 4s/step - loss: 0.1117 - bounding_box_loss: 0.0552 - class_label_loss: 0.0564 - bounding_box_accuracy: 0.5075 - class_label_accuracy: 0.9872 - val_loss: 0.3546 - val_bounding_box_loss: 0.0148 - val_class_label_loss: 0.3398 - val_bounding_box_accuracy: 0.3942 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 14/30\n",
    "# 30/30 [==============================] - 108s 4s/step - loss: 0.1220 - bounding_box_loss: 0.0540 - class_label_loss: 0.0680 - bounding_box_accuracy: 0.4925 - class_label_accuracy: 0.9754 - val_loss: 0.4162 - val_bounding_box_loss: 0.0164 - val_class_label_loss: 0.3998 - val_bounding_box_accuracy: 0.4423 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 15/30\n",
    "# 30/30 [==============================] - 109s 4s/step - loss: 0.0935 - bounding_box_loss: 0.0500 - class_label_loss: 0.0435 - bounding_box_accuracy: 0.5160 - class_label_accuracy: 0.9893 - val_loss: 0.4231 - val_bounding_box_loss: 0.0158 - val_class_label_loss: 0.4073 - val_bounding_box_accuracy: 0.4135 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 16/30\n",
    "# 30/30 [==============================] - 108s 4s/step - loss: 0.1038 - bounding_box_loss: 0.0478 - class_label_loss: 0.0559 - bounding_box_accuracy: 0.5043 - class_label_accuracy: 0.9829 - val_loss: 0.4138 - val_bounding_box_loss: 0.0126 - val_class_label_loss: 0.4012 - val_bounding_box_accuracy: 0.5096 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 17/30\n",
    "# 30/30 [==============================] - 108s 4s/step - loss: 0.1145 - bounding_box_loss: 0.0469 - class_label_loss: 0.0676 - bounding_box_accuracy: 0.4968 - class_label_accuracy: 0.9818 - val_loss: 0.3834 - val_bounding_box_loss: 0.0132 - val_class_label_loss: 0.3702 - val_bounding_box_accuracy: 0.5000 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 18/30\n",
    "# 30/30 [==============================] - 109s 4s/step - loss: 0.1040 - bounding_box_loss: 0.0444 - class_label_loss: 0.0595 - bounding_box_accuracy: 0.4840 - class_label_accuracy: 0.9840 - val_loss: 0.4459 - val_bounding_box_loss: 0.0133 - val_class_label_loss: 0.4326 - val_bounding_box_accuracy: 0.4904 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 19/30\n",
    "# 30/30 [==============================] - 108s 4s/step - loss: 0.0710 - bounding_box_loss: 0.0421 - class_label_loss: 0.0290 - bounding_box_accuracy: 0.5043 - class_label_accuracy: 0.9882 - val_loss: 0.4696 - val_bounding_box_loss: 0.0124 - val_class_label_loss: 0.4572 - val_bounding_box_accuracy: 0.4135 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 20/30\n",
    "# 30/30 [==============================] - 108s 4s/step - loss: 0.0809 - bounding_box_loss: 0.0408 - class_label_loss: 0.0400 - bounding_box_accuracy: 0.5032 - class_label_accuracy: 0.9872 - val_loss: 0.4514 - val_bounding_box_loss: 0.0130 - val_class_label_loss: 0.4383 - val_bounding_box_accuracy: 0.3942 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 21/30\n",
    "# 30/30 [==============================] - 108s 4s/step - loss: 0.0852 - bounding_box_loss: 0.0384 - class_label_loss: 0.0468 - bounding_box_accuracy: 0.5085 - class_label_accuracy: 0.9808 - val_loss: 0.4371 - val_bounding_box_loss: 0.0144 - val_class_label_loss: 0.4226 - val_bounding_box_accuracy: 0.4519 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 22/30\n",
    "# 30/30 [==============================] - 108s 4s/step - loss: 0.0783 - bounding_box_loss: 0.0401 - class_label_loss: 0.0382 - bounding_box_accuracy: 0.4594 - class_label_accuracy: 0.9893 - val_loss: 0.4996 - val_bounding_box_loss: 0.0117 - val_class_label_loss: 0.4879 - val_bounding_box_accuracy: 0.4038 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 23/30\n",
    "# 30/30 [==============================] - 109s 4s/step - loss: 0.0757 - bounding_box_loss: 0.0396 - class_label_loss: 0.0361 - bounding_box_accuracy: 0.4797 - class_label_accuracy: 0.9882 - val_loss: 0.4969 - val_bounding_box_loss: 0.0115 - val_class_label_loss: 0.4854 - val_bounding_box_accuracy: 0.4808 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 24/30\n",
    "# 30/30 [==============================] - 108s 4s/step - loss: 0.0607 - bounding_box_loss: 0.0362 - class_label_loss: 0.0245 - bounding_box_accuracy: 0.4989 - class_label_accuracy: 0.9915 - val_loss: 0.5034 - val_bounding_box_loss: 0.0123 - val_class_label_loss: 0.4912 - val_bounding_box_accuracy: 0.4712 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 25/30\n",
    "# 30/30 [==============================] - 108s 4s/step - loss: 0.0695 - bounding_box_loss: 0.0369 - class_label_loss: 0.0326 - bounding_box_accuracy: 0.4818 - class_label_accuracy: 0.9904 - val_loss: 0.6469 - val_bounding_box_loss: 0.0117 - val_class_label_loss: 0.6352 - val_bounding_box_accuracy: 0.4519 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 26/30\n",
    "# 30/30 [==============================] - 109s 4s/step - loss: 0.0684 - bounding_box_loss: 0.0351 - class_label_loss: 0.0333 - bounding_box_accuracy: 0.4872 - class_label_accuracy: 0.9872 - val_loss: 0.5690 - val_bounding_box_loss: 0.0138 - val_class_label_loss: 0.5552 - val_bounding_box_accuracy: 0.4423 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 27/30\n",
    "# 30/30 [==============================] - 108s 4s/step - loss: 0.0699 - bounding_box_loss: 0.0337 - class_label_loss: 0.0362 - bounding_box_accuracy: 0.5000 - class_label_accuracy: 0.9872 - val_loss: 0.4541 - val_bounding_box_loss: 0.0135 - val_class_label_loss: 0.4406 - val_bounding_box_accuracy: 0.4327 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 28/30\n",
    "# 30/30 [==============================] - 108s 4s/step - loss: 0.0619 - bounding_box_loss: 0.0339 - class_label_loss: 0.0280 - bounding_box_accuracy: 0.4979 - class_label_accuracy: 0.9915 - val_loss: 0.5709 - val_bounding_box_loss: 0.0135 - val_class_label_loss: 0.5574 - val_bounding_box_accuracy: 0.3846 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 29/30\n",
    "# 30/30 [==============================] - 109s 4s/step - loss: 0.0643 - bounding_box_loss: 0.0323 - class_label_loss: 0.0321 - bounding_box_accuracy: 0.5021 - class_label_accuracy: 0.9915 - val_loss: 0.7048 - val_bounding_box_loss: 0.0112 - val_class_label_loss: 0.6935 - val_bounding_box_accuracy: 0.3942 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 30/30\n",
    "# 30/30 [==============================] - 110s 4s/step - loss: 0.0536 - bounding_box_loss: 0.0322 - class_label_loss: 0.0214 - bounding_box_accuracy: 0.4936 - class_label_accuracy: 0.9915 - val_loss: 0.6004 - val_bounding_box_loss: 0.0106 - val_class_label_loss: 0.5898 - val_bounding_box_accuracy: 0.3942 - val_class_label_accuracy: 0.9327"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting loss for modeling training\n",
    "plot_loss(model_name='densenet201_model_v2', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training total loss during densenet201_model_v2 training: 0.053632043302059174\n",
    "# Final epoch training class label loss during densenet201_model_v2 training: 0.021420832723379135\n",
    "# Final epoch training bounding box loss during densenet201_model_v2 training: 0.03221120312809944\n",
    "# Final epoch validation total loss during densenet201_model_v2 training: 0.600429356098175\n",
    "# Final epoch validation class label loss during densenet201_model_v2 training: 0.5897904634475708\n",
    "# Final epoch validation bounding box loss during densenet201_model_v2 training: 0.010638875886797905"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting accuracy for modeling training\n",
    "plot_accuracy(model_name='densenet201_model_v2', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training class label accurracy during densenet201_model_v2 training: 0.9914529919624329\n",
    "# Final epoch validation class label accurracy during densenet201_model_v2 training: 0.932692289352417"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating model for mAP metrics and inference time and storing result\n",
    "result_val = evaluate_val(model=densenet201_model_v2, model_name='densenet201_model_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC PASCAL mAP for densenet201_model_v2 validation data: 0.1916954219341278\n",
    "# VOC PASCAL mAP in all points for densenet201_model_v2 validation data: 0.13978013396263123\n",
    "# COCO mAP for densenet201_model_v2 validation data: 0.3991105854511261\n",
    "# Average inference time for densenet201_model_v2 validation data: 0.20599062396929815"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending results of model evaluation to results dataframe (df_results)\n",
    "df_results_val = df_results_val.append(result_val, ignore_index=True)\n",
    "df_results_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading network but leaving off output layers\n",
    "densenet201 = DenseNet201(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "# freezing all backbone layers during training so they are not updated\n",
    "densenet201.trainable = False\n",
    "\n",
    "# flattening the max-pooling output of network\n",
    "flatten = densenet201.output\n",
    "flat = Flatten()(flatten)\n",
    "\n",
    "# constructing an output layer to predict bounding box coordinates\n",
    "bbox_head1 = Dense(128, activation=\"relu\")(flat)\n",
    "bbox_head2 = Dropout(0.5)(bbox_head1)\n",
    "bbox_head3 = Dense(64, activation=\"relu\")(bbox_head2)\n",
    "bbox_head4 = Dense(32, activation=\"relu\")(bbox_head3)\n",
    "bbox_head_v3 = Dense(4, activation=\"sigmoid\", name=\"bounding_box\")(bbox_head4)\n",
    "\n",
    "# constructing an output layer to predict class labels\n",
    "class_head1 = Dense(512, activation=\"relu\")(flat)\n",
    "class_head2 = Dropout(0.5)(class_head1)\n",
    "class_head3 = Dense(512, activation=\"relu\")(class_head2)\n",
    "class_head4 = Dropout(0.5)(class_head3)\n",
    "class_head5 = Dense(512, activation=\"relu\")(class_head4)\n",
    "class_head6 = Dropout(0.5)(class_head5)\n",
    "class_head7 = Dense(512, activation=\"relu\")(class_head6)\n",
    "class_head8 = Dropout(0.5)(class_head7)\n",
    "class_head_v3 = Dense(2, activation=\"sigmoid\", name=\"class_label\")(class_head8)\n",
    "\n",
    "# creating a model that accepts an input image and outputs bounding box coordinates and a class label\n",
    "densenet201_model_v3 = Model(inputs=densenet201.input, outputs=(bbox_head_v3, class_head_v3))\n",
    "\n",
    "# creating dictionary for setting loss functions\n",
    "losses = {\"class_label\": \"binary_crossentropy\", \"bounding_box\": \"mean_squared_error\"}\n",
    "\n",
    "# creating dictionry for weighting loss functions\n",
    "loss_weights = {\"class_label\": 1.0, \"bounding_box\": 1.0}\n",
    "\n",
    "# initilizing an \"Adam\" optimizer with learning rate of 0.0001\n",
    "opt = Adam(learning_rate = 0.0001)\n",
    "\n",
    "# compiling model\n",
    "densenet201_model_v3.compile(loss=losses, optimizer=opt, metrics=[\"accuracy\"], loss_weights=loss_weights)\n",
    "print(densenet201_model_v3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing a dictionary for our training outputs\n",
    "train_targets = {\"class_label\": labels_train, \"bounding_box\": bboxes_train}\n",
    "\n",
    "# constructing a dictionary for our test outputs\n",
    "val_targets = {\"class_label\": labels_val, \"bounding_box\": bboxes_val}\n",
    "\n",
    "# training model for class and bounding box predictions\n",
    "H = densenet201_model_v3.fit(x_train, train_targets, validation_data=(x_val, val_targets), batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "\n",
    "# saving trained model\n",
    "densenet201_model_v3.save('../Models/densenet201_model_v3.h5', save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 1/30\n",
    "# 30/30 [==============================] - 123s 4s/step - loss: 0.8642 - bounding_box_loss: 0.0378 - class_label_loss: 0.8265 - bounding_box_accuracy: 0.4979 - class_label_accuracy: 0.7158 - val_loss: 0.2432 - val_bounding_box_loss: 0.0151 - val_class_label_loss: 0.2281 - val_bounding_box_accuracy: 0.5865 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 2/30\n",
    "# 30/30 [==============================] - 125s 4s/step - loss: 0.4292 - bounding_box_loss: 0.0198 - class_label_loss: 0.4093 - bounding_box_accuracy: 0.5299 - class_label_accuracy: 0.8761 - val_loss: 0.2539 - val_bounding_box_loss: 0.0101 - val_class_label_loss: 0.2438 - val_bounding_box_accuracy: 0.6154 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 3/30\n",
    "# 30/30 [==============================] - 123s 4s/step - loss: 0.2717 - bounding_box_loss: 0.0164 - class_label_loss: 0.2554 - bounding_box_accuracy: 0.5385 - class_label_accuracy: 0.9455 - val_loss: 0.3122 - val_bounding_box_loss: 0.0102 - val_class_label_loss: 0.3020 - val_bounding_box_accuracy: 0.6538 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 4/30\n",
    "# 30/30 [==============================] - 124s 4s/step - loss: 0.1976 - bounding_box_loss: 0.0141 - class_label_loss: 0.1836 - bounding_box_accuracy: 0.5812 - class_label_accuracy: 0.9583 - val_loss: 0.3136 - val_bounding_box_loss: 0.0093 - val_class_label_loss: 0.3043 - val_bounding_box_accuracy: 0.6346 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 5/30\n",
    "# 30/30 [==============================] - 125s 4s/step - loss: 0.2085 - bounding_box_loss: 0.0136 - class_label_loss: 0.1949 - bounding_box_accuracy: 0.5694 - class_label_accuracy: 0.9669 - val_loss: 0.2936 - val_bounding_box_loss: 0.0107 - val_class_label_loss: 0.2829 - val_bounding_box_accuracy: 0.6058 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 6/30\n",
    "# 30/30 [==============================] - 123s 4s/step - loss: 0.1588 - bounding_box_loss: 0.0135 - class_label_loss: 0.1453 - bounding_box_accuracy: 0.6207 - class_label_accuracy: 0.9647 - val_loss: 0.3038 - val_bounding_box_loss: 0.0086 - val_class_label_loss: 0.2952 - val_bounding_box_accuracy: 0.6346 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 7/30\n",
    "# 30/30 [==============================] - 128s 4s/step - loss: 0.1536 - bounding_box_loss: 0.0127 - class_label_loss: 0.1409 - bounding_box_accuracy: 0.5962 - class_label_accuracy: 0.9722 - val_loss: 0.3541 - val_bounding_box_loss: 0.0110 - val_class_label_loss: 0.3431 - val_bounding_box_accuracy: 0.6346 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 8/30\n",
    "# 30/30 [==============================] - 134s 4s/step - loss: 0.1394 - bounding_box_loss: 0.0130 - class_label_loss: 0.1264 - bounding_box_accuracy: 0.5855 - class_label_accuracy: 0.9701 - val_loss: 0.3743 - val_bounding_box_loss: 0.0101 - val_class_label_loss: 0.3642 - val_bounding_box_accuracy: 0.6154 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 9/30\n",
    "# 30/30 [==============================] - 131s 4s/step - loss: 0.1193 - bounding_box_loss: 0.0119 - class_label_loss: 0.1074 - bounding_box_accuracy: 0.6015 - class_label_accuracy: 0.9744 - val_loss: 0.3239 - val_bounding_box_loss: 0.0094 - val_class_label_loss: 0.3145 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 10/30\n",
    "# 30/30 [==============================] - 134s 4s/step - loss: 0.0907 - bounding_box_loss: 0.0126 - class_label_loss: 0.0781 - bounding_box_accuracy: 0.6325 - class_label_accuracy: 0.9776 - val_loss: 0.3531 - val_bounding_box_loss: 0.0101 - val_class_label_loss: 0.3430 - val_bounding_box_accuracy: 0.6154 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 11/30\n",
    "# 30/30 [==============================] - 132s 4s/step - loss: 0.0788 - bounding_box_loss: 0.0119 - class_label_loss: 0.0669 - bounding_box_accuracy: 0.6175 - class_label_accuracy: 0.9840 - val_loss: 0.3805 - val_bounding_box_loss: 0.0094 - val_class_label_loss: 0.3711 - val_bounding_box_accuracy: 0.6442 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 12/30\n",
    "# 30/30 [==============================] - 132s 4s/step - loss: 0.0782 - bounding_box_loss: 0.0113 - class_label_loss: 0.0668 - bounding_box_accuracy: 0.6186 - class_label_accuracy: 0.9754 - val_loss: 0.4225 - val_bounding_box_loss: 0.0094 - val_class_label_loss: 0.4131 - val_bounding_box_accuracy: 0.6346 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 13/30\n",
    "# 30/30 [==============================] - 134s 4s/step - loss: 0.0752 - bounding_box_loss: 0.0118 - class_label_loss: 0.0633 - bounding_box_accuracy: 0.6250 - class_label_accuracy: 0.9786 - val_loss: 0.4826 - val_bounding_box_loss: 0.0097 - val_class_label_loss: 0.4729 - val_bounding_box_accuracy: 0.6538 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 14/30\n",
    "# 30/30 [==============================] - 131s 4s/step - loss: 0.0799 - bounding_box_loss: 0.0107 - class_label_loss: 0.0691 - bounding_box_accuracy: 0.6090 - class_label_accuracy: 0.9776 - val_loss: 0.3805 - val_bounding_box_loss: 0.0090 - val_class_label_loss: 0.3715 - val_bounding_box_accuracy: 0.6538 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 15/30\n",
    "# 30/30 [==============================] - 133s 4s/step - loss: 0.0826 - bounding_box_loss: 0.0117 - class_label_loss: 0.0709 - bounding_box_accuracy: 0.6346 - class_label_accuracy: 0.9872 - val_loss: 0.4833 - val_bounding_box_loss: 0.0114 - val_class_label_loss: 0.4719 - val_bounding_box_accuracy: 0.6538 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 16/30\n",
    "# 30/30 [==============================] - 134s 4s/step - loss: 0.0643 - bounding_box_loss: 0.0108 - class_label_loss: 0.0535 - bounding_box_accuracy: 0.6122 - class_label_accuracy: 0.9840 - val_loss: 0.4772 - val_bounding_box_loss: 0.0100 - val_class_label_loss: 0.4672 - val_bounding_box_accuracy: 0.6538 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 17/30\n",
    "# 30/30 [==============================] - 134s 4s/step - loss: 0.0813 - bounding_box_loss: 0.0110 - class_label_loss: 0.0703 - bounding_box_accuracy: 0.6400 - class_label_accuracy: 0.9776 - val_loss: 0.4837 - val_bounding_box_loss: 0.0095 - val_class_label_loss: 0.4743 - val_bounding_box_accuracy: 0.6538 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 18/30\n",
    "# 30/30 [==============================] - 135s 5s/step - loss: 0.0710 - bounding_box_loss: 0.0110 - class_label_loss: 0.0600 - bounding_box_accuracy: 0.5983 - class_label_accuracy: 0.9829 - val_loss: 0.4353 - val_bounding_box_loss: 0.0116 - val_class_label_loss: 0.4237 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 19/30\n",
    "# 30/30 [==============================] - 131s 4s/step - loss: 0.0784 - bounding_box_loss: 0.0107 - class_label_loss: 0.0676 - bounding_box_accuracy: 0.6229 - class_label_accuracy: 0.9861 - val_loss: 0.4985 - val_bounding_box_loss: 0.0094 - val_class_label_loss: 0.4890 - val_bounding_box_accuracy: 0.6154 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 20/30\n",
    "# 30/30 [==============================] - 129s 4s/step - loss: 0.0672 - bounding_box_loss: 0.0111 - class_label_loss: 0.0560 - bounding_box_accuracy: 0.6335 - class_label_accuracy: 0.9818 - val_loss: 0.4297 - val_bounding_box_loss: 0.0098 - val_class_label_loss: 0.4199 - val_bounding_box_accuracy: 0.6635 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 21/30\n",
    "# 30/30 [==============================] - 131s 4s/step - loss: 0.0567 - bounding_box_loss: 0.0106 - class_label_loss: 0.0461 - bounding_box_accuracy: 0.6453 - class_label_accuracy: 0.9904 - val_loss: 0.4811 - val_bounding_box_loss: 0.0100 - val_class_label_loss: 0.4711 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 22/30\n",
    "# 30/30 [==============================] - 131s 4s/step - loss: 0.0403 - bounding_box_loss: 0.0106 - class_label_loss: 0.0297 - bounding_box_accuracy: 0.6271 - class_label_accuracy: 0.9936 - val_loss: 0.5083 - val_bounding_box_loss: 0.0093 - val_class_label_loss: 0.4991 - val_bounding_box_accuracy: 0.6346 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 23/30\n",
    "# 30/30 [==============================] - 131s 4s/step - loss: 0.0663 - bounding_box_loss: 0.0101 - class_label_loss: 0.0563 - bounding_box_accuracy: 0.6442 - class_label_accuracy: 0.9829 - val_loss: 0.4377 - val_bounding_box_loss: 0.0103 - val_class_label_loss: 0.4274 - val_bounding_box_accuracy: 0.6346 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 24/30\n",
    "# 30/30 [==============================] - 130s 4s/step - loss: 0.0611 - bounding_box_loss: 0.0100 - class_label_loss: 0.0511 - bounding_box_accuracy: 0.6400 - class_label_accuracy: 0.9872 - val_loss: 0.4534 - val_bounding_box_loss: 0.0090 - val_class_label_loss: 0.4444 - val_bounding_box_accuracy: 0.6442 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 25/30\n",
    "# 30/30 [==============================] - 136s 5s/step - loss: 0.0578 - bounding_box_loss: 0.0099 - class_label_loss: 0.0479 - bounding_box_accuracy: 0.6603 - class_label_accuracy: 0.9861 - val_loss: 0.5572 - val_bounding_box_loss: 0.0088 - val_class_label_loss: 0.5484 - val_bounding_box_accuracy: 0.6827 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 26/30\n",
    "# 30/30 [==============================] - 137s 5s/step - loss: 0.0503 - bounding_box_loss: 0.0099 - class_label_loss: 0.0404 - bounding_box_accuracy: 0.6571 - class_label_accuracy: 0.9893 - val_loss: 0.5039 - val_bounding_box_loss: 0.0100 - val_class_label_loss: 0.4939 - val_bounding_box_accuracy: 0.6538 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 27/30\n",
    "# 30/30 [==============================] - 132s 4s/step - loss: 0.0595 - bounding_box_loss: 0.0099 - class_label_loss: 0.0496 - bounding_box_accuracy: 0.6389 - class_label_accuracy: 0.9936 - val_loss: 0.5094 - val_bounding_box_loss: 0.0098 - val_class_label_loss: 0.4996 - val_bounding_box_accuracy: 0.6346 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 28/30\n",
    "# 30/30 [==============================] - 132s 4s/step - loss: 0.0413 - bounding_box_loss: 0.0096 - class_label_loss: 0.0316 - bounding_box_accuracy: 0.6592 - class_label_accuracy: 0.9882 - val_loss: 0.5161 - val_bounding_box_loss: 0.0098 - val_class_label_loss: 0.5063 - val_bounding_box_accuracy: 0.6058 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 29/30\n",
    "# 30/30 [==============================] - 128s 4s/step - loss: 0.0287 - bounding_box_loss: 0.0097 - class_label_loss: 0.0191 - bounding_box_accuracy: 0.6763 - class_label_accuracy: 0.9947 - val_loss: 0.6341 - val_bounding_box_loss: 0.0091 - val_class_label_loss: 0.6250 - val_bounding_box_accuracy: 0.6154 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 30/30\n",
    "# 30/30 [==============================] - 129s 4s/step - loss: 0.0345 - bounding_box_loss: 0.0093 - class_label_loss: 0.0253 - bounding_box_accuracy: 0.6645 - class_label_accuracy: 0.9925 - val_loss: 0.6163 - val_bounding_box_loss: 0.0088 - val_class_label_loss: 0.6074 - val_bounding_box_accuracy: 0.6442 - val_class_label_accuracy: 0.9327"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting loss for modeling training\n",
    "plot_loss(model_name='densenet201_model_v3', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training total loss during densenet201_model_v3 training: 0.03454061970114708\n",
    "# Final epoch training class label loss during densenet201_model_v3 training: 0.025253819301724434\n",
    "# Final epoch training bounding box loss during densenet201_model_v3 training: 0.009286798536777496\n",
    "# Final epoch validation total loss during densenet201_model_v3 training: 0.6162612438201904\n",
    "# Final epoch validation class label loss during densenet201_model_v3 training: 0.6074371933937073\n",
    "# Final epoch validation bounding box loss during densenet201_model_v3 training: 0.008824067190289497"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting accuracy for modeling training\n",
    "plot_accuracy(model_name='densenet201_model_v3', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training class label accurracy during densenet201_model_v3 training: 0.992521345615387\n",
    "# Final epoch validation class label accurracy during densenet201_model_v3 training: 0.932692289352417"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating model for mAP metrics and inference time and storing result\n",
    "result_val = evaluate_val(model=densenet201_model_v3, model_name='densenet201_model_v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC PASCAL mAP for densenet201_model_v3 validation data: 0.3423176407814026\n",
    "# VOC PASCAL mAP in all points for densenet201_model_v3 validation data: 0.3218238055706024\n",
    "# COCO mAP for densenet201_model_v3 validation data: 0.414972722530365\n",
    "# Average inference time for densenet201_model_v3 validation data: 0.20755169024834266"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending results of model evaluation to results dataframe (df_results)\n",
    "df_results_val = df_results_val.append(result_val, ignore_index=True)\n",
    "df_results_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NASNetLarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading network but leaving off output layers\n",
    "nasnetlarge = NASNetLarge(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "# freezing all backbone layers during training so they are not updated\n",
    "nasnetlarge.trainable = False\n",
    "\n",
    "# flattening the max-pooling output of network\n",
    "flatten = nasnetlarge.output\n",
    "flat = Flatten()(flatten)\n",
    "\n",
    "# constructing an output layer to predict bounding box coordinates\n",
    "bbox_head1 = Dense(128, activation=\"relu\")(flat)\n",
    "bbox_head2 = Dense(64, activation=\"relu\")(bbox_head1)\n",
    "bbox_head3 = Dense(32, activation=\"relu\")(bbox_head2)\n",
    "bbox_head_v1 = Dense(4, activation=\"sigmoid\", name=\"bounding_box\")(bbox_head3)\n",
    "\n",
    "# constructing an output layer to predict class labels\n",
    "class_head1 = Dense(512, activation=\"relu\")(flat)\n",
    "class_head2 = Dropout(0.5)(class_head1)\n",
    "class_head3 = Dense(512, activation=\"relu\")(class_head2)\n",
    "class_head4 = Dropout(0.5)(class_head3)\n",
    "class_head_v1 = Dense(2, activation=\"sigmoid\", name=\"class_label\")(class_head4)\n",
    "\n",
    "# creating a model that accepts an input image and outputs bounding box coordinates and a class label\n",
    "nasnetlarge_model_v1 = Model(inputs=nasnetlarge.input, outputs=(bbox_head_v1, class_head_v1))\n",
    "\n",
    "# creating dictionary for setting loss functions\n",
    "losses = {\"class_label\": \"binary_crossentropy\", \"bounding_box\": \"mean_squared_error\"}\n",
    "\n",
    "# creating dictionry for weighting loss functions\n",
    "loss_weights = {\"class_label\": 1.0, \"bounding_box\": 1.0}\n",
    "\n",
    "# initilizing an \"Adam\" optimizer with learning rate of 0.0001\n",
    "opt = Adam(learning_rate = 0.0001)\n",
    "\n",
    "# compiling model\n",
    "nasnetlarge_model_v1.compile(loss=losses, optimizer=opt, metrics=[\"accuracy\"], loss_weights=loss_weights)\n",
    "print(nasnetlarge_model_v1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing a dictionary for our training outputs\n",
    "train_targets = {\"class_label\": labels_train, \"bounding_box\": bboxes_train}\n",
    "\n",
    "# constructing a dictionary for our test outputs\n",
    "val_targets = {\"class_label\": labels_val, \"bounding_box\": bboxes_val}\n",
    "\n",
    "# training model for class and bounding box predictions\n",
    "H = nasnetlarge_model_v1.fit(x_train, train_targets, validation_data=(x_val, val_targets), batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "\n",
    "# saving trained model\n",
    "nasnetlarge_model_v1.save('../Models/nasnetlarge_model_v1.h5', save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 1/30\n",
    "# 30/30 [==============================] - 187s 6s/step - loss: 0.4258 - bounding_box_loss: 0.0346 - class_label_loss: 0.3912 - bounding_box_accuracy: 0.5321 - class_label_accuracy: 0.9156 - val_loss: 0.5280 - val_bounding_box_loss: 0.0193 - val_class_label_loss: 0.5087 - val_bounding_box_accuracy: 0.5769 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 2/30\n",
    "# 30/30 [==============================] - 186s 6s/step - loss: 0.2482 - bounding_box_loss: 0.0146 - class_label_loss: 0.2335 - bounding_box_accuracy: 0.5962 - class_label_accuracy: 0.9701 - val_loss: 0.5871 - val_bounding_box_loss: 0.0111 - val_class_label_loss: 0.5760 - val_bounding_box_accuracy: 0.5769 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 3/30\n",
    "# 30/30 [==============================] - 189s 6s/step - loss: 0.1806 - bounding_box_loss: 0.0093 - class_label_loss: 0.1713 - bounding_box_accuracy: 0.6485 - class_label_accuracy: 0.9701 - val_loss: 0.5311 - val_bounding_box_loss: 0.0099 - val_class_label_loss: 0.5212 - val_bounding_box_accuracy: 0.5865 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 4/30\n",
    "# 30/30 [==============================] - 188s 6s/step - loss: 0.1590 - bounding_box_loss: 0.0074 - class_label_loss: 0.1516 - bounding_box_accuracy: 0.6741 - class_label_accuracy: 0.9722 - val_loss: 0.5054 - val_bounding_box_loss: 0.0093 - val_class_label_loss: 0.4961 - val_bounding_box_accuracy: 0.6058 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 5/30\n",
    "# 30/30 [==============================] - 189s 6s/step - loss: 0.0948 - bounding_box_loss: 0.0063 - class_label_loss: 0.0886 - bounding_box_accuracy: 0.7062 - class_label_accuracy: 0.9754 - val_loss: 0.5368 - val_bounding_box_loss: 0.0088 - val_class_label_loss: 0.5280 - val_bounding_box_accuracy: 0.5865 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 6/30\n",
    "# 30/30 [==============================] - 189s 6s/step - loss: 0.0893 - bounding_box_loss: 0.0048 - class_label_loss: 0.0845 - bounding_box_accuracy: 0.7361 - class_label_accuracy: 0.9808 - val_loss: 0.4312 - val_bounding_box_loss: 0.0080 - val_class_label_loss: 0.4232 - val_bounding_box_accuracy: 0.6442 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 7/30\n",
    "# 30/30 [==============================] - 189s 6s/step - loss: 0.0774 - bounding_box_loss: 0.0039 - class_label_loss: 0.0735 - bounding_box_accuracy: 0.7671 - class_label_accuracy: 0.9818 - val_loss: 0.4691 - val_bounding_box_loss: 0.0079 - val_class_label_loss: 0.4612 - val_bounding_box_accuracy: 0.6827 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 8/30\n",
    "# 30/30 [==============================] - 190s 6s/step - loss: 0.1002 - bounding_box_loss: 0.0034 - class_label_loss: 0.0968 - bounding_box_accuracy: 0.7778 - class_label_accuracy: 0.9797 - val_loss: 0.5277 - val_bounding_box_loss: 0.0082 - val_class_label_loss: 0.5195 - val_bounding_box_accuracy: 0.6442 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 9/30\n",
    "# 30/30 [==============================] - 190s 6s/step - loss: 0.0659 - bounding_box_loss: 0.0033 - class_label_loss: 0.0626 - bounding_box_accuracy: 0.7970 - class_label_accuracy: 0.9818 - val_loss: 0.6035 - val_bounding_box_loss: 0.0078 - val_class_label_loss: 0.5958 - val_bounding_box_accuracy: 0.6923 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 10/30\n",
    "# 30/30 [==============================] - 189s 6s/step - loss: 0.0441 - bounding_box_loss: 0.0031 - class_label_loss: 0.0410 - bounding_box_accuracy: 0.8184 - class_label_accuracy: 0.9904 - val_loss: 0.6824 - val_bounding_box_loss: 0.0073 - val_class_label_loss: 0.6751 - val_bounding_box_accuracy: 0.7308 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 11/30\n",
    "# 30/30 [==============================] - 189s 6s/step - loss: 0.0807 - bounding_box_loss: 0.0026 - class_label_loss: 0.0781 - bounding_box_accuracy: 0.8323 - class_label_accuracy: 0.9808 - val_loss: 0.5191 - val_bounding_box_loss: 0.0080 - val_class_label_loss: 0.5111 - val_bounding_box_accuracy: 0.7019 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 12/30\n",
    "# 30/30 [==============================] - 188s 6s/step - loss: 0.0603 - bounding_box_loss: 0.0021 - class_label_loss: 0.0583 - bounding_box_accuracy: 0.8718 - class_label_accuracy: 0.9893 - val_loss: 0.6809 - val_bounding_box_loss: 0.0074 - val_class_label_loss: 0.6735 - val_bounding_box_accuracy: 0.7115 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 13/30\n",
    "# 30/30 [==============================] - 188s 6s/step - loss: 0.0598 - bounding_box_loss: 0.0020 - class_label_loss: 0.0578 - bounding_box_accuracy: 0.8590 - class_label_accuracy: 0.9904 - val_loss: 0.5480 - val_bounding_box_loss: 0.0071 - val_class_label_loss: 0.5409 - val_bounding_box_accuracy: 0.7404 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 14/30\n",
    "# 30/30 [==============================] - 190s 6s/step - loss: 0.0568 - bounding_box_loss: 0.0019 - class_label_loss: 0.0548 - bounding_box_accuracy: 0.8771 - class_label_accuracy: 0.9893 - val_loss: 0.5194 - val_bounding_box_loss: 0.0077 - val_class_label_loss: 0.5116 - val_bounding_box_accuracy: 0.7692 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 15/30\n",
    "# 30/30 [==============================] - 192s 6s/step - loss: 0.0415 - bounding_box_loss: 0.0016 - class_label_loss: 0.0398 - bounding_box_accuracy: 0.8750 - class_label_accuracy: 0.9882 - val_loss: 0.5325 - val_bounding_box_loss: 0.0084 - val_class_label_loss: 0.5240 - val_bounding_box_accuracy: 0.7596 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 16/30\n",
    "# 30/30 [==============================] - 190s 6s/step - loss: 0.0376 - bounding_box_loss: 0.0019 - class_label_loss: 0.0357 - bounding_box_accuracy: 0.8814 - class_label_accuracy: 0.9936 - val_loss: 0.6315 - val_bounding_box_loss: 0.0076 - val_class_label_loss: 0.6238 - val_bounding_box_accuracy: 0.7596 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 17/30\n",
    "# 30/30 [==============================] - 190s 6s/step - loss: 0.0324 - bounding_box_loss: 0.0018 - class_label_loss: 0.0306 - bounding_box_accuracy: 0.8750 - class_label_accuracy: 0.9915 - val_loss: 0.6710 - val_bounding_box_loss: 0.0069 - val_class_label_loss: 0.6641 - val_bounding_box_accuracy: 0.7404 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 18/30\n",
    "# 30/30 [==============================] - 189s 6s/step - loss: 0.0401 - bounding_box_loss: 0.0017 - class_label_loss: 0.0383 - bounding_box_accuracy: 0.8782 - class_label_accuracy: 0.9882 - val_loss: 0.6118 - val_bounding_box_loss: 0.0074 - val_class_label_loss: 0.6044 - val_bounding_box_accuracy: 0.7500 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 19/30\n",
    "# 30/30 [==============================] - 190s 6s/step - loss: 0.0478 - bounding_box_loss: 0.0017 - class_label_loss: 0.0461 - bounding_box_accuracy: 0.8878 - class_label_accuracy: 0.9882 - val_loss: 0.5121 - val_bounding_box_loss: 0.0068 - val_class_label_loss: 0.5053 - val_bounding_box_accuracy: 0.7981 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 20/30\n",
    "# 30/30 [==============================] - 189s 6s/step - loss: 0.0426 - bounding_box_loss: 0.0017 - class_label_loss: 0.0409 - bounding_box_accuracy: 0.8718 - class_label_accuracy: 0.9904 - val_loss: 0.6970 - val_bounding_box_loss: 0.0079 - val_class_label_loss: 0.6892 - val_bounding_box_accuracy: 0.7308 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 21/30\n",
    "# 30/30 [==============================] - 189s 6s/step - loss: 0.0282 - bounding_box_loss: 0.0016 - class_label_loss: 0.0266 - bounding_box_accuracy: 0.8964 - class_label_accuracy: 0.9915 - val_loss: 0.8258 - val_bounding_box_loss: 0.0068 - val_class_label_loss: 0.8190 - val_bounding_box_accuracy: 0.7692 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 22/30\n",
    "# 30/30 [==============================] - 190s 6s/step - loss: 0.0287 - bounding_box_loss: 0.0015 - class_label_loss: 0.0273 - bounding_box_accuracy: 0.8996 - class_label_accuracy: 0.9936 - val_loss: 0.8013 - val_bounding_box_loss: 0.0070 - val_class_label_loss: 0.7943 - val_bounding_box_accuracy: 0.7500 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 23/30\n",
    "# 30/30 [==============================] - 189s 6s/step - loss: 0.0437 - bounding_box_loss: 0.0012 - class_label_loss: 0.0425 - bounding_box_accuracy: 0.9071 - class_label_accuracy: 0.9904 - val_loss: 0.8306 - val_bounding_box_loss: 0.0074 - val_class_label_loss: 0.8232 - val_bounding_box_accuracy: 0.7500 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 24/30\n",
    "# 30/30 [==============================] - 190s 6s/step - loss: 0.0472 - bounding_box_loss: 0.0014 - class_label_loss: 0.0458 - bounding_box_accuracy: 0.9049 - class_label_accuracy: 0.9936 - val_loss: 0.6911 - val_bounding_box_loss: 0.0074 - val_class_label_loss: 0.6837 - val_bounding_box_accuracy: 0.7404 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 25/30\n",
    "# 30/30 [==============================] - 188s 6s/step - loss: 0.0211 - bounding_box_loss: 0.0014 - class_label_loss: 0.0197 - bounding_box_accuracy: 0.9017 - class_label_accuracy: 0.9957 - val_loss: 0.6987 - val_bounding_box_loss: 0.0076 - val_class_label_loss: 0.6911 - val_bounding_box_accuracy: 0.7596 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 26/30\n",
    "# 30/30 [==============================] - 189s 6s/step - loss: 0.0276 - bounding_box_loss: 0.0015 - class_label_loss: 0.0262 - bounding_box_accuracy: 0.9006 - class_label_accuracy: 0.9915 - val_loss: 0.8946 - val_bounding_box_loss: 0.0076 - val_class_label_loss: 0.8870 - val_bounding_box_accuracy: 0.7404 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 27/30\n",
    "# 30/30 [==============================] - 189s 6s/step - loss: 0.0140 - bounding_box_loss: 0.0015 - class_label_loss: 0.0125 - bounding_box_accuracy: 0.9060 - class_label_accuracy: 0.9979 - val_loss: 0.8941 - val_bounding_box_loss: 0.0072 - val_class_label_loss: 0.8869 - val_bounding_box_accuracy: 0.7212 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 28/30\n",
    "# 30/30 [==============================] - 191s 6s/step - loss: 0.0259 - bounding_box_loss: 0.0014 - class_label_loss: 0.0245 - bounding_box_accuracy: 0.8985 - class_label_accuracy: 0.9957 - val_loss: 0.8546 - val_bounding_box_loss: 0.0073 - val_class_label_loss: 0.8474 - val_bounding_box_accuracy: 0.7212 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 29/30\n",
    "# 30/30 [==============================] - 189s 6s/step - loss: 0.0288 - bounding_box_loss: 0.0013 - class_label_loss: 0.0274 - bounding_box_accuracy: 0.9060 - class_label_accuracy: 0.9968 - val_loss: 0.9131 - val_bounding_box_loss: 0.0071 - val_class_label_loss: 0.9059 - val_bounding_box_accuracy: 0.7692 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 30/30\n",
    "# 30/30 [==============================] - 188s 6s/step - loss: 0.0302 - bounding_box_loss: 0.0012 - class_label_loss: 0.0290 - bounding_box_accuracy: 0.9060 - class_label_accuracy: 0.9957 - val_loss: 0.7424 - val_bounding_box_loss: 0.0069 - val_class_label_loss: 0.7356 - val_bounding_box_accuracy: 0.7404 - val_class_label_accuracy: 0.9327"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting loss for modeling training\n",
    "plot_loss(model_name='nasnetlarge_model_v1', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training total loss during nasnetlarge_model_v1 training: 0.03019646927714348\n",
    "# Final epoch training class label loss during nasnetlarge_model_v1 training: 0.028972936794161797\n",
    "# Final epoch training bounding box loss during nasnetlarge_model_v1 training: 0.0012235351605340838\n",
    "# Final epoch validation total loss during nasnetlarge_model_v1 training: 0.7424468994140625\n",
    "# Final epoch validation class label loss during nasnetlarge_model_v1 training: 0.7355585098266602\n",
    "# Final epoch validation bounding box loss during nasnetlarge_model_v1 training: 0.006888363976031542"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting accuracy for modeling training\n",
    "plot_accuracy(model_name='nasnetlarge_model_v1', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training class label accurracy during nasnetlarge_model_v1 training: 0.995726466178894\n",
    "# Final epoch validation class label accurracy during nasnetlarge_model_v1 training: 0.932692289352417"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating model for mAP metrics and inference time and storing result\n",
    "result_val = evaluate_val(model=nasnetlarge_model_v1, model_name='nasnetlarge_model_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC PASCAL mAP for nasnetlarge_model_v1 validation data: 0.45195767283439636\n",
    "# VOC PASCAL mAP in all points for nasnetlarge_model_v1 validation data: 0.43749165534973145\n",
    "# COCO mAP for nasnetlarge_model_v1 validation data: 0.5148961544036865\n",
    "# Average inference time for nasnetlarge_model_v1 validation data: 0.27340717499072736"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending results of model evaluation to results dataframe (df_results)\n",
    "df_results_val = df_results_val.append(result_val, ignore_index=True)\n",
    "df_results_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading network but leaving off output layers\n",
    "nasnetlarge = NASNetLarge(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "# freezing all backbone layers during training so they are not updated\n",
    "nasnetlarge.trainable = False\n",
    "\n",
    "# flattening the max-pooling output of network\n",
    "flatten = nasnetlarge.output\n",
    "flat = Flatten()(flatten)\n",
    "\n",
    "# constructing an output layer to predict bounding box coordinates\n",
    "bbox_head1 = Dense(128, activation=\"relu\")(flat)\n",
    "bbox_head2 = Dropout(0.5)(bbox_head1)\n",
    "bbox_head3 = Dense(64, activation=\"relu\")(bbox_head2)\n",
    "bbox_head4 = Dropout(0.5)(bbox_head3)\n",
    "bbox_head5 = Dense(32, activation=\"relu\")(bbox_head4)\n",
    "bbox_head6 = Dropout(0.5)(bbox_head5)\n",
    "bbox_head_v2 = Dense(4, activation=\"sigmoid\", name=\"bounding_box\")(bbox_head6)\n",
    "\n",
    "# constructing an output layer to predict class labels\n",
    "class_head1 = Dense(512, activation=\"relu\")(flat)\n",
    "class_head2 = Dropout(0.5)(class_head1)\n",
    "class_head3 = Dense(512, activation=\"relu\")(class_head2)\n",
    "class_head4 = Dropout(0.5)(class_head3)\n",
    "class_head5 = Dense(512, activation=\"relu\")(class_head4)\n",
    "class_head6 = Dropout(0.5)(class_head5)\n",
    "class_head_v2 = Dense(2, activation=\"sigmoid\", name=\"class_label\")(class_head6)\n",
    "\n",
    "# creating a model that accepts an input image and outputs bounding box coordinates and a class label\n",
    "nasnetlarge_model_v2 = Model(inputs=nasnetlarge.input, outputs=(bbox_head_v2, class_head_v2))\n",
    "\n",
    "# creating dictionary for setting loss functions\n",
    "losses = {\"class_label\": \"binary_crossentropy\", \"bounding_box\": \"mean_squared_error\"}\n",
    "\n",
    "# creating dictionry for weighting loss functions\n",
    "loss_weights = {\"class_label\": 1.0, \"bounding_box\": 1.0}\n",
    "\n",
    "# initilizing an \"Adam\" optimizer with learning rate of 0.0001\n",
    "opt = Adam(learning_rate = 0.0001)\n",
    "\n",
    "# compiling model\n",
    "nasnetlarge_model_v2.compile(loss=losses, optimizer=opt, metrics=[\"accuracy\"], loss_weights=loss_weights)\n",
    "print(nasnetlarge_model_v2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing a dictionary for our training outputs\n",
    "train_targets = {\"class_label\": labels_train, \"bounding_box\": bboxes_train}\n",
    "\n",
    "# constructing a dictionary for our test outputs\n",
    "val_targets = {\"class_label\": labels_val, \"bounding_box\": bboxes_val}\n",
    "\n",
    "# training model for class and bounding box predictions\n",
    "H = nasnetlarge_model_v2.fit(x_train, train_targets, validation_data=(x_val, val_targets), batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "\n",
    "# saving trained model\n",
    "nasnetlarge_model_v2.save('../Models/nasnetlarge_model_v2.h5', save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 1/30\n",
    "# 30/30 [==============================] - 187s 6s/step - loss: 0.6802 - bounding_box_loss: 0.1687 - class_label_loss: 0.5116 - bounding_box_accuracy: 0.3237 - class_label_accuracy: 0.8494 - val_loss: 0.5748 - val_bounding_box_loss: 0.0304 - val_class_label_loss: 0.5444 - val_bounding_box_accuracy: 0.4712 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 2/30\n",
    "# 30/30 [==============================] - 181s 6s/step - loss: 0.4717 - bounding_box_loss: 0.1490 - class_label_loss: 0.3227 - bounding_box_accuracy: 0.3761 - class_label_accuracy: 0.9509 - val_loss: 0.5577 - val_bounding_box_loss: 0.0352 - val_class_label_loss: 0.5225 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 3/30\n",
    "# 30/30 [==============================] - 182s 6s/step - loss: 0.4382 - bounding_box_loss: 0.1273 - class_label_loss: 0.3109 - bounding_box_accuracy: 0.3504 - class_label_accuracy: 0.9444 - val_loss: 0.4787 - val_bounding_box_loss: 0.0618 - val_class_label_loss: 0.4168 - val_bounding_box_accuracy: 0.4808 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 4/30\n",
    "# 30/30 [==============================] - 181s 6s/step - loss: 0.2957 - bounding_box_loss: 0.1172 - class_label_loss: 0.1785 - bounding_box_accuracy: 0.3675 - class_label_accuracy: 0.9615 - val_loss: 0.5194 - val_bounding_box_loss: 0.0569 - val_class_label_loss: 0.4624 - val_bounding_box_accuracy: 0.4135 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 5/30\n",
    "# 30/30 [==============================] - 181s 6s/step - loss: 0.2821 - bounding_box_loss: 0.1120 - class_label_loss: 0.1701 - bounding_box_accuracy: 0.3579 - class_label_accuracy: 0.9679 - val_loss: 0.5398 - val_bounding_box_loss: 0.0627 - val_class_label_loss: 0.4771 - val_bounding_box_accuracy: 0.4038 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 6/30\n",
    "# 30/30 [==============================] - 182s 6s/step - loss: 0.2828 - bounding_box_loss: 0.1053 - class_label_loss: 0.1774 - bounding_box_accuracy: 0.4338 - class_label_accuracy: 0.9669 - val_loss: 0.5433 - val_bounding_box_loss: 0.0471 - val_class_label_loss: 0.4962 - val_bounding_box_accuracy: 0.5385 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 7/30\n",
    "# 30/30 [==============================] - 183s 6s/step - loss: 0.2345 - bounding_box_loss: 0.1056 - class_label_loss: 0.1289 - bounding_box_accuracy: 0.3996 - class_label_accuracy: 0.9722 - val_loss: 0.4550 - val_bounding_box_loss: 0.0372 - val_class_label_loss: 0.4177 - val_bounding_box_accuracy: 0.4135 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 8/30\n",
    "# 30/30 [==============================] - 185s 6s/step - loss: 0.2246 - bounding_box_loss: 0.0943 - class_label_loss: 0.1304 - bounding_box_accuracy: 0.4038 - class_label_accuracy: 0.9744 - val_loss: 0.5089 - val_bounding_box_loss: 0.0495 - val_class_label_loss: 0.4595 - val_bounding_box_accuracy: 0.4231 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 9/30\n",
    "# 30/30 [==============================] - 189s 6s/step - loss: 0.2267 - bounding_box_loss: 0.0961 - class_label_loss: 0.1306 - bounding_box_accuracy: 0.3910 - class_label_accuracy: 0.9754 - val_loss: 0.5034 - val_bounding_box_loss: 0.0364 - val_class_label_loss: 0.4671 - val_bounding_box_accuracy: 0.4423 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 10/30\n",
    "# 30/30 [==============================] - 189s 6s/step - loss: 0.1921 - bounding_box_loss: 0.0853 - class_label_loss: 0.1069 - bounding_box_accuracy: 0.4370 - class_label_accuracy: 0.9797 - val_loss: 0.4639 - val_bounding_box_loss: 0.0368 - val_class_label_loss: 0.4271 - val_bounding_box_accuracy: 0.4231 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 11/30\n",
    "# 30/30 [==============================] - 191s 6s/step - loss: 0.1581 - bounding_box_loss: 0.0813 - class_label_loss: 0.0768 - bounding_box_accuracy: 0.4551 - class_label_accuracy: 0.9850 - val_loss: 0.4965 - val_bounding_box_loss: 0.0294 - val_class_label_loss: 0.4671 - val_bounding_box_accuracy: 0.4615 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 12/30\n",
    "# 30/30 [==============================] - 190s 6s/step - loss: 0.1635 - bounding_box_loss: 0.0760 - class_label_loss: 0.0875 - bounding_box_accuracy: 0.4583 - class_label_accuracy: 0.9808 - val_loss: 0.5468 - val_bounding_box_loss: 0.0236 - val_class_label_loss: 0.5233 - val_bounding_box_accuracy: 0.5769 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 13/30\n",
    "# 30/30 [==============================] - 191s 6s/step - loss: 0.1459 - bounding_box_loss: 0.0743 - class_label_loss: 0.0716 - bounding_box_accuracy: 0.4434 - class_label_accuracy: 0.9818 - val_loss: 0.5481 - val_bounding_box_loss: 0.0311 - val_class_label_loss: 0.5170 - val_bounding_box_accuracy: 0.4808 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 14/30\n",
    "# 30/30 [==============================] - 192s 6s/step - loss: 0.1428 - bounding_box_loss: 0.0695 - class_label_loss: 0.0733 - bounding_box_accuracy: 0.4690 - class_label_accuracy: 0.9861 - val_loss: 0.5110 - val_bounding_box_loss: 0.0218 - val_class_label_loss: 0.4892 - val_bounding_box_accuracy: 0.3846 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 15/30\n",
    "# 30/30 [==============================] - 190s 6s/step - loss: 0.1053 - bounding_box_loss: 0.0676 - class_label_loss: 0.0377 - bounding_box_accuracy: 0.4487 - class_label_accuracy: 0.9893 - val_loss: 0.6122 - val_bounding_box_loss: 0.0227 - val_class_label_loss: 0.5895 - val_bounding_box_accuracy: 0.5192 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 16/30\n",
    "# 30/30 [==============================] - 191s 6s/step - loss: 0.1085 - bounding_box_loss: 0.0624 - class_label_loss: 0.0461 - bounding_box_accuracy: 0.4615 - class_label_accuracy: 0.9882 - val_loss: 0.5889 - val_bounding_box_loss: 0.0156 - val_class_label_loss: 0.5733 - val_bounding_box_accuracy: 0.4712 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 17/30\n",
    "# 30/30 [==============================] - 192s 6s/step - loss: 0.0942 - bounding_box_loss: 0.0578 - class_label_loss: 0.0364 - bounding_box_accuracy: 0.4690 - class_label_accuracy: 0.9893 - val_loss: 0.6458 - val_bounding_box_loss: 0.0173 - val_class_label_loss: 0.6285 - val_bounding_box_accuracy: 0.4519 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 18/30\n",
    "# 30/30 [==============================] - 189s 6s/step - loss: 0.0988 - bounding_box_loss: 0.0573 - class_label_loss: 0.0415 - bounding_box_accuracy: 0.4594 - class_label_accuracy: 0.9872 - val_loss: 0.6560 - val_bounding_box_loss: 0.0182 - val_class_label_loss: 0.6378 - val_bounding_box_accuracy: 0.4808 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 19/30\n",
    "# 30/30 [==============================] - 186s 6s/step - loss: 0.1056 - bounding_box_loss: 0.0550 - class_label_loss: 0.0506 - bounding_box_accuracy: 0.4850 - class_label_accuracy: 0.9904 - val_loss: 0.6147 - val_bounding_box_loss: 0.0159 - val_class_label_loss: 0.5988 - val_bounding_box_accuracy: 0.3942 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 20/30\n",
    "# 30/30 [==============================] - 187s 6s/step - loss: 0.0835 - bounding_box_loss: 0.0553 - class_label_loss: 0.0281 - bounding_box_accuracy: 0.4690 - class_label_accuracy: 0.9915 - val_loss: 0.6014 - val_bounding_box_loss: 0.0178 - val_class_label_loss: 0.5836 - val_bounding_box_accuracy: 0.3846 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 21/30\n",
    "# 30/30 [==============================] - 186s 6s/step - loss: 0.0963 - bounding_box_loss: 0.0506 - class_label_loss: 0.0458 - bounding_box_accuracy: 0.4669 - class_label_accuracy: 0.9925 - val_loss: 0.6746 - val_bounding_box_loss: 0.0177 - val_class_label_loss: 0.6569 - val_bounding_box_accuracy: 0.3942 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 22/30\n",
    "# 30/30 [==============================] - 189s 6s/step - loss: 0.0775 - bounding_box_loss: 0.0510 - class_label_loss: 0.0265 - bounding_box_accuracy: 0.5107 - class_label_accuracy: 0.9936 - val_loss: 0.5938 - val_bounding_box_loss: 0.0156 - val_class_label_loss: 0.5782 - val_bounding_box_accuracy: 0.3654 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 23/30\n",
    "# 30/30 [==============================] - 189s 6s/step - loss: 0.0847 - bounding_box_loss: 0.0477 - class_label_loss: 0.0369 - bounding_box_accuracy: 0.4947 - class_label_accuracy: 0.9904 - val_loss: 0.6907 - val_bounding_box_loss: 0.0169 - val_class_label_loss: 0.6738 - val_bounding_box_accuracy: 0.3942 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 24/30\n",
    "# 30/30 [==============================] - 190s 6s/step - loss: 0.0684 - bounding_box_loss: 0.0443 - class_label_loss: 0.0241 - bounding_box_accuracy: 0.4519 - class_label_accuracy: 0.9925 - val_loss: 0.7593 - val_bounding_box_loss: 0.0167 - val_class_label_loss: 0.7427 - val_bounding_box_accuracy: 0.4038 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 25/30\n",
    "# 30/30 [==============================] - 189s 6s/step - loss: 0.0820 - bounding_box_loss: 0.0489 - class_label_loss: 0.0331 - bounding_box_accuracy: 0.4776 - class_label_accuracy: 0.9893 - val_loss: 0.7286 - val_bounding_box_loss: 0.0163 - val_class_label_loss: 0.7124 - val_bounding_box_accuracy: 0.4231 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 26/30\n",
    "# 30/30 [==============================] - 190s 6s/step - loss: 0.0732 - bounding_box_loss: 0.0474 - class_label_loss: 0.0258 - bounding_box_accuracy: 0.4776 - class_label_accuracy: 0.9957 - val_loss: 0.6933 - val_bounding_box_loss: 0.0150 - val_class_label_loss: 0.6783 - val_bounding_box_accuracy: 0.3846 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 27/30\n",
    "# 30/30 [==============================] - 193s 6s/step - loss: 0.0768 - bounding_box_loss: 0.0426 - class_label_loss: 0.0342 - bounding_box_accuracy: 0.4872 - class_label_accuracy: 0.9925 - val_loss: 0.7553 - val_bounding_box_loss: 0.0152 - val_class_label_loss: 0.7401 - val_bounding_box_accuracy: 0.3846 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 28/30\n",
    "# 30/30 [==============================] - 190s 6s/step - loss: 0.0646 - bounding_box_loss: 0.0433 - class_label_loss: 0.0213 - bounding_box_accuracy: 0.4679 - class_label_accuracy: 0.9947 - val_loss: 0.7704 - val_bounding_box_loss: 0.0158 - val_class_label_loss: 0.7546 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 29/30\n",
    "# 30/30 [==============================] - 190s 6s/step - loss: 0.0752 - bounding_box_loss: 0.0430 - class_label_loss: 0.0322 - bounding_box_accuracy: 0.4765 - class_label_accuracy: 0.9925 - val_loss: 0.7285 - val_bounding_box_loss: 0.0163 - val_class_label_loss: 0.7123 - val_bounding_box_accuracy: 0.3942 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 30/30\n",
    "# 30/30 [==============================] - 189s 6s/step - loss: 0.0679 - bounding_box_loss: 0.0396 - class_label_loss: 0.0282 - bounding_box_accuracy: 0.5192 - class_label_accuracy: 0.9925 - val_loss: 0.7523 - val_bounding_box_loss: 0.0145 - val_class_label_loss: 0.7378 - val_bounding_box_accuracy: 0.3846 - val_class_label_accuracy: 0.9231"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting loss for modeling training\n",
    "plot_loss(model_name='nasnetlarge_model_v2', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training total loss during nasnetlarge_model_v2 training: 0.06785937398672104\n",
    "# Final epoch training class label loss during nasnetlarge_model_v2 training: 0.0282115638256073\n",
    "# Final epoch training bounding box loss during nasnetlarge_model_v2 training: 0.039647821336984634\n",
    "# Final epoch validation total loss during nasnetlarge_model_v2 training: 0.7523079514503479\n",
    "# Final epoch validation class label loss during nasnetlarge_model_v2 training: 0.737829327583313\n",
    "# Final epoch validation bounding box loss during nasnetlarge_model_v2 training: 0.014478589408099651"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting accuracy for modeling training\n",
    "plot_accuracy(model_name='nasnetlarge_model_v2', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training class label accurracy during nasnetlarge_model_v2 training: 0.992521345615387\n",
    "# Final epoch validation class label accurracy during nasnetlarge_model_v2 training: 0.9230769276618958"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating model for mAP metrics and inference time and storing result\n",
    "result_val = evaluate_val(model=nasnetlarge_model_v2, model_name='nasnetlarge_model_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC PASCAL mAP for nasnetlarge_model_v2 validation data: 0.13538233935832977\n",
    "# VOC PASCAL mAP in all points for nasnetlarge_model_v2 validation data: 0.0663333460688591\n",
    "# COCO mAP for nasnetlarge_model_v2 validation data: 0.31842878460884094\n",
    "# Average inference time for nasnetlarge_model_v2 validation data: 0.37117377382058364"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending results of model evaluation to results dataframe (df_results)\n",
    "df_results_val = df_results_val.append(result_val, ignore_index=True)\n",
    "df_results_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating custom model\n",
    "input_layer = Input(shape=(224,224,3), name='input_layer')\n",
    "conv1 = Conv2D(32, (3,3), activation='relu')(input_layer)\n",
    "pool1 = MaxPool2D(pool_size=(2, 2))(conv1)\n",
    "conv2 = Conv2D(64, (3,3), activation='relu')(pool1)\n",
    "pool2 = MaxPool2D(pool_size=(2, 2))(conv2)\n",
    "conv3 = Conv2D(128, (3,3), activation='relu')(pool2)\n",
    "pool3 = MaxPool2D(pool_size=(2, 2))(conv3)\n",
    "conv4 = Conv2D(256, (3,3), activation='relu')(pool3)\n",
    "pool4 = MaxPool2D(pool_size=(2, 2))(conv4)\n",
    "conv5 = Conv2D(512, (3,3), activation='relu')(pool4)\n",
    "pool5 = MaxPool2D(pool_size=(2, 2))(conv5)\n",
    "flat = Flatten()(pool5)\n",
    "\n",
    "# constructing an output layer to predict bounding box coordinates\n",
    "bbox_head1 = Dense(128, activation=\"relu\")(flat)\n",
    "bbox_head2 = Dense(64, activation=\"relu\")(bbox_head1)\n",
    "bbox_head3 = Dense(32, activation=\"relu\")(bbox_head2)\n",
    "bbox_head_v1 = Dense(4, activation=\"sigmoid\", name=\"bounding_box\")(bbox_head3)\n",
    "\n",
    "# constructing an output layer to predict class labels\n",
    "class_head1 = Dense(512, activation=\"relu\")(flat)\n",
    "class_head2 = Dropout(0.5)(class_head1)\n",
    "class_head3 = Dense(512, activation=\"relu\")(class_head2)\n",
    "class_head4 = Dropout(0.5)(class_head3)\n",
    "class_head_v1 = Dense(2, activation=\"sigmoid\", name=\"class_label\")(class_head4)\n",
    "\n",
    "# creating a model that accepts an input image and outputs bounding box coordinates and a class label\n",
    "custom_model_v1 = Model(inputs=input_layer, outputs=(bbox_head_v1, class_head_v1))\n",
    "\n",
    "# creating dictionary for setting loss functions\n",
    "losses = {\"class_label\": \"binary_crossentropy\", \"bounding_box\": \"mean_squared_error\"}\n",
    "\n",
    "# creating dictionry for weighting loss functions\n",
    "loss_weights = {\"class_label\": 1.0, \"bounding_box\": 1.0}\n",
    "\n",
    "# initilizing an \"Adam\" optimizer with learning rate of 0.0001\n",
    "opt = Adam(learning_rate = 0.0001)\n",
    "\n",
    "# compiling the model\n",
    "custom_model_v1.compile(loss=losses, optimizer=opt, metrics=[\"accuracy\"], loss_weights=loss_weights)\n",
    "print(custom_model_v1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing a dictionary for our training outputs\n",
    "train_targets = {\"class_label\": labels_train, \"bounding_box\": bboxes_train}\n",
    "\n",
    "# constructing a dictionary for our test outputs\n",
    "val_targets = {\"class_label\": labels_val, \"bounding_box\": bboxes_val}\n",
    "\n",
    "# training model for class and bounding box predictions\n",
    "H = custom_model_v1.fit(x_train, train_targets, validation_data=(x_val, val_targets), batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "\n",
    "# saving trained model\n",
    "custom_model_v1.save('../Models/custom_model_v1.h5', save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 1/30\n",
    "# 30/30 [==============================] - 32s 1s/step - loss: 0.7646 - bounding_box_loss: 0.0641 - class_label_loss: 0.7005 - bounding_box_accuracy: 0.6154 - class_label_accuracy: 0.5096 - val_loss: 0.7235 - val_bounding_box_loss: 0.0202 - val_class_label_loss: 0.7032 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.3942\n",
    "# Epoch 2/30\n",
    "# 30/30 [==============================] - 34s 1s/step - loss: 0.6932 - bounding_box_loss: 0.0216 - class_label_loss: 0.6716 - bounding_box_accuracy: 0.6154 - class_label_accuracy: 0.6068 - val_loss: 0.6223 - val_bounding_box_loss: 0.0175 - val_class_label_loss: 0.6048 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.6827\n",
    "# Epoch 3/30\n",
    "# 30/30 [==============================] - 34s 1s/step - loss: 0.4892 - bounding_box_loss: 0.0166 - class_label_loss: 0.4726 - bounding_box_accuracy: 0.6229 - class_label_accuracy: 0.8312 - val_loss: 0.3879 - val_bounding_box_loss: 0.0162 - val_class_label_loss: 0.3717 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.8846\n",
    "# Epoch 4/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.3669 - bounding_box_loss: 0.0149 - class_label_loss: 0.3520 - bounding_box_accuracy: 0.6154 - class_label_accuracy: 0.8782 - val_loss: 0.3872 - val_bounding_box_loss: 0.0177 - val_class_label_loss: 0.3695 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.8846\n",
    "# Epoch 5/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.3672 - bounding_box_loss: 0.0152 - class_label_loss: 0.3520 - bounding_box_accuracy: 0.6154 - class_label_accuracy: 0.8729 - val_loss: 0.3993 - val_bounding_box_loss: 0.0135 - val_class_label_loss: 0.3858 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.8558\n",
    "# Epoch 6/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.3310 - bounding_box_loss: 0.0142 - class_label_loss: 0.3168 - bounding_box_accuracy: 0.6132 - class_label_accuracy: 0.8953 - val_loss: 0.3570 - val_bounding_box_loss: 0.0143 - val_class_label_loss: 0.3427 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.8846\n",
    "# Epoch 7/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.3278 - bounding_box_loss: 0.0139 - class_label_loss: 0.3139 - bounding_box_accuracy: 0.6239 - class_label_accuracy: 0.8857 - val_loss: 0.3402 - val_bounding_box_loss: 0.0114 - val_class_label_loss: 0.3287 - val_bounding_box_accuracy: 0.6346 - val_class_label_accuracy: 0.8846\n",
    "# Epoch 8/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.2971 - bounding_box_loss: 0.0141 - class_label_loss: 0.2830 - bounding_box_accuracy: 0.6506 - class_label_accuracy: 0.9071 - val_loss: 0.4196 - val_bounding_box_loss: 0.0116 - val_class_label_loss: 0.4080 - val_bounding_box_accuracy: 0.6346 - val_class_label_accuracy: 0.8558\n",
    "# Epoch 9/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.3370 - bounding_box_loss: 0.0148 - class_label_loss: 0.3222 - bounding_box_accuracy: 0.6442 - class_label_accuracy: 0.8857 - val_loss: 0.3343 - val_bounding_box_loss: 0.0113 - val_class_label_loss: 0.3230 - val_bounding_box_accuracy: 0.6538 - val_class_label_accuracy: 0.8654\n",
    "# Epoch 10/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.2638 - bounding_box_loss: 0.0115 - class_label_loss: 0.2522 - bounding_box_accuracy: 0.6528 - class_label_accuracy: 0.9081 - val_loss: 0.2939 - val_bounding_box_loss: 0.0100 - val_class_label_loss: 0.2839 - val_bounding_box_accuracy: 0.6442 - val_class_label_accuracy: 0.8942\n",
    "# Epoch 11/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.2569 - bounding_box_loss: 0.0119 - class_label_loss: 0.2450 - bounding_box_accuracy: 0.6624 - class_label_accuracy: 0.9103 - val_loss: 0.3295 - val_bounding_box_loss: 0.0103 - val_class_label_loss: 0.3192 - val_bounding_box_accuracy: 0.6635 - val_class_label_accuracy: 0.8750\n",
    "# Epoch 12/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.2656 - bounding_box_loss: 0.0121 - class_label_loss: 0.2535 - bounding_box_accuracy: 0.6784 - class_label_accuracy: 0.9113 - val_loss: 0.2933 - val_bounding_box_loss: 0.0114 - val_class_label_loss: 0.2819 - val_bounding_box_accuracy: 0.6827 - val_class_label_accuracy: 0.9038\n",
    "# Epoch 13/30\n",
    "# 30/30 [==============================] - 34s 1s/step - loss: 0.2169 - bounding_box_loss: 0.0110 - class_label_loss: 0.2059 - bounding_box_accuracy: 0.6870 - class_label_accuracy: 0.9327 - val_loss: 0.2663 - val_bounding_box_loss: 0.0094 - val_class_label_loss: 0.2568 - val_bounding_box_accuracy: 0.6827 - val_class_label_accuracy: 0.9135\n",
    "# Epoch 14/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.1945 - bounding_box_loss: 0.0102 - class_label_loss: 0.1843 - bounding_box_accuracy: 0.7115 - class_label_accuracy: 0.9476 - val_loss: 0.2643 - val_bounding_box_loss: 0.0090 - val_class_label_loss: 0.2554 - val_bounding_box_accuracy: 0.7019 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 15/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.1902 - bounding_box_loss: 0.0102 - class_label_loss: 0.1800 - bounding_box_accuracy: 0.7115 - class_label_accuracy: 0.9530 - val_loss: 0.3353 - val_bounding_box_loss: 0.0088 - val_class_label_loss: 0.3265 - val_bounding_box_accuracy: 0.7212 - val_class_label_accuracy: 0.8942\n",
    "# Epoch 16/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.1932 - bounding_box_loss: 0.0100 - class_label_loss: 0.1832 - bounding_box_accuracy: 0.7137 - class_label_accuracy: 0.9434 - val_loss: 0.3358 - val_bounding_box_loss: 0.0093 - val_class_label_loss: 0.3265 - val_bounding_box_accuracy: 0.7308 - val_class_label_accuracy: 0.8750\n",
    "# Epoch 17/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.1718 - bounding_box_loss: 0.0098 - class_label_loss: 0.1620 - bounding_box_accuracy: 0.7073 - class_label_accuracy: 0.9615 - val_loss: 0.3276 - val_bounding_box_loss: 0.0092 - val_class_label_loss: 0.3184 - val_bounding_box_accuracy: 0.7019 - val_class_label_accuracy: 0.8846\n",
    "# Epoch 18/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.1945 - bounding_box_loss: 0.0101 - class_label_loss: 0.1844 - bounding_box_accuracy: 0.7286 - class_label_accuracy: 0.9509 - val_loss: 0.3460 - val_bounding_box_loss: 0.0092 - val_class_label_loss: 0.3368 - val_bounding_box_accuracy: 0.7500 - val_class_label_accuracy: 0.8846\n",
    "# Epoch 19/30\n",
    "# 30/30 [==============================] - 34s 1s/step - loss: 0.1795 - bounding_box_loss: 0.0102 - class_label_loss: 0.1693 - bounding_box_accuracy: 0.7147 - class_label_accuracy: 0.9562 - val_loss: 0.2850 - val_bounding_box_loss: 0.0089 - val_class_label_loss: 0.2761 - val_bounding_box_accuracy: 0.7308 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 20/30\n",
    "# 30/30 [==============================] - 34s 1s/step - loss: 0.1737 - bounding_box_loss: 0.0097 - class_label_loss: 0.1640 - bounding_box_accuracy: 0.7212 - class_label_accuracy: 0.9615 - val_loss: 0.4180 - val_bounding_box_loss: 0.0100 - val_class_label_loss: 0.4080 - val_bounding_box_accuracy: 0.7692 - val_class_label_accuracy: 0.8558\n",
    "# Epoch 21/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.1795 - bounding_box_loss: 0.0097 - class_label_loss: 0.1698 - bounding_box_accuracy: 0.7126 - class_label_accuracy: 0.9519 - val_loss: 0.2426 - val_bounding_box_loss: 0.0089 - val_class_label_loss: 0.2338 - val_bounding_box_accuracy: 0.7212 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 22/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.1623 - bounding_box_loss: 0.0094 - class_label_loss: 0.1529 - bounding_box_accuracy: 0.7137 - class_label_accuracy: 0.9594 - val_loss: 0.2285 - val_bounding_box_loss: 0.0090 - val_class_label_loss: 0.2196 - val_bounding_box_accuracy: 0.7212 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 23/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.1679 - bounding_box_loss: 0.0093 - class_label_loss: 0.1586 - bounding_box_accuracy: 0.7340 - class_label_accuracy: 0.9605 - val_loss: 0.4612 - val_bounding_box_loss: 0.0129 - val_class_label_loss: 0.4483 - val_bounding_box_accuracy: 0.7308 - val_class_label_accuracy: 0.8462\n",
    "# Epoch 24/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.1666 - bounding_box_loss: 0.0091 - class_label_loss: 0.1574 - bounding_box_accuracy: 0.7222 - class_label_accuracy: 0.9562 - val_loss: 0.2417 - val_bounding_box_loss: 0.0091 - val_class_label_loss: 0.2326 - val_bounding_box_accuracy: 0.7019 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 25/30\n",
    "# 30/30 [==============================] - 34s 1s/step - loss: 0.1513 - bounding_box_loss: 0.0095 - class_label_loss: 0.1418 - bounding_box_accuracy: 0.7147 - class_label_accuracy: 0.9679 - val_loss: 0.3392 - val_bounding_box_loss: 0.0087 - val_class_label_loss: 0.3305 - val_bounding_box_accuracy: 0.7404 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 26/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.1476 - bounding_box_loss: 0.0087 - class_label_loss: 0.1389 - bounding_box_accuracy: 0.7276 - class_label_accuracy: 0.9594 - val_loss: 0.2317 - val_bounding_box_loss: 0.0085 - val_class_label_loss: 0.2232 - val_bounding_box_accuracy: 0.7308 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 27/30\n",
    "# 30/30 [==============================] - 34s 1s/step - loss: 0.1492 - bounding_box_loss: 0.0089 - class_label_loss: 0.1402 - bounding_box_accuracy: 0.7382 - class_label_accuracy: 0.9637 - val_loss: 0.3530 - val_bounding_box_loss: 0.0092 - val_class_label_loss: 0.3438 - val_bounding_box_accuracy: 0.7212 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 28/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.1397 - bounding_box_loss: 0.0084 - class_label_loss: 0.1313 - bounding_box_accuracy: 0.7372 - class_label_accuracy: 0.9669 - val_loss: 0.2467 - val_bounding_box_loss: 0.0085 - val_class_label_loss: 0.2381 - val_bounding_box_accuracy: 0.7115 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 29/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.1492 - bounding_box_loss: 0.0085 - class_label_loss: 0.1407 - bounding_box_accuracy: 0.7361 - class_label_accuracy: 0.9647 - val_loss: 0.3485 - val_bounding_box_loss: 0.0086 - val_class_label_loss: 0.3399 - val_bounding_box_accuracy: 0.7308 - val_class_label_accuracy: 0.9038\n",
    "# Epoch 30/30\n",
    "# 30/30 [==============================] - 34s 1s/step - loss: 0.1641 - bounding_box_loss: 0.0088 - class_label_loss: 0.1554 - bounding_box_accuracy: 0.7318 - class_label_accuracy: 0.9530 - val_loss: 0.2807 - val_bounding_box_loss: 0.0082 - val_class_label_loss: 0.2725 - val_bounding_box_accuracy: 0.7500 - val_class_label_accuracy: 0.9423"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting loss for modeling training\n",
    "plot_loss(model_name='custom_model_v1', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training total loss during custom_model_v1 training: 0.1641349196434021\n",
    "# Final epoch training class label loss during custom_model_v1 training: 0.15537679195404053\n",
    "# Final epoch training bounding box loss during custom_model_v1 training: 0.008758130483329296\n",
    "# Final epoch validation total loss during custom_model_v1 training: 0.2807350158691406\n",
    "# Final epoch validation class label loss during custom_model_v1 training: 0.2725324034690857\n",
    "# Final epoch validation bounding box loss during custom_model_v1 training: 0.00820261798799038"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting accuracy for modeling training\n",
    "plot_accuracy(model_name='custom_model_v1', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training class label accurracy during custom_model_v1 training: 0.9529914259910583\n",
    "# Final epoch validation class label accurracy during custom_model_v1 training: 0.942307710647583"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating model for mAP metrics and inference time and storing result\n",
    "result_val = evaluate_val(model=custom_model_v1, model_name='custom_model_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC PASCAL mAP for custom_model_v1 validation data: 0.29302656650543213\n",
    "# VOC PASCAL mAP in all points for custom_model_v1 validation data: 0.2762465476989746\n",
    "# COCO mAP for custom_model_v1 validation data: 0.526224672794342\n",
    "# Average inference time for custom_model_v1 validation data: 0.04016861548790565"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending results of model evaluation to results dataframe (df_results)\n",
    "df_results_val = df_results_val.append(result_val, ignore_index=True)\n",
    "df_results_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating custom model\n",
    "input_layer = Input(shape=(224,224,3), name='input_layer')\n",
    "conv1 = Conv2D(32, (3,3), activation='relu')(input_layer)\n",
    "pool1 = MaxPool2D(pool_size=(2, 2))(conv1)\n",
    "conv2 = Conv2D(64, (3,3), activation='relu')(pool1)\n",
    "pool2 = MaxPool2D(pool_size=(2, 2))(conv2)\n",
    "conv3 = Conv2D(128, (3,3), activation='relu')(pool2)\n",
    "pool3 = MaxPool2D(pool_size=(2, 2))(conv3)\n",
    "conv4 = Conv2D(256, (3,3), activation='relu')(pool3)\n",
    "pool4 = MaxPool2D(pool_size=(2, 2))(conv4)\n",
    "conv5 = Conv2D(512, (3,3), activation='relu')(pool4)\n",
    "pool5 = MaxPool2D(pool_size=(2, 2))(conv5)\n",
    "flat = Flatten()(pool5)\n",
    "\n",
    "# constructing an output layer to predict bounding box coordinates\n",
    "bbox_head1 = Dense(128, activation=\"relu\")(flat)\n",
    "bbox_head2 = Dropout(0.5)(bbox_head1)\n",
    "bbox_head3 = Dense(64, activation=\"relu\")(bbox_head2)\n",
    "bbox_head4 = Dropout(0.5)(bbox_head3)\n",
    "bbox_head5 = Dense(32, activation=\"relu\")(bbox_head4)\n",
    "bbox_head6 = Dropout(0.5)(bbox_head5)\n",
    "bbox_head_v2 = Dense(4, activation=\"sigmoid\", name=\"bounding_box\")(bbox_head6)\n",
    "\n",
    "# constructing an output layer to predict class labels\n",
    "class_head1 = Dense(512, activation=\"relu\")(flat)\n",
    "class_head2 = Dropout(0.5)(class_head1)\n",
    "class_head3 = Dense(512, activation=\"relu\")(class_head2)\n",
    "class_head4 = Dropout(0.5)(class_head3)\n",
    "class_head5 = Dense(512, activation=\"relu\")(class_head4)\n",
    "class_head6 = Dropout(0.5)(class_head5)\n",
    "class_head_v2 = Dense(2, activation=\"sigmoid\", name=\"class_label\")(class_head6)\n",
    "\n",
    "# creating a model that accepts an input image and outputs bounding box coordinates and a class label\n",
    "custom_model_v2 = Model(inputs=input_layer, outputs=(bbox_head_v2, class_head_v2))\n",
    "\n",
    "# creating dictionary for setting loss functions\n",
    "losses = {\"class_label\": \"binary_crossentropy\", \"bounding_box\": \"mean_squared_error\"}\n",
    "\n",
    "# creating dictionry for weighting loss functions\n",
    "loss_weights = {\"class_label\": 1.0, \"bounding_box\": 1.0}\n",
    "\n",
    "# initilizing an \"Adam\" optimizer with learning rate of 0.0001\n",
    "opt = Adam(learning_rate = 0.0001)\n",
    "\n",
    "# compiling the model\n",
    "custom_model_v2.compile(loss=losses, optimizer=opt, metrics=[\"accuracy\"], loss_weights=loss_weights)\n",
    "print(custom_model_v2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing a dictionary for our training outputs\n",
    "train_targets = {\"class_label\": labels_train, \"bounding_box\": bboxes_train}\n",
    "\n",
    "# constructing a dictionary for our test outputs\n",
    "val_targets = {\"class_label\": labels_val, \"bounding_box\": bboxes_val}\n",
    "\n",
    "# training model for class and bounding box predictions\n",
    "H = custom_model_v2.fit(x_train, train_targets, validation_data=(x_val, val_targets), batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "\n",
    "# saving trained model\n",
    "custom_model_v2.save('../Models/custom_model_v2.h5', save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 1/30\n",
    "# 30/30 [==============================] - 31s 1s/step - loss: 0.7912 - bounding_box_loss: 0.0912 - class_label_loss: 0.7000 - bounding_box_accuracy: 0.3857 - class_label_accuracy: 0.4979 - val_loss: 0.7370 - val_bounding_box_loss: 0.0604 - val_class_label_loss: 0.6766 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.6058\n",
    "# Epoch 2/30\n",
    "# 30/30 [==============================] - 34s 1s/step - loss: 0.7683 - bounding_box_loss: 0.0761 - class_label_loss: 0.6922 - bounding_box_accuracy: 0.4284 - class_label_accuracy: 0.5224 - val_loss: 0.7046 - val_bounding_box_loss: 0.0363 - val_class_label_loss: 0.6684 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.6058\n",
    "# Epoch 3/30\n",
    "# 30/30 [==============================] - 34s 1s/step - loss: 0.7062 - bounding_box_loss: 0.0698 - class_label_loss: 0.6364 - bounding_box_accuracy: 0.4359 - class_label_accuracy: 0.6774 - val_loss: 0.5761 - val_bounding_box_loss: 0.0173 - val_class_label_loss: 0.5588 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.6731\n",
    "# Epoch 4/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.5210 - bounding_box_loss: 0.0638 - class_label_loss: 0.4573 - bounding_box_accuracy: 0.4391 - class_label_accuracy: 0.8130 - val_loss: 0.3941 - val_bounding_box_loss: 0.0211 - val_class_label_loss: 0.3730 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.8750\n",
    "# Epoch 5/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.4258 - bounding_box_loss: 0.0572 - class_label_loss: 0.3685 - bounding_box_accuracy: 0.4402 - class_label_accuracy: 0.8846 - val_loss: 0.4080 - val_bounding_box_loss: 0.0203 - val_class_label_loss: 0.3877 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.8558\n",
    "# Epoch 6/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.4023 - bounding_box_loss: 0.0562 - class_label_loss: 0.3461 - bounding_box_accuracy: 0.4498 - class_label_accuracy: 0.8739 - val_loss: 0.3706 - val_bounding_box_loss: 0.0218 - val_class_label_loss: 0.3488 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.8750\n",
    "# Epoch 7/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.3980 - bounding_box_loss: 0.0515 - class_label_loss: 0.3464 - bounding_box_accuracy: 0.4466 - class_label_accuracy: 0.8729 - val_loss: 0.4061 - val_bounding_box_loss: 0.0181 - val_class_label_loss: 0.3879 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.8654\n",
    "# Epoch 8/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.3525 - bounding_box_loss: 0.0501 - class_label_loss: 0.3024 - bounding_box_accuracy: 0.4754 - class_label_accuracy: 0.8889 - val_loss: 0.3180 - val_bounding_box_loss: 0.0133 - val_class_label_loss: 0.3047 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.8942\n",
    "# Epoch 9/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.3330 - bounding_box_loss: 0.0458 - class_label_loss: 0.2873 - bounding_box_accuracy: 0.4744 - class_label_accuracy: 0.8942 - val_loss: 0.3783 - val_bounding_box_loss: 0.0188 - val_class_label_loss: 0.3595 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.8942\n",
    "# Epoch 10/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.3115 - bounding_box_loss: 0.0439 - class_label_loss: 0.2676 - bounding_box_accuracy: 0.4712 - class_label_accuracy: 0.9028 - val_loss: 0.3539 - val_bounding_box_loss: 0.0145 - val_class_label_loss: 0.3395 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.8846\n",
    "# Epoch 11/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.2932 - bounding_box_loss: 0.0411 - class_label_loss: 0.2521 - bounding_box_accuracy: 0.4818 - class_label_accuracy: 0.9049 - val_loss: 0.3186 - val_bounding_box_loss: 0.0133 - val_class_label_loss: 0.3052 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9038\n",
    "# Epoch 12/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.2682 - bounding_box_loss: 0.0384 - class_label_loss: 0.2298 - bounding_box_accuracy: 0.4776 - class_label_accuracy: 0.9306 - val_loss: 0.3264 - val_bounding_box_loss: 0.0140 - val_class_label_loss: 0.3124 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9038\n",
    "# Epoch 13/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.2354 - bounding_box_loss: 0.0409 - class_label_loss: 0.1945 - bounding_box_accuracy: 0.4519 - class_label_accuracy: 0.9434 - val_loss: 0.2828 - val_bounding_box_loss: 0.0124 - val_class_label_loss: 0.2704 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9135\n",
    "# Epoch 14/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.2450 - bounding_box_loss: 0.0403 - class_label_loss: 0.2047 - bounding_box_accuracy: 0.4765 - class_label_accuracy: 0.9476 - val_loss: 0.2811 - val_bounding_box_loss: 0.0125 - val_class_label_loss: 0.2687 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9135\n",
    "# Epoch 15/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.2557 - bounding_box_loss: 0.0380 - class_label_loss: 0.2177 - bounding_box_accuracy: 0.4669 - class_label_accuracy: 0.9274 - val_loss: 0.5713 - val_bounding_box_loss: 0.0202 - val_class_label_loss: 0.5511 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.7981\n",
    "# Epoch 16/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.3106 - bounding_box_loss: 0.0378 - class_label_loss: 0.2728 - bounding_box_accuracy: 0.4925 - class_label_accuracy: 0.8996 - val_loss: 0.2624 - val_bounding_box_loss: 0.0133 - val_class_label_loss: 0.2491 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9135\n",
    "# Epoch 17/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.2398 - bounding_box_loss: 0.0349 - class_label_loss: 0.2049 - bounding_box_accuracy: 0.4701 - class_label_accuracy: 0.9295 - val_loss: 0.3674 - val_bounding_box_loss: 0.0153 - val_class_label_loss: 0.3521 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.8942\n",
    "# Epoch 18/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.2216 - bounding_box_loss: 0.0348 - class_label_loss: 0.1869 - bounding_box_accuracy: 0.4541 - class_label_accuracy: 0.9434 - val_loss: 0.2564 - val_bounding_box_loss: 0.0124 - val_class_label_loss: 0.2440 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 19/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.2058 - bounding_box_loss: 0.0343 - class_label_loss: 0.1715 - bounding_box_accuracy: 0.4925 - class_label_accuracy: 0.9487 - val_loss: 0.2619 - val_bounding_box_loss: 0.0133 - val_class_label_loss: 0.2486 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 20/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.2281 - bounding_box_loss: 0.0320 - class_label_loss: 0.1961 - bounding_box_accuracy: 0.4915 - class_label_accuracy: 0.9412 - val_loss: 0.3785 - val_bounding_box_loss: 0.0155 - val_class_label_loss: 0.3630 - val_bounding_box_accuracy: 0.3654 - val_class_label_accuracy: 0.8846\n",
    "# Epoch 21/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.2464 - bounding_box_loss: 0.0333 - class_label_loss: 0.2131 - bounding_box_accuracy: 0.4647 - class_label_accuracy: 0.9380 - val_loss: 0.3352 - val_bounding_box_loss: 0.0138 - val_class_label_loss: 0.3213 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9038\n",
    "# Epoch 22/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.2025 - bounding_box_loss: 0.0310 - class_label_loss: 0.1715 - bounding_box_accuracy: 0.5064 - class_label_accuracy: 0.9605 - val_loss: 0.2872 - val_bounding_box_loss: 0.0122 - val_class_label_loss: 0.2749 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.8750\n",
    "# Epoch 23/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.1860 - bounding_box_loss: 0.0285 - class_label_loss: 0.1575 - bounding_box_accuracy: 0.4829 - class_label_accuracy: 0.9583 - val_loss: 0.2973 - val_bounding_box_loss: 0.0127 - val_class_label_loss: 0.2846 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 24/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.2139 - bounding_box_loss: 0.0292 - class_label_loss: 0.1848 - bounding_box_accuracy: 0.4679 - class_label_accuracy: 0.9455 - val_loss: 0.3398 - val_bounding_box_loss: 0.0162 - val_class_label_loss: 0.3236 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.8750\n",
    "# Epoch 25/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.1792 - bounding_box_loss: 0.0296 - class_label_loss: 0.1496 - bounding_box_accuracy: 0.4957 - class_label_accuracy: 0.9626 - val_loss: 0.2562 - val_bounding_box_loss: 0.0126 - val_class_label_loss: 0.2437 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 26/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.1656 - bounding_box_loss: 0.0280 - class_label_loss: 0.1376 - bounding_box_accuracy: 0.4744 - class_label_accuracy: 0.9637 - val_loss: 0.2894 - val_bounding_box_loss: 0.0126 - val_class_label_loss: 0.2768 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 27/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.1927 - bounding_box_loss: 0.0282 - class_label_loss: 0.1645 - bounding_box_accuracy: 0.4925 - class_label_accuracy: 0.9530 - val_loss: 0.2464 - val_bounding_box_loss: 0.0131 - val_class_label_loss: 0.2334 - val_bounding_box_accuracy: 0.3942 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 28/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.1645 - bounding_box_loss: 0.0278 - class_label_loss: 0.1367 - bounding_box_accuracy: 0.4947 - class_label_accuracy: 0.9669 - val_loss: 0.3694 - val_bounding_box_loss: 0.0129 - val_class_label_loss: 0.3565 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.8942\n",
    "# Epoch 29/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.1692 - bounding_box_loss: 0.0283 - class_label_loss: 0.1409 - bounding_box_accuracy: 0.5085 - class_label_accuracy: 0.9637 - val_loss: 0.2413 - val_bounding_box_loss: 0.0118 - val_class_label_loss: 0.2296 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 30/30\n",
    "# 30/30 [==============================] - 33s 1s/step - loss: 0.1541 - bounding_box_loss: 0.0272 - class_label_loss: 0.1269 - bounding_box_accuracy: 0.4818 - class_label_accuracy: 0.9669 - val_loss: 0.3512 - val_bounding_box_loss: 0.0132 - val_class_label_loss: 0.3380 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.9038"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting loss for modeling training\n",
    "plot_loss(model_name='custom_model_v2', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training total loss during custom_model_v2 training: 0.1541239619255066\n",
    "# Final epoch training class label loss during custom_model_v2 training: 0.12691470980644226\n",
    "# Final epoch training bounding box loss during custom_model_v2 training: 0.027209246531128883\n",
    "# Final epoch validation total loss during custom_model_v2 training: 0.35123470425605774\n",
    "# Final epoch validation class label loss during custom_model_v2 training: 0.3380441963672638\n",
    "# Final epoch validation bounding box loss during custom_model_v2 training: 0.013190504163503647"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting accuracy for modeling training\n",
    "plot_accuracy(model_name='custom_model_v2', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training class label accurracy during custom_model_v2 training: 0.9668803215026855\n",
    "# Final epoch validation class label accurracy during custom_model_v2 training: 0.9038461446762085"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating model for mAP metrics and inference time and storing result\n",
    "result_val = evaluate_val(model=custom_model_v2, model_name='custom_model_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC PASCAL mAP for custom_model_v2 validation data: 0.14951299130916595\n",
    "# VOC PASCAL mAP in all points for custom_model_v2 validation data: 0.11141972243785858\n",
    "# COCO mAP for custom_model_v2 validation data: 0.3960426449775696\n",
    "# Average inference time for custom_model_v2 validation data: 0.04015564230772165"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending results of model evaluation to results dataframe (df_results)\n",
    "df_results_val = df_results_val.append(result_val, ignore_index=True)\n",
    "df_results_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating custom model\n",
    "input_layer = Input(shape=(224,224,3), name='input_layer')\n",
    "conv1 = Conv2D(32, (3,3), activation='relu')(input_layer)\n",
    "pool1 = MaxPool2D(pool_size=(2, 2))(conv1)\n",
    "conv2 = Conv2D(64, (3,3), activation='relu')(pool1)\n",
    "pool2 = MaxPool2D(pool_size=(2, 2))(conv2)\n",
    "conv3 = Conv2D(128, (3,3), activation='relu')(pool2)\n",
    "pool3 = MaxPool2D(pool_size=(2, 2))(conv3)\n",
    "conv4 = Conv2D(256, (3,3), activation='relu')(pool3)\n",
    "pool4 = MaxPool2D(pool_size=(2, 2))(conv4)\n",
    "conv5 = Conv2D(512, (3,3), activation='relu')(pool4)\n",
    "pool5 = MaxPool2D(pool_size=(2, 2))(conv5)\n",
    "flat = Flatten()(pool5)\n",
    "\n",
    "# constructing an output layer to predict bounding box coordinates\n",
    "bbox_head1 = Dense(128, activation=\"relu\")(flat)\n",
    "bbox_head2 = Dropout(0.5)(bbox_head1)\n",
    "bbox_head3 = Dense(64, activation=\"relu\")(bbox_head2)\n",
    "bbox_head4 = Dense(32, activation=\"relu\")(bbox_head3)\n",
    "bbox_head_v3 = Dense(4, activation=\"sigmoid\", name=\"bounding_box\")(bbox_head4)\n",
    "\n",
    "# constructing an output layer to predict class labels\n",
    "class_head1 = Dense(512, activation=\"relu\")(flat)\n",
    "class_head2 = Dropout(0.5)(class_head1)\n",
    "class_head3 = Dense(512, activation=\"relu\")(class_head2)\n",
    "class_head4 = Dropout(0.5)(class_head3)\n",
    "class_head5 = Dense(512, activation=\"relu\")(class_head4)\n",
    "class_head6 = Dropout(0.5)(class_head5)\n",
    "class_head7 = Dense(512, activation=\"relu\")(class_head6)\n",
    "class_head8 = Dropout(0.5)(class_head7)\n",
    "class_head_v3 = Dense(2, activation=\"sigmoid\", name=\"class_label\")(class_head8)\n",
    "\n",
    "# creating a model that accepts an input image and outputs bounding box coordinates and a class label\n",
    "custom_model_v3 = Model(inputs=input_layer, outputs=(bbox_head_v3, class_head_v3))\n",
    "\n",
    "# creating dictionary for setting loss functions\n",
    "losses = {\"class_label\": \"binary_crossentropy\", \"bounding_box\": \"mean_squared_error\"}\n",
    "\n",
    "# creating dictionry for weighting loss functions\n",
    "loss_weights = {\"class_label\": 1.0, \"bounding_box\": 1.0}\n",
    "\n",
    "# initilizing an \"Adam\" optimizer with learning rate of 0.0001\n",
    "opt = Adam(learning_rate = 0.0001)\n",
    "\n",
    "# compiling the model\n",
    "custom_model_v3.compile(loss=losses, optimizer=opt, metrics=[\"accuracy\"], loss_weights=loss_weights)\n",
    "print(custom_model_v3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing a dictionary for our training outputs\n",
    "train_targets = {\"class_label\": labels_train, \"bounding_box\": bboxes_train}\n",
    "\n",
    "# constructing a dictionary for our test outputs\n",
    "val_targets = {\"class_label\": labels_val, \"bounding_box\": bboxes_val}\n",
    "\n",
    "# training model for class and bounding box predictions\n",
    "H = custom_model_v3.fit(x_train, train_targets, validation_data=(x_val, val_targets), batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "\n",
    "# saving trained model\n",
    "custom_model_v3.save('../Models/custom_model_v3.h5', save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 1/30\n",
    "# 30/30 [==============================] - 36s 1s/step - loss: 0.7662 - bounding_box_loss: 0.0658 - class_label_loss: 0.7004 - bounding_box_accuracy: 0.3654 - class_label_accuracy: 0.5128 - val_loss: 0.7130 - val_bounding_box_loss: 0.0247 - val_class_label_loss: 0.6882 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.6058\n",
    "# Epoch 2/30\n",
    "# 30/30 [==============================] - 38s 1s/step - loss: 0.7245 - bounding_box_loss: 0.0308 - class_label_loss: 0.6937 - bounding_box_accuracy: 0.3942 - class_label_accuracy: 0.5032 - val_loss: 0.7178 - val_bounding_box_loss: 0.0242 - val_class_label_loss: 0.6936 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.3942\n",
    "# Epoch 3/30\n",
    "# 30/30 [==============================] - 43s 1s/step - loss: 0.7264 - bounding_box_loss: 0.0277 - class_label_loss: 0.6987 - bounding_box_accuracy: 0.4541 - class_label_accuracy: 0.4712 - val_loss: 0.7178 - val_bounding_box_loss: 0.0224 - val_class_label_loss: 0.6954 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.3942\n",
    "# Epoch 4/30\n",
    "# 30/30 [==============================] - 45s 1s/step - loss: 0.7190 - bounding_box_loss: 0.0260 - class_label_loss: 0.6930 - bounding_box_accuracy: 0.4519 - class_label_accuracy: 0.5299 - val_loss: 0.7143 - val_bounding_box_loss: 0.0215 - val_class_label_loss: 0.6927 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.4615\n",
    "# Epoch 5/30\n",
    "# 30/30 [==============================] - 42s 1s/step - loss: 0.7186 - bounding_box_loss: 0.0250 - class_label_loss: 0.6936 - bounding_box_accuracy: 0.4252 - class_label_accuracy: 0.5096 - val_loss: 0.7133 - val_bounding_box_loss: 0.0215 - val_class_label_loss: 0.6918 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.7019\n",
    "# Epoch 6/30\n",
    "# 30/30 [==============================] - 40s 1s/step - loss: 0.7122 - bounding_box_loss: 0.0244 - class_label_loss: 0.6879 - bounding_box_accuracy: 0.5085 - class_label_accuracy: 0.5449 - val_loss: 0.7070 - val_bounding_box_loss: 0.0225 - val_class_label_loss: 0.6845 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.6731\n",
    "# Epoch 7/30\n",
    "# 30/30 [==============================] - 41s 1s/step - loss: 0.7084 - bounding_box_loss: 0.0232 - class_label_loss: 0.6852 - bounding_box_accuracy: 0.4957 - class_label_accuracy: 0.5128 - val_loss: 0.6805 - val_bounding_box_loss: 0.0218 - val_class_label_loss: 0.6587 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.6058\n",
    "# Epoch 8/30\n",
    "# 30/30 [==============================] - 43s 1s/step - loss: 0.6493 - bounding_box_loss: 0.0212 - class_label_loss: 0.6281 - bounding_box_accuracy: 0.5064 - class_label_accuracy: 0.5887 - val_loss: 0.4927 - val_bounding_box_loss: 0.0147 - val_class_label_loss: 0.4780 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.7885\n",
    "# Epoch 9/30\n",
    "# 30/30 [==============================] - 42s 1s/step - loss: 0.5539 - bounding_box_loss: 0.0182 - class_label_loss: 0.5356 - bounding_box_accuracy: 0.5171 - class_label_accuracy: 0.7917 - val_loss: 0.3762 - val_bounding_box_loss: 0.0144 - val_class_label_loss: 0.3619 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.8750\n",
    "# Epoch 10/30\n",
    "# 30/30 [==============================] - 42s 1s/step - loss: 0.4658 - bounding_box_loss: 0.0208 - class_label_loss: 0.4450 - bounding_box_accuracy: 0.5534 - class_label_accuracy: 0.8162 - val_loss: 0.5577 - val_bounding_box_loss: 0.0149 - val_class_label_loss: 0.5428 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.7596\n",
    "# Epoch 11/30\n",
    "# 30/30 [==============================] - 40s 1s/step - loss: 0.3672 - bounding_box_loss: 0.0186 - class_label_loss: 0.3486 - bounding_box_accuracy: 0.5588 - class_label_accuracy: 0.8761 - val_loss: 0.3012 - val_bounding_box_loss: 0.0133 - val_class_label_loss: 0.2880 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.8942\n",
    "# Epoch 12/30\n",
    "# 30/30 [==============================] - 40s 1s/step - loss: 0.3168 - bounding_box_loss: 0.0187 - class_label_loss: 0.2981 - bounding_box_accuracy: 0.5556 - class_label_accuracy: 0.8964 - val_loss: 0.3668 - val_bounding_box_loss: 0.0145 - val_class_label_loss: 0.3523 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.8846\n",
    "# Epoch 13/30\n",
    "# 30/30 [==============================] - 40s 1s/step - loss: 0.2875 - bounding_box_loss: 0.0162 - class_label_loss: 0.2713 - bounding_box_accuracy: 0.5566 - class_label_accuracy: 0.8985 - val_loss: 0.2738 - val_bounding_box_loss: 0.0120 - val_class_label_loss: 0.2618 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9038\n",
    "# Epoch 14/30\n",
    "# 30/30 [==============================] - 39s 1s/step - loss: 0.3404 - bounding_box_loss: 0.0161 - class_label_loss: 0.3243 - bounding_box_accuracy: 0.5620 - class_label_accuracy: 0.8718 - val_loss: 0.3479 - val_bounding_box_loss: 0.0120 - val_class_label_loss: 0.3360 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.8750\n",
    "# Epoch 15/30\n",
    "# 30/30 [==============================] - 39s 1s/step - loss: 0.2664 - bounding_box_loss: 0.0151 - class_label_loss: 0.2514 - bounding_box_accuracy: 0.5972 - class_label_accuracy: 0.9113 - val_loss: 0.2688 - val_bounding_box_loss: 0.0116 - val_class_label_loss: 0.2572 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9038\n",
    "# Epoch 16/30\n",
    "# 30/30 [==============================] - 41s 1s/step - loss: 0.2593 - bounding_box_loss: 0.0157 - class_label_loss: 0.2437 - bounding_box_accuracy: 0.5694 - class_label_accuracy: 0.9092 - val_loss: 0.2507 - val_bounding_box_loss: 0.0119 - val_class_label_loss: 0.2387 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.8942\n",
    "# Epoch 17/30\n",
    "# 30/30 [==============================] - 42s 1s/step - loss: 0.2758 - bounding_box_loss: 0.0157 - class_label_loss: 0.2601 - bounding_box_accuracy: 0.5769 - class_label_accuracy: 0.8985 - val_loss: 0.2483 - val_bounding_box_loss: 0.0133 - val_class_label_loss: 0.2350 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 18/30\n",
    "# 30/30 [==============================] - 42s 1s/step - loss: 0.2441 - bounding_box_loss: 0.0156 - class_label_loss: 0.2285 - bounding_box_accuracy: 0.5780 - class_label_accuracy: 0.9327 - val_loss: 0.2637 - val_bounding_box_loss: 0.0113 - val_class_label_loss: 0.2524 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 19/30\n",
    "# 30/30 [==============================] - 43s 1s/step - loss: 0.2225 - bounding_box_loss: 0.0142 - class_label_loss: 0.2084 - bounding_box_accuracy: 0.6026 - class_label_accuracy: 0.9583 - val_loss: 0.2515 - val_bounding_box_loss: 0.0115 - val_class_label_loss: 0.2400 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 20/30\n",
    "# 30/30 [==============================] - 42s 1s/step - loss: 0.1962 - bounding_box_loss: 0.0142 - class_label_loss: 0.1820 - bounding_box_accuracy: 0.6154 - class_label_accuracy: 0.9530 - val_loss: 0.2858 - val_bounding_box_loss: 0.0110 - val_class_label_loss: 0.2747 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 21/30\n",
    "# 30/30 [==============================] - 43s 1s/step - loss: 0.1911 - bounding_box_loss: 0.0140 - class_label_loss: 0.1771 - bounding_box_accuracy: 0.5705 - class_label_accuracy: 0.9519 - val_loss: 0.4122 - val_bounding_box_loss: 0.0114 - val_class_label_loss: 0.4008 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.8654\n",
    "# Epoch 22/30\n",
    "# 30/30 [==============================] - 43s 1s/step - loss: 0.2358 - bounding_box_loss: 0.0144 - class_label_loss: 0.2214 - bounding_box_accuracy: 0.6165 - class_label_accuracy: 0.9327 - val_loss: 0.2454 - val_bounding_box_loss: 0.0108 - val_class_label_loss: 0.2346 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 23/30\n",
    "# 30/30 [==============================] - 42s 1s/step - loss: 0.1919 - bounding_box_loss: 0.0133 - class_label_loss: 0.1786 - bounding_box_accuracy: 0.6036 - class_label_accuracy: 0.9530 - val_loss: 0.2319 - val_bounding_box_loss: 0.0112 - val_class_label_loss: 0.2207 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 24/30\n",
    "# 30/30 [==============================] - 42s 1s/step - loss: 0.1865 - bounding_box_loss: 0.0136 - class_label_loss: 0.1729 - bounding_box_accuracy: 0.6047 - class_label_accuracy: 0.9541 - val_loss: 0.4558 - val_bounding_box_loss: 0.0111 - val_class_label_loss: 0.4447 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.8846\n",
    "# Epoch 25/30\n",
    "# 30/30 [==============================] - 44s 1s/step - loss: 0.2030 - bounding_box_loss: 0.0135 - class_label_loss: 0.1895 - bounding_box_accuracy: 0.5887 - class_label_accuracy: 0.9498 - val_loss: 0.3780 - val_bounding_box_loss: 0.0104 - val_class_label_loss: 0.3676 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.8942\n",
    "# Epoch 26/30\n",
    "# 30/30 [==============================] - 44s 1s/step - loss: 0.2231 - bounding_box_loss: 0.0153 - class_label_loss: 0.2078 - bounding_box_accuracy: 0.6026 - class_label_accuracy: 0.9519 - val_loss: 0.2651 - val_bounding_box_loss: 0.0107 - val_class_label_loss: 0.2544 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 27/30\n",
    "# 30/30 [==============================] - 41s 1s/step - loss: 0.1711 - bounding_box_loss: 0.0130 - class_label_loss: 0.1582 - bounding_box_accuracy: 0.6207 - class_label_accuracy: 0.9626 - val_loss: 0.2408 - val_bounding_box_loss: 0.0128 - val_class_label_loss: 0.2280 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 28/30\n",
    "# 30/30 [==============================] - 41s 1s/step - loss: 0.1686 - bounding_box_loss: 0.0131 - class_label_loss: 0.1555 - bounding_box_accuracy: 0.6058 - class_label_accuracy: 0.9637 - val_loss: 0.2372 - val_bounding_box_loss: 0.0112 - val_class_label_loss: 0.2260 - val_bounding_box_accuracy: 0.6346 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 29/30\n",
    "# 30/30 [==============================] - 42s 1s/step - loss: 0.1744 - bounding_box_loss: 0.0128 - class_label_loss: 0.1616 - bounding_box_accuracy: 0.6036 - class_label_accuracy: 0.9594 - val_loss: 0.2697 - val_bounding_box_loss: 0.0101 - val_class_label_loss: 0.2596 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 30/30\n",
    "# 30/30 [==============================] - 43s 1s/step - loss: 0.1763 - bounding_box_loss: 0.0131 - class_label_loss: 0.1633 - bounding_box_accuracy: 0.6132 - class_label_accuracy: 0.9594 - val_loss: 0.2612 - val_bounding_box_loss: 0.0112 - val_class_label_loss: 0.2500 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9038"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting loss for modeling training\n",
    "plot_loss(model_name='custom_model_v3', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training total loss during custom_model_v3 training: 0.17630688846111298\n",
    "# Final epoch training class label loss during custom_model_v3 training: 0.16325326263904572\n",
    "# Final epoch training bounding box loss during custom_model_v3 training: 0.013053649105131626\n",
    "# Final epoch validation total loss during custom_model_v3 training: 0.2611725628376007\n",
    "# Final epoch validation class label loss during custom_model_v3 training: 0.24996604025363922\n",
    "# Final epoch validation bounding box loss during custom_model_v3 training: 0.01120653934776783"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting accuracy for modeling training\n",
    "plot_accuracy(model_name='custom_model_v3', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training class label accurracy during custom_model_v3 training: 0.9594017267227173\n",
    "# Final epoch validation class label accurracy during custom_model_v3 training: 0.9038461446762085"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating model for mAP metrics and inference time and storing result\n",
    "result_val = evaluate_val(model=custom_model_v3, model_name='custom_model_v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC PASCAL mAP for custom_model_v3 validation data: 0.11353646218776703\n",
    "# VOC PASCAL mAP in all points for custom_model_v3 validation data: 0.07739706337451935\n",
    "# COCO mAP for custom_model_v3 validation data: 0.4901755750179291\n",
    "# Average inference time for custom_model_v3 validation data: 0.03954241367486807"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending results of model evaluation to results dataframe (df_results)\n",
    "df_results_val = df_results_val.append(result_val, ignore_index=True)\n",
    "df_results_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating custom model\n",
    "input_layer = Input(shape=(224,224,3), name='input_layer')\n",
    "conv1 = Conv2D(32, (3,3), activation='relu')(input_layer)\n",
    "pool1 = MaxPool2D(pool_size=(2, 2))(conv1)\n",
    "conv2 = Conv2D(64, (3,3), activation='relu')(pool1)\n",
    "pool2 = MaxPool2D(pool_size=(2, 2))(conv2)\n",
    "conv3 = Conv2D(128, (3,3), activation='relu')(pool2)\n",
    "pool3 = MaxPool2D(pool_size=(2, 2))(conv3)\n",
    "conv4 = Conv2D(256, (3,3), activation='relu')(pool3)\n",
    "pool4 = MaxPool2D(pool_size=(2, 2))(conv4)\n",
    "conv5 = Conv2D(512, (3,3), activation='relu')(pool4)\n",
    "pool5 = MaxPool2D(pool_size=(2, 2))(conv5)\n",
    "flat = Flatten()(pool5)\n",
    "\n",
    "# constructing an output layer to predict bounding box coordinates\n",
    "bbox_head1 = Dense(256, activation=\"relu\")(flat)\n",
    "bbox_head2 = Dropout(0.5)(bbox_head1)\n",
    "bbox_head3 = Dense(128, activation=\"relu\")(bbox_head2)\n",
    "bbox_head4 = Dense(64, activation=\"relu\")(bbox_head3)\n",
    "bbox_head5 = Dense(32, activation=\"relu\")(bbox_head4)\n",
    "bbox_head_v4 = Dense(4, activation=\"sigmoid\", name=\"bounding_box\")(bbox_head5)\n",
    "\n",
    "# constructing an output layer to predict class labels\n",
    "class_head1 = Dense(512, activation=\"relu\")(flat)\n",
    "class_head2 = Dropout(0.5)(class_head1)\n",
    "class_head3 = Dense(512, activation=\"relu\")(class_head2)\n",
    "class_head4 = Dropout(0.5)(class_head3)\n",
    "class_head5 = Dense(256, activation=\"relu\")(class_head4)\n",
    "class_head6 = Dropout(0.5)(class_head5)\n",
    "class_head7 = Dense(128, activation=\"relu\")(class_head6)\n",
    "class_head8 = Dropout(0.5)(class_head7)\n",
    "class_head_v4 = Dense(2, activation=\"sigmoid\", name=\"class_label\")(class_head8)\n",
    "\n",
    "# creating a model that accepts an input image and outputs bounding box coordinates and a class label\n",
    "custom_model_v4 = Model(inputs=input_layer, outputs=(bbox_head_v4, class_head_v4))\n",
    "\n",
    "# creating dictionary for setting loss functions\n",
    "losses = {\"class_label\": \"binary_crossentropy\", \"bounding_box\": \"mean_squared_error\"}\n",
    "\n",
    "# creating dictionry for weighting loss functions\n",
    "loss_weights = {\"class_label\": 1.0, \"bounding_box\": 1.0}\n",
    "\n",
    "# initilizing an \"Adam\" optimizer with learning rate of 0.0001\n",
    "opt = Adam(learning_rate = 0.0001)\n",
    "\n",
    "# compiling the model\n",
    "custom_model_v4.compile(loss=losses, optimizer=opt, metrics=[\"accuracy\"], loss_weights=loss_weights)\n",
    "print(custom_model_v4.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing a dictionary for our training outputs\n",
    "train_targets = {\"class_label\": labels_train, \"bounding_box\": bboxes_train}\n",
    "\n",
    "# constructing a dictionary for our test outputs\n",
    "val_targets = {\"class_label\": labels_val, \"bounding_box\": bboxes_val}\n",
    "\n",
    "# training model for class and bounding box predictions\n",
    "H = custom_model_v4.fit(x_train, train_targets, validation_data=(x_val, val_targets), batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "\n",
    "# saving trained model\n",
    "custom_model_v4.save('../Models/custom_model_v4.h5', save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 1/30\n",
    "# 30/30 [==============================] - 37s 1s/step - loss: 0.8049 - bounding_box_loss: 0.1022 - class_label_loss: 0.7027 - bounding_box_accuracy: 0.5577 - class_label_accuracy: 0.4679 - val_loss: 0.7346 - val_bounding_box_loss: 0.0525 - val_class_label_loss: 0.6820 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.6058\n",
    "# Epoch 2/30\n",
    "# 30/30 [==============================] - 38s 1s/step - loss: 0.7368 - bounding_box_loss: 0.0362 - class_label_loss: 0.7005 - bounding_box_accuracy: 0.6036 - class_label_accuracy: 0.5214 - val_loss: 0.7082 - val_bounding_box_loss: 0.0251 - val_class_label_loss: 0.6831 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.6058\n",
    "# Epoch 3/30\n",
    "# 30/30 [==============================] - 42s 1s/step - loss: 0.7246 - bounding_box_loss: 0.0279 - class_label_loss: 0.6967 - bounding_box_accuracy: 0.5748 - class_label_accuracy: 0.4915 - val_loss: 0.7107 - val_bounding_box_loss: 0.0227 - val_class_label_loss: 0.6880 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.6058\n",
    "# Epoch 4/30\n",
    "# 30/30 [==============================] - 42s 1s/step - loss: 0.7176 - bounding_box_loss: 0.0226 - class_label_loss: 0.6950 - bounding_box_accuracy: 0.6036 - class_label_accuracy: 0.5128 - val_loss: 0.7026 - val_bounding_box_loss: 0.0165 - val_class_label_loss: 0.6861 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.6058\n",
    "# Epoch 5/30\n",
    "# 30/30 [==============================] - 40s 1s/step - loss: 0.7123 - bounding_box_loss: 0.0183 - class_label_loss: 0.6940 - bounding_box_accuracy: 0.5694 - class_label_accuracy: 0.5085 - val_loss: 0.7007 - val_bounding_box_loss: 0.0128 - val_class_label_loss: 0.6879 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.6058\n",
    "# Epoch 6/30\n",
    "# 30/30 [==============================] - 39s 1s/step - loss: 0.6940 - bounding_box_loss: 0.0162 - class_label_loss: 0.6778 - bounding_box_accuracy: 0.5972 - class_label_accuracy: 0.5684 - val_loss: 0.6555 - val_bounding_box_loss: 0.0144 - val_class_label_loss: 0.6411 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.7788\n",
    "# Epoch 7/30\n",
    "# 30/30 [==============================] - 39s 1s/step - loss: 0.5680 - bounding_box_loss: 0.0179 - class_label_loss: 0.5501 - bounding_box_accuracy: 0.5908 - class_label_accuracy: 0.7265 - val_loss: 0.4208 - val_bounding_box_loss: 0.0140 - val_class_label_loss: 0.4068 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.8462\n",
    "# Epoch 8/30\n",
    "# 30/30 [==============================] - 39s 1s/step - loss: 0.4204 - bounding_box_loss: 0.0192 - class_label_loss: 0.4013 - bounding_box_accuracy: 0.5684 - class_label_accuracy: 0.8558 - val_loss: 0.5032 - val_bounding_box_loss: 0.0149 - val_class_label_loss: 0.4884 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.8365\n",
    "# Epoch 9/30\n",
    "# 30/30 [==============================] - 40s 1s/step - loss: 0.4163 - bounding_box_loss: 0.0168 - class_label_loss: 0.3995 - bounding_box_accuracy: 0.6132 - class_label_accuracy: 0.8718 - val_loss: 0.3585 - val_bounding_box_loss: 0.0142 - val_class_label_loss: 0.3443 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.8846\n",
    "# Epoch 10/30\n",
    "# 30/30 [==============================] - 40s 1s/step - loss: 0.3735 - bounding_box_loss: 0.0161 - class_label_loss: 0.3574 - bounding_box_accuracy: 0.5588 - class_label_accuracy: 0.8846 - val_loss: 0.3640 - val_bounding_box_loss: 0.0128 - val_class_label_loss: 0.3512 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.8846\n",
    "# Epoch 11/30\n",
    "# 30/30 [==============================] - 40s 1s/step - loss: 0.3897 - bounding_box_loss: 0.0174 - class_label_loss: 0.3723 - bounding_box_accuracy: 0.6111 - class_label_accuracy: 0.8932 - val_loss: 0.3746 - val_bounding_box_loss: 0.0150 - val_class_label_loss: 0.3596 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.8942\n",
    "# Epoch 12/30\n",
    "# 30/30 [==============================] - 40s 1s/step - loss: 0.3396 - bounding_box_loss: 0.0156 - class_label_loss: 0.3240 - bounding_box_accuracy: 0.5887 - class_label_accuracy: 0.8985 - val_loss: 0.3775 - val_bounding_box_loss: 0.0115 - val_class_label_loss: 0.3660 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.8942\n",
    "# Epoch 13/30\n",
    "# 30/30 [==============================] - 40s 1s/step - loss: 0.4043 - bounding_box_loss: 0.0155 - class_label_loss: 0.3888 - bounding_box_accuracy: 0.6004 - class_label_accuracy: 0.8600 - val_loss: 0.3304 - val_bounding_box_loss: 0.0113 - val_class_label_loss: 0.3191 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.8942\n",
    "# Epoch 14/30\n",
    "# 30/30 [==============================] - 40s 1s/step - loss: 0.3335 - bounding_box_loss: 0.0141 - class_label_loss: 0.3194 - bounding_box_accuracy: 0.6079 - class_label_accuracy: 0.9049 - val_loss: 0.2915 - val_bounding_box_loss: 0.0105 - val_class_label_loss: 0.2810 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9038\n",
    "# Epoch 15/30\n",
    "# 30/30 [==============================] - 41s 1s/step - loss: 0.2951 - bounding_box_loss: 0.0133 - class_label_loss: 0.2818 - bounding_box_accuracy: 0.5951 - class_label_accuracy: 0.9071 - val_loss: 0.2780 - val_bounding_box_loss: 0.0107 - val_class_label_loss: 0.2674 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9135\n",
    "# Epoch 16/30\n",
    "# 30/30 [==============================] - 40s 1s/step - loss: 0.2903 - bounding_box_loss: 0.0151 - class_label_loss: 0.2751 - bounding_box_accuracy: 0.5962 - class_label_accuracy: 0.9124 - val_loss: 0.2750 - val_bounding_box_loss: 0.0107 - val_class_label_loss: 0.2643 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.8942\n",
    "# Epoch 17/30\n",
    "# 30/30 [==============================] - 40s 1s/step - loss: 0.2959 - bounding_box_loss: 0.0133 - class_label_loss: 0.2827 - bounding_box_accuracy: 0.5994 - class_label_accuracy: 0.8942 - val_loss: 0.2670 - val_bounding_box_loss: 0.0100 - val_class_label_loss: 0.2570 - val_bounding_box_accuracy: 0.6346 - val_class_label_accuracy: 0.9135\n",
    "# Epoch 18/30\n",
    "# 30/30 [==============================] - 41s 1s/step - loss: 0.2439 - bounding_box_loss: 0.0127 - class_label_loss: 0.2312 - bounding_box_accuracy: 0.6293 - class_label_accuracy: 0.9263 - val_loss: 0.2784 - val_bounding_box_loss: 0.0100 - val_class_label_loss: 0.2683 - val_bounding_box_accuracy: 0.6346 - val_class_label_accuracy: 0.9038\n",
    "# Epoch 19/30\n",
    "# 30/30 [==============================] - 40s 1s/step - loss: 0.2219 - bounding_box_loss: 0.0124 - class_label_loss: 0.2095 - bounding_box_accuracy: 0.6250 - class_label_accuracy: 0.9423 - val_loss: 0.2612 - val_bounding_box_loss: 0.0121 - val_class_label_loss: 0.2491 - val_bounding_box_accuracy: 0.6442 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 20/30\n",
    "# 30/30 [==============================] - 40s 1s/step - loss: 0.2196 - bounding_box_loss: 0.0126 - class_label_loss: 0.2071 - bounding_box_accuracy: 0.6335 - class_label_accuracy: 0.9444 - val_loss: 0.2994 - val_bounding_box_loss: 0.0096 - val_class_label_loss: 0.2898 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.8846\n",
    "# Epoch 21/30\n",
    "# 30/30 [==============================] - 40s 1s/step - loss: 0.2845 - bounding_box_loss: 0.0123 - class_label_loss: 0.2723 - bounding_box_accuracy: 0.6421 - class_label_accuracy: 0.9081 - val_loss: 0.2388 - val_bounding_box_loss: 0.0097 - val_class_label_loss: 0.2291 - val_bounding_box_accuracy: 0.6346 - val_class_label_accuracy: 0.9135\n",
    "# Epoch 22/30\n",
    "# 30/30 [==============================] - 40s 1s/step - loss: 0.2057 - bounding_box_loss: 0.0120 - class_label_loss: 0.1937 - bounding_box_accuracy: 0.6357 - class_label_accuracy: 0.9466 - val_loss: 0.2681 - val_bounding_box_loss: 0.0097 - val_class_label_loss: 0.2584 - val_bounding_box_accuracy: 0.6442 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 23/30\n",
    "# 30/30 [==============================] - 40s 1s/step - loss: 0.2125 - bounding_box_loss: 0.0115 - class_label_loss: 0.2010 - bounding_box_accuracy: 0.6442 - class_label_accuracy: 0.9498 - val_loss: 0.2669 - val_bounding_box_loss: 0.0113 - val_class_label_loss: 0.2556 - val_bounding_box_accuracy: 0.7019 - val_class_label_accuracy: 0.9135\n",
    "# Epoch 24/30\n",
    "# 30/30 [==============================] - 41s 1s/step - loss: 0.2209 - bounding_box_loss: 0.0126 - class_label_loss: 0.2083 - bounding_box_accuracy: 0.6485 - class_label_accuracy: 0.9402 - val_loss: 0.2522 - val_bounding_box_loss: 0.0106 - val_class_label_loss: 0.2416 - val_bounding_box_accuracy: 0.6635 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 25/30\n",
    "# 30/30 [==============================] - 40s 1s/step - loss: 0.2077 - bounding_box_loss: 0.0113 - class_label_loss: 0.1964 - bounding_box_accuracy: 0.6592 - class_label_accuracy: 0.9519 - val_loss: 0.2460 - val_bounding_box_loss: 0.0093 - val_class_label_loss: 0.2366 - val_bounding_box_accuracy: 0.6827 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 26/30\n",
    "# 30/30 [==============================] - 40s 1s/step - loss: 0.2011 - bounding_box_loss: 0.0113 - class_label_loss: 0.1898 - bounding_box_accuracy: 0.6677 - class_label_accuracy: 0.9551 - val_loss: 0.2820 - val_bounding_box_loss: 0.0085 - val_class_label_loss: 0.2735 - val_bounding_box_accuracy: 0.6731 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 27/30\n",
    "# 30/30 [==============================] - 40s 1s/step - loss: 0.2027 - bounding_box_loss: 0.0109 - class_label_loss: 0.1918 - bounding_box_accuracy: 0.6538 - class_label_accuracy: 0.9498 - val_loss: 0.3636 - val_bounding_box_loss: 0.0125 - val_class_label_loss: 0.3511 - val_bounding_box_accuracy: 0.6827 - val_class_label_accuracy: 0.8846\n",
    "# Epoch 28/30\n",
    "# 30/30 [==============================] - 40s 1s/step - loss: 0.1954 - bounding_box_loss: 0.0112 - class_label_loss: 0.1842 - bounding_box_accuracy: 0.6806 - class_label_accuracy: 0.9541 - val_loss: 0.2596 - val_bounding_box_loss: 0.0088 - val_class_label_loss: 0.2508 - val_bounding_box_accuracy: 0.7212 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 29/30\n",
    "# 30/30 [==============================] - 40s 1s/step - loss: 0.1863 - bounding_box_loss: 0.0107 - class_label_loss: 0.1756 - bounding_box_accuracy: 0.6870 - class_label_accuracy: 0.9530 - val_loss: 0.2623 - val_bounding_box_loss: 0.0084 - val_class_label_loss: 0.2538 - val_bounding_box_accuracy: 0.7019 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 30/30\n",
    "# 30/30 [==============================] - 40s 1s/step - loss: 0.1893 - bounding_box_loss: 0.0113 - class_label_loss: 0.1780 - bounding_box_accuracy: 0.6592 - class_label_accuracy: 0.9583 - val_loss: 0.3014 - val_bounding_box_loss: 0.0093 - val_class_label_loss: 0.2921 - val_bounding_box_accuracy: 0.6731 - val_class_label_accuracy: 0.9038"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting loss for modeling training\n",
    "plot_loss(model_name='custom_model_v4', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training total loss during custom_model_v4 training: 0.18926353752613068\n",
    "# Final epoch training class label loss during custom_model_v4 training: 0.17799201607704163\n",
    "# Final epoch training bounding box loss during custom_model_v4 training: 0.011271528899669647\n",
    "# Final epoch validation total loss during custom_model_v4 training: 0.30140143632888794\n",
    "# Final epoch validation class label loss during custom_model_v4 training: 0.29205840826034546\n",
    "# Final epoch validation bounding box loss during custom_model_v4 training: 0.009343033656477928"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting accuracy for modeling training\n",
    "plot_accuracy(model_name='custom_model_v4', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training class label accurracy during custom_model_v4 training: 0.9583333134651184\n",
    "# Final epoch validation class label accurracy during custom_model_v4 training: 0.9038461446762085"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating model for mAP metrics and inference time and storing result\n",
    "result_val = evaluate_val(model=custom_model_v4, model_name='custom_model_v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC PASCAL mAP for custom_model_v4 validation data: 0.18899163603782654\n",
    "# VOC PASCAL mAP in all points for custom_model_v4 validation data: 0.1653704047203064\n",
    "# COCO mAP for custom_model_v4 validation data: 0.4760621190071106\n",
    "# Average inference time for custom_model_v4 validation data: 0.04254447726102976"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending results of model evaluation to results dataframe (df_results)\n",
    "df_results_val = df_results_val.append(result_val, ignore_index=True)\n",
    "df_results_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating custom model\n",
    "input_layer = Input(shape=(224,224,3), name='input_layer')\n",
    "conv1 = Conv2D(32, (3,3), activation='relu')(input_layer)\n",
    "pool1 = MaxPool2D(pool_size=(2, 2))(conv1)\n",
    "conv2 = Conv2D(64, (3,3), activation='relu')(pool1)\n",
    "pool2 = MaxPool2D(pool_size=(2, 2))(conv2)\n",
    "conv3 = Conv2D(128, (3,3), activation='relu')(pool2)\n",
    "pool3 = MaxPool2D(pool_size=(2, 2))(conv3)\n",
    "conv4 = Conv2D(256, (3,3), activation='relu')(pool3)\n",
    "pool4 = MaxPool2D(pool_size=(2, 2))(conv4)\n",
    "conv5 = Conv2D(512, (3,3), activation='relu')(pool4)\n",
    "pool5 = MaxPool2D(pool_size=(2, 2))(conv5)\n",
    "flat = Flatten()(pool5)\n",
    "\n",
    "# constructing an output layer to predict bounding box coordinates\n",
    "bbox_head1 = Dense(256, activation=\"relu\")(flat)\n",
    "bbox_head2 = Dropout(0.5)(bbox_head1)\n",
    "bbox_head3 = Dense(128, activation=\"relu\")(bbox_head2)\n",
    "bbox_head4 = Dense(64, activation=\"relu\")(bbox_head3)\n",
    "bbox_head5 = Dense(32, activation=\"relu\")(bbox_head4)\n",
    "bbox_head_v5 = Dense(4, activation=\"sigmoid\", name=\"bounding_box\")(bbox_head5)\n",
    "\n",
    "# constructing an output layer to predict class labels\n",
    "class_head1 = Dense(512, activation=\"relu\")(flat)\n",
    "class_head2 = Dropout(0.5)(class_head1)\n",
    "class_head3 = Dense(256, activation=\"relu\")(class_head2)\n",
    "class_head4 = Dropout(0.5)(class_head3)\n",
    "class_head_v5 = Dense(2, activation=\"sigmoid\", name=\"class_label\")(class_head4)\n",
    "\n",
    "# creating a model that accepts an input image and outputs bounding box coordinates and a class label\n",
    "custom_model_v5 = Model(inputs=input_layer, outputs=(bbox_head_v5, class_head_v5))\n",
    "\n",
    "# creating dictionary for setting loss functions\n",
    "losses = {\"class_label\": \"binary_crossentropy\", \"bounding_box\": \"mean_squared_error\"}\n",
    "\n",
    "# creating dictionry for weighting loss functions\n",
    "loss_weights = {\"class_label\": 1.0, \"bounding_box\": 1.0}\n",
    "\n",
    "# initilizing an \"Adam\" optimizer with learning rate of 0.0001\n",
    "opt = Adam(learning_rate = 0.0001)\n",
    "\n",
    "# compiling the model\n",
    "custom_model_v5.compile(loss=losses, optimizer=opt, metrics=[\"accuracy\"], loss_weights=loss_weights)\n",
    "print(custom_model_v5.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing a dictionary for our training outputs\n",
    "train_targets = {\"class_label\": labels_train, \"bounding_box\": bboxes_train}\n",
    "\n",
    "# constructing a dictionary for our test outputs\n",
    "val_targets = {\"class_label\": labels_val, \"bounding_box\": bboxes_val}\n",
    "\n",
    "# training model for class and bounding box predictions\n",
    "H = custom_model_v5.fit(x_train, train_targets, validation_data=(x_val, val_targets), batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "\n",
    "# saving trained model\n",
    "custom_model_v5.save('../Models/custom_model_v5.h5', save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 1/30\n",
    "# 30/30 [==============================] - 35s 1s/step - loss: 0.7581 - bounding_box_loss: 0.0594 - class_label_loss: 0.6987 - bounding_box_accuracy: 0.3974 - class_label_accuracy: 0.5139 - val_loss: 0.7317 - val_bounding_box_loss: 0.0219 - val_class_label_loss: 0.7098 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.3942\n",
    "# Epoch 2/30\n",
    "# 30/30 [==============================] - 39s 1s/step - loss: 0.7216 - bounding_box_loss: 0.0257 - class_label_loss: 0.6958 - bounding_box_accuracy: 0.3921 - class_label_accuracy: 0.5182 - val_loss: 0.6929 - val_bounding_box_loss: 0.0197 - val_class_label_loss: 0.6732 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.6058\n",
    "# Epoch 3/30\n",
    "# 30/30 [==============================] - 37s 1s/step - loss: 0.6929 - bounding_box_loss: 0.0206 - class_label_loss: 0.6723 - bounding_box_accuracy: 0.4925 - class_label_accuracy: 0.6239 - val_loss: 0.6238 - val_bounding_box_loss: 0.0156 - val_class_label_loss: 0.6082 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.7115\n",
    "# Epoch 4/30\n",
    "# 30/30 [==============================] - 42s 1s/step - loss: 0.4831 - bounding_box_loss: 0.0169 - class_label_loss: 0.4662 - bounding_box_accuracy: 0.5438 - class_label_accuracy: 0.8419 - val_loss: 0.4173 - val_bounding_box_loss: 0.0149 - val_class_label_loss: 0.4024 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.8558\n",
    "# Epoch 5/30\n",
    "# 30/30 [==============================] - 46s 2s/step - loss: 0.4136 - bounding_box_loss: 0.0176 - class_label_loss: 0.3960 - bounding_box_accuracy: 0.5929 - class_label_accuracy: 0.8686 - val_loss: 0.4579 - val_bounding_box_loss: 0.0192 - val_class_label_loss: 0.4387 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.8269\n",
    "# Epoch 6/30\n",
    "# 30/30 [==============================] - 50s 2s/step - loss: 0.4133 - bounding_box_loss: 0.0201 - class_label_loss: 0.3932 - bounding_box_accuracy: 0.5652 - class_label_accuracy: 0.8707 - val_loss: 0.5070 - val_bounding_box_loss: 0.0140 - val_class_label_loss: 0.4930 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.8173\n",
    "# Epoch 7/30\n",
    "# 30/30 [==============================] - 45s 1s/step - loss: 0.3684 - bounding_box_loss: 0.0189 - class_label_loss: 0.3494 - bounding_box_accuracy: 0.5748 - class_label_accuracy: 0.8782 - val_loss: 0.3564 - val_bounding_box_loss: 0.0136 - val_class_label_loss: 0.3428 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.8846\n",
    "# Epoch 8/30\n",
    "# 30/30 [==============================] - 44s 1s/step - loss: 0.3606 - bounding_box_loss: 0.0160 - class_label_loss: 0.3446 - bounding_box_accuracy: 0.6026 - class_label_accuracy: 0.8697 - val_loss: 0.3711 - val_bounding_box_loss: 0.0129 - val_class_label_loss: 0.3583 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.8654\n",
    "# Epoch 9/30\n",
    "# 30/30 [==============================] - 43s 1s/step - loss: 0.3404 - bounding_box_loss: 0.0154 - class_label_loss: 0.3250 - bounding_box_accuracy: 0.6111 - class_label_accuracy: 0.8846 - val_loss: 0.3347 - val_bounding_box_loss: 0.0117 - val_class_label_loss: 0.3230 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.8750\n",
    "# Epoch 10/30\n",
    "# 30/30 [==============================] - 44s 1s/step - loss: 0.2901 - bounding_box_loss: 0.0140 - class_label_loss: 0.2760 - bounding_box_accuracy: 0.5844 - class_label_accuracy: 0.9028 - val_loss: 0.3101 - val_bounding_box_loss: 0.0108 - val_class_label_loss: 0.2993 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9038\n",
    "# Epoch 11/30\n",
    "# 30/30 [==============================] - 44s 1s/step - loss: 0.2661 - bounding_box_loss: 0.0138 - class_label_loss: 0.2523 - bounding_box_accuracy: 0.5951 - class_label_accuracy: 0.9092 - val_loss: 0.2993 - val_bounding_box_loss: 0.0111 - val_class_label_loss: 0.2883 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9135\n",
    "# Epoch 12/30\n",
    "# 30/30 [==============================] - 45s 1s/step - loss: 0.2975 - bounding_box_loss: 0.0152 - class_label_loss: 0.2823 - bounding_box_accuracy: 0.6079 - class_label_accuracy: 0.9017 - val_loss: 0.3326 - val_bounding_box_loss: 0.0259 - val_class_label_loss: 0.3066 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.8942\n",
    "# Epoch 13/30\n",
    "# 30/30 [==============================] - 44s 1s/step - loss: 0.2646 - bounding_box_loss: 0.0145 - class_label_loss: 0.2501 - bounding_box_accuracy: 0.6047 - class_label_accuracy: 0.9092 - val_loss: 0.3156 - val_bounding_box_loss: 0.0106 - val_class_label_loss: 0.3051 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.8846\n",
    "# Epoch 14/30\n",
    "# 30/30 [==============================] - 45s 1s/step - loss: 0.2343 - bounding_box_loss: 0.0127 - class_label_loss: 0.2216 - bounding_box_accuracy: 0.6154 - class_label_accuracy: 0.9220 - val_loss: 0.2873 - val_bounding_box_loss: 0.0100 - val_class_label_loss: 0.2773 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9038\n",
    "# Epoch 15/30\n",
    "# 30/30 [==============================] - 45s 2s/step - loss: 0.2326 - bounding_box_loss: 0.0136 - class_label_loss: 0.2190 - bounding_box_accuracy: 0.6143 - class_label_accuracy: 0.9370 - val_loss: 0.2731 - val_bounding_box_loss: 0.0109 - val_class_label_loss: 0.2622 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9135\n",
    "# Epoch 16/30\n",
    "# 30/30 [==============================] - 46s 2s/step - loss: 0.2079 - bounding_box_loss: 0.0129 - class_label_loss: 0.1950 - bounding_box_accuracy: 0.6111 - class_label_accuracy: 0.9402 - val_loss: 0.2760 - val_bounding_box_loss: 0.0100 - val_class_label_loss: 0.2660 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.8942\n",
    "# Epoch 17/30\n",
    "# 30/30 [==============================] - 49s 2s/step - loss: 0.2123 - bounding_box_loss: 0.0123 - class_label_loss: 0.2000 - bounding_box_accuracy: 0.6175 - class_label_accuracy: 0.9391 - val_loss: 0.3019 - val_bounding_box_loss: 0.0105 - val_class_label_loss: 0.2913 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.8942\n",
    "# Epoch 18/30\n",
    "# 30/30 [==============================] - 52s 2s/step - loss: 0.1866 - bounding_box_loss: 0.0122 - class_label_loss: 0.1744 - bounding_box_accuracy: 0.6282 - class_label_accuracy: 0.9519 - val_loss: 0.2863 - val_bounding_box_loss: 0.0097 - val_class_label_loss: 0.2766 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.8942\n",
    "# Epoch 19/30\n",
    "# 30/30 [==============================] - 53s 2s/step - loss: 0.1987 - bounding_box_loss: 0.0119 - class_label_loss: 0.1868 - bounding_box_accuracy: 0.6100 - class_label_accuracy: 0.9487 - val_loss: 0.2652 - val_bounding_box_loss: 0.0098 - val_class_label_loss: 0.2555 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9038\n",
    "# Epoch 20/30\n",
    "# 30/30 [==============================] - 52s 2s/step - loss: 0.1838 - bounding_box_loss: 0.0119 - class_label_loss: 0.1719 - bounding_box_accuracy: 0.6143 - class_label_accuracy: 0.9562 - val_loss: 0.2490 - val_bounding_box_loss: 0.0096 - val_class_label_loss: 0.2395 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 21/30\n",
    "# 30/30 [==============================] - 53s 2s/step - loss: 0.1976 - bounding_box_loss: 0.0121 - class_label_loss: 0.1856 - bounding_box_accuracy: 0.6154 - class_label_accuracy: 0.9487 - val_loss: 0.2659 - val_bounding_box_loss: 0.0097 - val_class_label_loss: 0.2562 - val_bounding_box_accuracy: 0.6154 - val_class_label_accuracy: 0.9135\n",
    "# Epoch 22/30\n",
    "# 30/30 [==============================] - 52s 2s/step - loss: 0.1733 - bounding_box_loss: 0.0114 - class_label_loss: 0.1619 - bounding_box_accuracy: 0.6132 - class_label_accuracy: 0.9594 - val_loss: 0.2496 - val_bounding_box_loss: 0.0094 - val_class_label_loss: 0.2402 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 23/30\n",
    "# 30/30 [==============================] - 53s 2s/step - loss: 0.1658 - bounding_box_loss: 0.0114 - class_label_loss: 0.1544 - bounding_box_accuracy: 0.6143 - class_label_accuracy: 0.9541 - val_loss: 0.2757 - val_bounding_box_loss: 0.0097 - val_class_label_loss: 0.2660 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9135\n",
    "# Epoch 24/30\n",
    "# 30/30 [==============================] - 52s 2s/step - loss: 0.2021 - bounding_box_loss: 0.0117 - class_label_loss: 0.1904 - bounding_box_accuracy: 0.6261 - class_label_accuracy: 0.9455 - val_loss: 0.2578 - val_bounding_box_loss: 0.0100 - val_class_label_loss: 0.2478 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 25/30\n",
    "# 30/30 [==============================] - 54s 2s/step - loss: 0.1634 - bounding_box_loss: 0.0112 - class_label_loss: 0.1522 - bounding_box_accuracy: 0.6239 - class_label_accuracy: 0.9594 - val_loss: 0.2715 - val_bounding_box_loss: 0.0090 - val_class_label_loss: 0.2625 - val_bounding_box_accuracy: 0.6346 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 26/30\n",
    "# 30/30 [==============================] - 51s 2s/step - loss: 0.1780 - bounding_box_loss: 0.0112 - class_label_loss: 0.1668 - bounding_box_accuracy: 0.6175 - class_label_accuracy: 0.9455 - val_loss: 0.2930 - val_bounding_box_loss: 0.0091 - val_class_label_loss: 0.2839 - val_bounding_box_accuracy: 0.6346 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 27/30\n",
    "# 30/30 [==============================] - 52s 2s/step - loss: 0.1671 - bounding_box_loss: 0.0109 - class_label_loss: 0.1562 - bounding_box_accuracy: 0.6239 - class_label_accuracy: 0.9530 - val_loss: 0.3676 - val_bounding_box_loss: 0.0097 - val_class_label_loss: 0.3579 - val_bounding_box_accuracy: 0.6154 - val_class_label_accuracy: 0.8846\n",
    "# Epoch 28/30\n",
    "# 30/30 [==============================] - 51s 2s/step - loss: 0.1839 - bounding_box_loss: 0.0109 - class_label_loss: 0.1730 - bounding_box_accuracy: 0.6132 - class_label_accuracy: 0.9466 - val_loss: 0.2645 - val_bounding_box_loss: 0.0088 - val_class_label_loss: 0.2557 - val_bounding_box_accuracy: 0.6346 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 29/30\n",
    "# 30/30 [==============================] - 52s 2s/step - loss: 0.1579 - bounding_box_loss: 0.0113 - class_label_loss: 0.1467 - bounding_box_accuracy: 0.6293 - class_label_accuracy: 0.9573 - val_loss: 0.2553 - val_bounding_box_loss: 0.0089 - val_class_label_loss: 0.2465 - val_bounding_box_accuracy: 0.6442 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 30/30\n",
    "# 30/30 [==============================] - 53s 2s/step - loss: 0.1572 - bounding_box_loss: 0.0105 - class_label_loss: 0.1468 - bounding_box_accuracy: 0.6207 - class_label_accuracy: 0.9647 - val_loss: 0.3647 - val_bounding_box_loss: 0.0124 - val_class_label_loss: 0.3523 - val_bounding_box_accuracy: 0.5865 - val_class_label_accuracy: 0.8846"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting loss for modeling training\n",
    "plot_loss(model_name='custom_model_v5', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training total loss during custom_model_v5 training: 0.1572408527135849\n",
    "# Final epoch training class label loss during custom_model_v5 training: 0.14675331115722656\n",
    "# Final epoch training bounding box loss during custom_model_v5 training: 0.010487538762390614\n",
    "# Final epoch validation total loss during custom_model_v5 training: 0.3647090494632721\n",
    "# Final epoch validation class label loss during custom_model_v5 training: 0.35227474570274353\n",
    "# Final epoch validation bounding box loss during custom_model_v5 training: 0.012434309348464012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting accuracy for modeling training\n",
    "plot_accuracy(model_name='custom_model_v5', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training class label accurracy during custom_model_v5 training: 0.9647436141967773\n",
    "# Final epoch validation class label accurracy during custom_model_v5 training: 0.8846153616905212"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating model for mAP metrics and inference time and storing result\n",
    "result_val = evaluate_val(model=custom_model_v5, model_name='custom_model_v5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC PASCAL mAP for custom_model_v5 validation data: 0.170617938041687\n",
    "# VOC PASCAL mAP in all points for custom_model_v5 validation data: 0.12523342669010162\n",
    "# COCO mAP for custom_model_v5 validation data: 0.3684782087802887\n",
    "# Average inference time for custom_model_v5 validation data: 0.04056189381159269"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending results of model evaluation to results dataframe (df_results)\n",
    "df_results_val = df_results_val.append(result_val, ignore_index=True)\n",
    "df_results_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating custom model\n",
    "input_layer = Input(shape=(224,224,3), name='input_layer')\n",
    "conv1 = Conv2D(32, (3,3), activation='relu')(input_layer)\n",
    "pool1 = MaxPool2D(pool_size=(2, 2))(conv1)\n",
    "conv2 = Conv2D(64, (3,3), activation='relu')(pool1)\n",
    "pool2 = MaxPool2D(pool_size=(2, 2))(conv2)\n",
    "conv3 = Conv2D(128, (3,3), activation='relu')(pool2)\n",
    "pool3 = MaxPool2D(pool_size=(2, 2))(conv3)\n",
    "conv4 = Conv2D(256, (3,3), activation='relu')(pool3)\n",
    "pool4 = MaxPool2D(pool_size=(2, 2))(conv4)\n",
    "conv5 = Conv2D(512, (3,3), activation='relu')(pool4)\n",
    "pool5 = MaxPool2D(pool_size=(2, 2))(conv5)\n",
    "flat = Flatten()(pool5)\n",
    "\n",
    "# constructing an output layer to predict bounding box coordinates\n",
    "bbox_head1 = Dense(256, activation=\"relu\")(flat)\n",
    "bbox_head2 = Dense(128, activation=\"relu\")(bbox_head1)\n",
    "bbox_head3 = Dense(64, activation=\"relu\")(bbox_head2)\n",
    "bbox_head4 = Dense(32, activation=\"relu\")(bbox_head3)\n",
    "bbox_head_v6 = Dense(4, activation=\"sigmoid\", name=\"bounding_box\")(bbox_head4)\n",
    "\n",
    "# constructing an output layer to predict class labels\n",
    "class_head1 = Dense(256, activation=\"relu\")(flat)\n",
    "class_head2 = Dropout(0.5)(class_head1)\n",
    "class_head3 = Dense(128, activation=\"relu\")(class_head2)\n",
    "class_head4 = Dropout(0.5)(class_head3)\n",
    "class_head_v6 = Dense(2, activation=\"sigmoid\", name=\"class_label\")(class_head4)\n",
    "\n",
    "# creating a model that accepts an input image and outputs bounding box coordinates and a class label\n",
    "custom_model_v6 = Model(inputs=input_layer, outputs=(bbox_head_v6, class_head_v6))\n",
    "\n",
    "# creating dictionary for setting loss functions\n",
    "losses = {\"class_label\": \"binary_crossentropy\", \"bounding_box\": \"mean_squared_error\"}\n",
    "\n",
    "# creating dictionry for weighting loss functions\n",
    "loss_weights = {\"class_label\": 1.0, \"bounding_box\": 1.0}\n",
    "\n",
    "# initilizing an \"Adam\" optimizer with learning rate of 0.0001\n",
    "opt = Adam(learning_rate = 0.0001)\n",
    "\n",
    "# compiling the model\n",
    "custom_model_v6.compile(loss=losses, optimizer=opt, metrics=[\"accuracy\"], loss_weights=loss_weights)\n",
    "print(custom_model_v6.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing a dictionary for our training outputs\n",
    "train_targets = {\"class_label\": labels_train, \"bounding_box\": bboxes_train}\n",
    "\n",
    "# constructing a dictionary for our test outputs\n",
    "val_targets = {\"class_label\": labels_val, \"bounding_box\": bboxes_val}\n",
    "\n",
    "# training model for class and bounding box predictions\n",
    "H = custom_model_v6.fit(x_train, train_targets, validation_data=(x_val, val_targets), batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "\n",
    "# saving trained model\n",
    "custom_model_v6.save('../Models/custom_model_v6.h5', save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 1/30\n",
    "# 30/30 [==============================] - 34s 1s/step - loss: 0.7527 - bounding_box_loss: 0.0512 - class_label_loss: 0.7015 - bounding_box_accuracy: 0.4679 - class_label_accuracy: 0.5043 - val_loss: 0.7169 - val_bounding_box_loss: 0.0230 - val_class_label_loss: 0.6940 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.3942\n",
    "# Epoch 2/30\n",
    "# 30/30 [==============================] - 38s 1s/step - loss: 0.7186 - bounding_box_loss: 0.0217 - class_label_loss: 0.6969 - bounding_box_accuracy: 0.6154 - class_label_accuracy: 0.4979 - val_loss: 0.7030 - val_bounding_box_loss: 0.0203 - val_class_label_loss: 0.6826 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.6058\n",
    "# Epoch 3/30\n",
    "# 30/30 [==============================] - 50s 2s/step - loss: 0.6887 - bounding_box_loss: 0.0176 - class_label_loss: 0.6711 - bounding_box_accuracy: 0.6154 - class_label_accuracy: 0.6464 - val_loss: 0.6297 - val_bounding_box_loss: 0.0178 - val_class_label_loss: 0.6119 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.8365\n",
    "# Epoch 4/30\n",
    "# 30/30 [==============================] - 51s 2s/step - loss: 0.5204 - bounding_box_loss: 0.0155 - class_label_loss: 0.5048 - bounding_box_accuracy: 0.6154 - class_label_accuracy: 0.8269 - val_loss: 0.4948 - val_bounding_box_loss: 0.0128 - val_class_label_loss: 0.4820 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.8077\n",
    "# Epoch 5/30\n",
    "# 30/30 [==============================] - 44s 1s/step - loss: 0.4173 - bounding_box_loss: 0.0146 - class_label_loss: 0.4027 - bounding_box_accuracy: 0.6132 - class_label_accuracy: 0.8515 - val_loss: 0.4297 - val_bounding_box_loss: 0.0133 - val_class_label_loss: 0.4164 - val_bounding_box_accuracy: 0.5385 - val_class_label_accuracy: 0.8462\n",
    "# Epoch 6/30\n",
    "# 30/30 [==============================] - 38s 1s/step - loss: 0.3998 - bounding_box_loss: 0.0141 - class_label_loss: 0.3857 - bounding_box_accuracy: 0.6079 - class_label_accuracy: 0.8675 - val_loss: 0.4163 - val_bounding_box_loss: 0.0123 - val_class_label_loss: 0.4041 - val_bounding_box_accuracy: 0.6154 - val_class_label_accuracy: 0.8654\n",
    "# Epoch 7/30\n",
    "# 30/30 [==============================] - 36s 1s/step - loss: 0.3829 - bounding_box_loss: 0.0140 - class_label_loss: 0.3689 - bounding_box_accuracy: 0.6410 - class_label_accuracy: 0.8803 - val_loss: 0.3819 - val_bounding_box_loss: 0.0122 - val_class_label_loss: 0.3697 - val_bounding_box_accuracy: 0.5096 - val_class_label_accuracy: 0.8654\n",
    "# Epoch 8/30\n",
    "# 30/30 [==============================] - 36s 1s/step - loss: 0.3718 - bounding_box_loss: 0.0127 - class_label_loss: 0.3591 - bounding_box_accuracy: 0.6378 - class_label_accuracy: 0.8910 - val_loss: 0.3309 - val_bounding_box_loss: 0.0124 - val_class_label_loss: 0.3185 - val_bounding_box_accuracy: 0.6346 - val_class_label_accuracy: 0.8942\n",
    "# Epoch 9/30\n",
    "# 30/30 [==============================] - 36s 1s/step - loss: 0.3283 - bounding_box_loss: 0.0129 - class_label_loss: 0.3154 - bounding_box_accuracy: 0.6603 - class_label_accuracy: 0.8953 - val_loss: 0.3281 - val_bounding_box_loss: 0.0101 - val_class_label_loss: 0.3180 - val_bounding_box_accuracy: 0.6442 - val_class_label_accuracy: 0.8846\n",
    "# Epoch 10/30\n",
    "# 30/30 [==============================] - 36s 1s/step - loss: 0.3026 - bounding_box_loss: 0.0118 - class_label_loss: 0.2908 - bounding_box_accuracy: 0.6709 - class_label_accuracy: 0.9006 - val_loss: 0.2951 - val_bounding_box_loss: 0.0101 - val_class_label_loss: 0.2850 - val_bounding_box_accuracy: 0.6827 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 11/30\n",
    "# 30/30 [==============================] - 37s 1s/step - loss: 0.2929 - bounding_box_loss: 0.0132 - class_label_loss: 0.2798 - bounding_box_accuracy: 0.6816 - class_label_accuracy: 0.9060 - val_loss: 0.2970 - val_bounding_box_loss: 0.0093 - val_class_label_loss: 0.2878 - val_bounding_box_accuracy: 0.6827 - val_class_label_accuracy: 0.9038\n",
    "# Epoch 12/30\n",
    "# 30/30 [==============================] - 35s 1s/step - loss: 0.2707 - bounding_box_loss: 0.0117 - class_label_loss: 0.2590 - bounding_box_accuracy: 0.7073 - class_label_accuracy: 0.9145 - val_loss: 0.2823 - val_bounding_box_loss: 0.0095 - val_class_label_loss: 0.2728 - val_bounding_box_accuracy: 0.6923 - val_class_label_accuracy: 0.8846\n",
    "# Epoch 13/30\n",
    "# 30/30 [==============================] - 35s 1s/step - loss: 0.2757 - bounding_box_loss: 0.0105 - class_label_loss: 0.2652 - bounding_box_accuracy: 0.7073 - class_label_accuracy: 0.9071 - val_loss: 0.2640 - val_bounding_box_loss: 0.0091 - val_class_label_loss: 0.2549 - val_bounding_box_accuracy: 0.7019 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 14/30\n",
    "# 30/30 [==============================] - 35s 1s/step - loss: 0.2401 - bounding_box_loss: 0.0104 - class_label_loss: 0.2297 - bounding_box_accuracy: 0.7115 - class_label_accuracy: 0.9306 - val_loss: 0.5152 - val_bounding_box_loss: 0.0125 - val_class_label_loss: 0.5027 - val_bounding_box_accuracy: 0.6923 - val_class_label_accuracy: 0.7981\n",
    "# Epoch 15/30\n",
    "# 30/30 [==============================] - 34s 1s/step - loss: 0.2699 - bounding_box_loss: 0.0113 - class_label_loss: 0.2586 - bounding_box_accuracy: 0.7190 - class_label_accuracy: 0.9156 - val_loss: 0.3041 - val_bounding_box_loss: 0.0089 - val_class_label_loss: 0.2952 - val_bounding_box_accuracy: 0.6827 - val_class_label_accuracy: 0.8750\n",
    "# Epoch 16/30\n",
    "# 30/30 [==============================] - 34s 1s/step - loss: 0.2314 - bounding_box_loss: 0.0107 - class_label_loss: 0.2207 - bounding_box_accuracy: 0.7276 - class_label_accuracy: 0.9284 - val_loss: 0.2984 - val_bounding_box_loss: 0.0105 - val_class_label_loss: 0.2879 - val_bounding_box_accuracy: 0.6731 - val_class_label_accuracy: 0.9038\n",
    "# Epoch 17/30\n",
    "# 30/30 [==============================] - 34s 1s/step - loss: 0.2666 - bounding_box_loss: 0.0106 - class_label_loss: 0.2559 - bounding_box_accuracy: 0.7372 - class_label_accuracy: 0.9338 - val_loss: 0.2554 - val_bounding_box_loss: 0.0085 - val_class_label_loss: 0.2469 - val_bounding_box_accuracy: 0.6923 - val_class_label_accuracy: 0.9327\n",
    "# Epoch 18/30\n",
    "# 30/30 [==============================] - 35s 1s/step - loss: 0.1919 - bounding_box_loss: 0.0101 - class_label_loss: 0.1818 - bounding_box_accuracy: 0.7244 - class_label_accuracy: 0.9594 - val_loss: 0.2528 - val_bounding_box_loss: 0.0087 - val_class_label_loss: 0.2441 - val_bounding_box_accuracy: 0.7019 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 19/30\n",
    "# 30/30 [==============================] - 34s 1s/step - loss: 0.1743 - bounding_box_loss: 0.0091 - class_label_loss: 0.1652 - bounding_box_accuracy: 0.7179 - class_label_accuracy: 0.9583 - val_loss: 0.2692 - val_bounding_box_loss: 0.0081 - val_class_label_loss: 0.2611 - val_bounding_box_accuracy: 0.7115 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 20/30\n",
    "# 30/30 [==============================] - 34s 1s/step - loss: 0.2133 - bounding_box_loss: 0.0090 - class_label_loss: 0.2044 - bounding_box_accuracy: 0.7511 - class_label_accuracy: 0.9487 - val_loss: 0.3273 - val_bounding_box_loss: 0.0088 - val_class_label_loss: 0.3184 - val_bounding_box_accuracy: 0.7019 - val_class_label_accuracy: 0.9038\n",
    "# Epoch 21/30\n",
    "# 30/30 [==============================] - 35s 1s/step - loss: 0.2056 - bounding_box_loss: 0.0092 - class_label_loss: 0.1964 - bounding_box_accuracy: 0.7318 - class_label_accuracy: 0.9434 - val_loss: 0.2640 - val_bounding_box_loss: 0.0093 - val_class_label_loss: 0.2547 - val_bounding_box_accuracy: 0.7212 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 22/30\n",
    "# 30/30 [==============================] - 34s 1s/step - loss: 0.1802 - bounding_box_loss: 0.0091 - class_label_loss: 0.1710 - bounding_box_accuracy: 0.7361 - class_label_accuracy: 0.9551 - val_loss: 0.2564 - val_bounding_box_loss: 0.0087 - val_class_label_loss: 0.2477 - val_bounding_box_accuracy: 0.6731 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 23/30\n",
    "# 30/30 [==============================] - 35s 1s/step - loss: 0.1658 - bounding_box_loss: 0.0087 - class_label_loss: 0.1572 - bounding_box_accuracy: 0.7404 - class_label_accuracy: 0.9637 - val_loss: 0.2522 - val_bounding_box_loss: 0.0095 - val_class_label_loss: 0.2427 - val_bounding_box_accuracy: 0.7019 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 24/30\n",
    "# 30/30 [==============================] - 35s 1s/step - loss: 0.1827 - bounding_box_loss: 0.0085 - class_label_loss: 0.1742 - bounding_box_accuracy: 0.7457 - class_label_accuracy: 0.9605 - val_loss: 0.2364 - val_bounding_box_loss: 0.0085 - val_class_label_loss: 0.2279 - val_bounding_box_accuracy: 0.6923 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 25/30\n",
    "# 30/30 [==============================] - 35s 1s/step - loss: 0.2065 - bounding_box_loss: 0.0088 - class_label_loss: 0.1977 - bounding_box_accuracy: 0.7457 - class_label_accuracy: 0.9444 - val_loss: 0.2591 - val_bounding_box_loss: 0.0119 - val_class_label_loss: 0.2472 - val_bounding_box_accuracy: 0.6538 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 26/30\n",
    "# 30/30 [==============================] - 35s 1s/step - loss: 0.2336 - bounding_box_loss: 0.0099 - class_label_loss: 0.2237 - bounding_box_accuracy: 0.7425 - class_label_accuracy: 0.9348 - val_loss: 0.2779 - val_bounding_box_loss: 0.0089 - val_class_label_loss: 0.2690 - val_bounding_box_accuracy: 0.7019 - val_class_label_accuracy: 0.9135\n",
    "# Epoch 27/30\n",
    "# 30/30 [==============================] - 36s 1s/step - loss: 0.1780 - bounding_box_loss: 0.0105 - class_label_loss: 0.1676 - bounding_box_accuracy: 0.7179 - class_label_accuracy: 0.9615 - val_loss: 0.2308 - val_bounding_box_loss: 0.0096 - val_class_label_loss: 0.2212 - val_bounding_box_accuracy: 0.7404 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 28/30\n",
    "# 30/30 [==============================] - 37s 1s/step - loss: 0.1562 - bounding_box_loss: 0.0087 - class_label_loss: 0.1475 - bounding_box_accuracy: 0.7286 - class_label_accuracy: 0.9647 - val_loss: 0.2802 - val_bounding_box_loss: 0.0094 - val_class_label_loss: 0.2707 - val_bounding_box_accuracy: 0.6731 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 29/30\n",
    "# 30/30 [==============================] - 40s 1s/step - loss: 0.1585 - bounding_box_loss: 0.0083 - class_label_loss: 0.1502 - bounding_box_accuracy: 0.7137 - class_label_accuracy: 0.9669 - val_loss: 0.2368 - val_bounding_box_loss: 0.0094 - val_class_label_loss: 0.2274 - val_bounding_box_accuracy: 0.7019 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 30/30\n",
    "# 30/30 [==============================] - 43s 1s/step - loss: 0.1542 - bounding_box_loss: 0.0080 - class_label_loss: 0.1462 - bounding_box_accuracy: 0.7607 - class_label_accuracy: 0.9712 - val_loss: 0.2295 - val_bounding_box_loss: 0.0085 - val_class_label_loss: 0.2211 - val_bounding_box_accuracy: 0.7019 - val_class_label_accuracy: 0.9519"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting loss for modeling training\n",
    "plot_loss(model_name='custom_model_v6', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training total loss during custom_model_v6 training: 0.1542416363954544\n",
    "# Final epoch training class label loss during custom_model_v6 training: 0.14620766043663025\n",
    "# Final epoch training bounding box loss during custom_model_v6 training: 0.008033973164856434\n",
    "# Final epoch validation total loss during custom_model_v6 training: 0.22953829169273376\n",
    "# Final epoch validation class label loss during custom_model_v6 training: 0.22106511890888214\n",
    "# Final epoch validation bounding box loss during custom_model_v6 training: 0.008473181165754795"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting accuracy for modeling training\n",
    "plot_accuracy(model_name='custom_model_v6', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training class label accurracy during custom_model_v6 training: 0.9711538553237915\n",
    "# Final epoch validation class label accurracy during custom_model_v6 training: 0.9519230723381042"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating model for mAP metrics and inference time and storing result\n",
    "result_val = evaluate_val(model=custom_model_v6, model_name='custom_model_v6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC PASCAL mAP for custom_model_v6 validation data: 0.32911473512649536\n",
    "# VOC PASCAL mAP in all points for custom_model_v6 validation data: 0.3077871799468994\n",
    "# COCO mAP for custom_model_v6 validation data: 0.48572319746017456\n",
    "# Average inference time for custom_model_v6 validation data: 0.0394685841523684"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending results of model evaluation to results dataframe (df_results)\n",
    "df_results_val = df_results_val.append(result_val, ignore_index=True)\n",
    "df_results_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating custom model\n",
    "input_layer = Input(shape=(224,224,3), name='input_layer')\n",
    "conv1 = Conv2D(32, (3,3), activation='relu')(input_layer)\n",
    "pool1 = MaxPool2D(pool_size=(2, 2))(conv1)\n",
    "conv2 = Conv2D(64, (3,3), activation='relu')(pool1)\n",
    "pool2 = MaxPool2D(pool_size=(2, 2))(conv2)\n",
    "conv3 = Conv2D(128, (3,3), activation='relu')(pool2)\n",
    "pool3 = MaxPool2D(pool_size=(2, 2))(conv3)\n",
    "conv4 = Conv2D(256, (3,3), activation='relu')(pool3)\n",
    "pool4 = MaxPool2D(pool_size=(2, 2))(conv4)\n",
    "conv5 = Conv2D(512, (3,3), activation='relu')(pool4)\n",
    "pool5 = MaxPool2D(pool_size=(2, 2))(conv5)\n",
    "flat = Flatten()(pool5)\n",
    "\n",
    "# constructing an output layer to predict bounding box coordinates\n",
    "bbox_head1 = Dense(128, activation=\"relu\")(flat)\n",
    "bbox_head2 = Dropout(0.5)(bbox_head1)\n",
    "bbox_head3 = Dense(64, activation=\"relu\")(bbox_head2)\n",
    "bbox_head4 = Dense(32, activation=\"relu\")(bbox_head3)\n",
    "bbox_head_v7 = Dense(4, activation=\"sigmoid\", name=\"bounding_box\")(bbox_head4)\n",
    "\n",
    "# constructing an output layer to predict class labels\n",
    "class_head1 = Dense(128, activation=\"relu\")(flat)\n",
    "class_head2 = Dropout(0.5)(class_head1)\n",
    "class_head3 = Dense(64, activation=\"relu\")(class_head2)\n",
    "class_head4 = Dropout(0.5)(class_head3)\n",
    "class_head_v7 = Dense(2, activation=\"sigmoid\", name=\"class_label\")(class_head4)\n",
    "\n",
    "# creating a model that accepts an input image and outputs bounding box coordinates and a class label\n",
    "custom_model_v7 = Model(inputs=input_layer, outputs=(bbox_head_v7, class_head_v7))\n",
    "\n",
    "# creating dictionary for setting loss functions\n",
    "losses = {\"class_label\": \"binary_crossentropy\", \"bounding_box\": \"mean_squared_error\"}\n",
    "\n",
    "# creating dictionry for weighting loss functions\n",
    "loss_weights = {\"class_label\": 1.0, \"bounding_box\": 1.0}\n",
    "\n",
    "# initilizing an \"Adam\" optimizer with learning rate of 0.0001\n",
    "opt = Adam(learning_rate = 0.0001)\n",
    "\n",
    "# compiling the model\n",
    "custom_model_v7.compile(loss=losses, optimizer=opt, metrics=[\"accuracy\"], loss_weights=loss_weights)\n",
    "print(custom_model_v7.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing a dictionary for our training outputs\n",
    "train_targets = {\"class_label\": labels_train, \"bounding_box\": bboxes_train}\n",
    "\n",
    "# constructing a dictionary for our test outputs\n",
    "val_targets = {\"class_label\": labels_val, \"bounding_box\": bboxes_val}\n",
    "\n",
    "# training model for class and bounding box predictions\n",
    "H = custom_model_v7.fit(x_train, train_targets, validation_data=(x_val, val_targets), batch_size=batch_size, epochs=50, verbose=1)\n",
    "\n",
    "# saving trained model\n",
    "custom_model_v7.save('../Models/custom_model_v7.h5', save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 1/50\n",
    "# 30/30 [==============================] - 36s 1s/step - loss: 0.7405 - bounding_box_loss: 0.0455 - class_label_loss: 0.6950 - bounding_box_accuracy: 0.5107 - class_label_accuracy: 0.5235 - val_loss: 0.7051 - val_bounding_box_loss: 0.0225 - val_class_label_loss: 0.6826 - val_bounding_box_accuracy: 0.3750 - val_class_label_accuracy: 0.6058\n",
    "# Epoch 2/50\n",
    "# 30/30 [==============================] - 38s 1s/step - loss: 0.7198 - bounding_box_loss: 0.0247 - class_label_loss: 0.6951 - bounding_box_accuracy: 0.5427 - class_label_accuracy: 0.5085 - val_loss: 0.7083 - val_bounding_box_loss: 0.0197 - val_class_label_loss: 0.6885 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.6346\n",
    "# Epoch 3/50\n",
    "# 30/30 [==============================] - 36s 1s/step - loss: 0.7124 - bounding_box_loss: 0.0230 - class_label_loss: 0.6894 - bounding_box_accuracy: 0.5684 - class_label_accuracy: 0.5288 - val_loss: 0.7000 - val_bounding_box_loss: 0.0181 - val_class_label_loss: 0.6819 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.6538\n",
    "# Epoch 4/50\n",
    "# 30/30 [==============================] - 36s 1s/step - loss: 0.6726 - bounding_box_loss: 0.0192 - class_label_loss: 0.6535 - bounding_box_accuracy: 0.5577 - class_label_accuracy: 0.6720 - val_loss: 0.5572 - val_bounding_box_loss: 0.0128 - val_class_label_loss: 0.5445 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.8654\n",
    "# Epoch 5/50\n",
    "# 30/30 [==============================] - 38s 1s/step - loss: 0.5787 - bounding_box_loss: 0.0179 - class_label_loss: 0.5608 - bounding_box_accuracy: 0.5769 - class_label_accuracy: 0.7596 - val_loss: 0.5160 - val_bounding_box_loss: 0.0138 - val_class_label_loss: 0.5022 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.7788\n",
    "# Epoch 6/50\n",
    "# 30/30 [==============================] - 38s 1s/step - loss: 0.4927 - bounding_box_loss: 0.0177 - class_label_loss: 0.4750 - bounding_box_accuracy: 0.5694 - class_label_accuracy: 0.8312 - val_loss: 0.4765 - val_bounding_box_loss: 0.0136 - val_class_label_loss: 0.4629 - val_bounding_box_accuracy: 0.5769 - val_class_label_accuracy: 0.7981\n",
    "# Epoch 7/50\n",
    "# 30/30 [==============================] - 41s 1s/step - loss: 0.4310 - bounding_box_loss: 0.0170 - class_label_loss: 0.4140 - bounding_box_accuracy: 0.5598 - class_label_accuracy: 0.8686 - val_loss: 0.3779 - val_bounding_box_loss: 0.0146 - val_class_label_loss: 0.3633 - val_bounding_box_accuracy: 0.6442 - val_class_label_accuracy: 0.8846\n",
    "# Epoch 8/50\n",
    "# 30/30 [==============================] - 40s 1s/step - loss: 0.4107 - bounding_box_loss: 0.0160 - class_label_loss: 0.3947 - bounding_box_accuracy: 0.6047 - class_label_accuracy: 0.8761 - val_loss: 0.4648 - val_bounding_box_loss: 0.0129 - val_class_label_loss: 0.4518 - val_bounding_box_accuracy: 0.6442 - val_class_label_accuracy: 0.8365\n",
    "# Epoch 9/50\n",
    "# 30/30 [==============================] - 41s 1s/step - loss: 0.3940 - bounding_box_loss: 0.0168 - class_label_loss: 0.3772 - bounding_box_accuracy: 0.5801 - class_label_accuracy: 0.8718 - val_loss: 0.3480 - val_bounding_box_loss: 0.0149 - val_class_label_loss: 0.3331 - val_bounding_box_accuracy: 0.5865 - val_class_label_accuracy: 0.8846\n",
    "# Epoch 10/50\n",
    "# 30/30 [==============================] - 40s 1s/step - loss: 0.3885 - bounding_box_loss: 0.0151 - class_label_loss: 0.3734 - bounding_box_accuracy: 0.5716 - class_label_accuracy: 0.8835 - val_loss: 0.4055 - val_bounding_box_loss: 0.0151 - val_class_label_loss: 0.3904 - val_bounding_box_accuracy: 0.5962 - val_class_label_accuracy: 0.8750\n",
    "# Epoch 11/50\n",
    "# 30/30 [==============================] - 40s 1s/step - loss: 0.4339 - bounding_box_loss: 0.0194 - class_label_loss: 0.4145 - bounding_box_accuracy: 0.5780 - class_label_accuracy: 0.8494 - val_loss: 0.4402 - val_bounding_box_loss: 0.0147 - val_class_label_loss: 0.4255 - val_bounding_box_accuracy: 0.6058 - val_class_label_accuracy: 0.8558\n",
    "# Epoch 12/50\n",
    "# 30/30 [==============================] - 42s 1s/step - loss: 0.4064 - bounding_box_loss: 0.0152 - class_label_loss: 0.3911 - bounding_box_accuracy: 0.5929 - class_label_accuracy: 0.8707 - val_loss: 0.3541 - val_bounding_box_loss: 0.0111 - val_class_label_loss: 0.3429 - val_bounding_box_accuracy: 0.6058 - val_class_label_accuracy: 0.8846\n",
    "# Epoch 13/50\n",
    "# 30/30 [==============================] - 41s 1s/step - loss: 0.3686 - bounding_box_loss: 0.0140 - class_label_loss: 0.3546 - bounding_box_accuracy: 0.6026 - class_label_accuracy: 0.8889 - val_loss: 0.3490 - val_bounding_box_loss: 0.0105 - val_class_label_loss: 0.3385 - val_bounding_box_accuracy: 0.6058 - val_class_label_accuracy: 0.8942\n",
    "# Epoch 14/50\n",
    "# 30/30 [==============================] - 42s 1s/step - loss: 0.3240 - bounding_box_loss: 0.0160 - class_label_loss: 0.3080 - bounding_box_accuracy: 0.6015 - class_label_accuracy: 0.8985 - val_loss: 0.3580 - val_bounding_box_loss: 0.0128 - val_class_label_loss: 0.3452 - val_bounding_box_accuracy: 0.6058 - val_class_label_accuracy: 0.8942\n",
    "# Epoch 15/50\n",
    "# 30/30 [==============================] - 40s 1s/step - loss: 0.3069 - bounding_box_loss: 0.0139 - class_label_loss: 0.2930 - bounding_box_accuracy: 0.5908 - class_label_accuracy: 0.9060 - val_loss: 0.3168 - val_bounding_box_loss: 0.0111 - val_class_label_loss: 0.3057 - val_bounding_box_accuracy: 0.6058 - val_class_label_accuracy: 0.8942\n",
    "# Epoch 16/50\n",
    "# 30/30 [==============================] - 40s 1s/step - loss: 0.3000 - bounding_box_loss: 0.0139 - class_label_loss: 0.2861 - bounding_box_accuracy: 0.5897 - class_label_accuracy: 0.8974 - val_loss: 0.2842 - val_bounding_box_loss: 0.0101 - val_class_label_loss: 0.2741 - val_bounding_box_accuracy: 0.6154 - val_class_label_accuracy: 0.8942\n",
    "# Epoch 17/50\n",
    "# 30/30 [==============================] - 41s 1s/step - loss: 0.2830 - bounding_box_loss: 0.0132 - class_label_loss: 0.2697 - bounding_box_accuracy: 0.6100 - class_label_accuracy: 0.9081 - val_loss: 0.2844 - val_bounding_box_loss: 0.0099 - val_class_label_loss: 0.2744 - val_bounding_box_accuracy: 0.6154 - val_class_label_accuracy: 0.9038\n",
    "# Epoch 18/50\n",
    "# 30/30 [==============================] - 42s 1s/step - loss: 0.3037 - bounding_box_loss: 0.0141 - class_label_loss: 0.2896 - bounding_box_accuracy: 0.6004 - class_label_accuracy: 0.9103 - val_loss: 0.2636 - val_bounding_box_loss: 0.0097 - val_class_label_loss: 0.2539 - val_bounding_box_accuracy: 0.6154 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 19/50\n",
    "# 30/30 [==============================] - 40s 1s/step - loss: 0.2681 - bounding_box_loss: 0.0126 - class_label_loss: 0.2555 - bounding_box_accuracy: 0.6026 - class_label_accuracy: 0.9156 - val_loss: 0.2679 - val_bounding_box_loss: 0.0123 - val_class_label_loss: 0.2555 - val_bounding_box_accuracy: 0.6154 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 20/50\n",
    "# 30/30 [==============================] - 40s 1s/step - loss: 0.2413 - bounding_box_loss: 0.0131 - class_label_loss: 0.2282 - bounding_box_accuracy: 0.6282 - class_label_accuracy: 0.9391 - val_loss: 0.2982 - val_bounding_box_loss: 0.0104 - val_class_label_loss: 0.2879 - val_bounding_box_accuracy: 0.6154 - val_class_label_accuracy: 0.9135\n",
    "# Epoch 21/50\n",
    "# 30/30 [==============================] - 41s 1s/step - loss: 0.2524 - bounding_box_loss: 0.0134 - class_label_loss: 0.2390 - bounding_box_accuracy: 0.6036 - class_label_accuracy: 0.9423 - val_loss: 0.3058 - val_bounding_box_loss: 0.0166 - val_class_label_loss: 0.2891 - val_bounding_box_accuracy: 0.6635 - val_class_label_accuracy: 0.9038\n",
    "# Epoch 22/50\n",
    "# 30/30 [==============================] - 41s 1s/step - loss: 0.2313 - bounding_box_loss: 0.0133 - class_label_loss: 0.2180 - bounding_box_accuracy: 0.6004 - class_label_accuracy: 0.9402 - val_loss: 0.2557 - val_bounding_box_loss: 0.0127 - val_class_label_loss: 0.2429 - val_bounding_box_accuracy: 0.6635 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 23/50\n",
    "# 30/30 [==============================] - 41s 1s/step - loss: 0.2425 - bounding_box_loss: 0.0121 - class_label_loss: 0.2304 - bounding_box_accuracy: 0.6368 - class_label_accuracy: 0.9509 - val_loss: 0.2981 - val_bounding_box_loss: 0.0122 - val_class_label_loss: 0.2859 - val_bounding_box_accuracy: 0.6442 - val_class_label_accuracy: 0.9038\n",
    "# Epoch 24/50\n",
    "# 30/30 [==============================] - 41s 1s/step - loss: 0.2323 - bounding_box_loss: 0.0122 - class_label_loss: 0.2202 - bounding_box_accuracy: 0.5972 - class_label_accuracy: 0.9476 - val_loss: 0.2423 - val_bounding_box_loss: 0.0130 - val_class_label_loss: 0.2293 - val_bounding_box_accuracy: 0.6635 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 25/50\n",
    "# 30/30 [==============================] - 42s 1s/step - loss: 0.2330 - bounding_box_loss: 0.0129 - class_label_loss: 0.2202 - bounding_box_accuracy: 0.6421 - class_label_accuracy: 0.9530 - val_loss: 0.2836 - val_bounding_box_loss: 0.0104 - val_class_label_loss: 0.2732 - val_bounding_box_accuracy: 0.6827 - val_class_label_accuracy: 0.9135\n",
    "# Epoch 26/50\n",
    "# 30/30 [==============================] - 40s 1s/step - loss: 0.2048 - bounding_box_loss: 0.0122 - class_label_loss: 0.1927 - bounding_box_accuracy: 0.6453 - class_label_accuracy: 0.9551 - val_loss: 0.3870 - val_bounding_box_loss: 0.0187 - val_class_label_loss: 0.3683 - val_bounding_box_accuracy: 0.6346 - val_class_label_accuracy: 0.8654\n",
    "# Epoch 27/50\n",
    "# 30/30 [==============================] - 41s 1s/step - loss: 0.2311 - bounding_box_loss: 0.0124 - class_label_loss: 0.2186 - bounding_box_accuracy: 0.6442 - class_label_accuracy: 0.9434 - val_loss: 0.2581 - val_bounding_box_loss: 0.0091 - val_class_label_loss: 0.2490 - val_bounding_box_accuracy: 0.6442 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 28/50\n",
    "# 30/30 [==============================] - 41s 1s/step - loss: 0.2130 - bounding_box_loss: 0.0118 - class_label_loss: 0.2011 - bounding_box_accuracy: 0.6325 - class_label_accuracy: 0.9594 - val_loss: 0.2423 - val_bounding_box_loss: 0.0103 - val_class_label_loss: 0.2320 - val_bounding_box_accuracy: 0.6635 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 29/50\n",
    "# 30/30 [==============================] - 40s 1s/step - loss: 0.2122 - bounding_box_loss: 0.0114 - class_label_loss: 0.2008 - bounding_box_accuracy: 0.6143 - class_label_accuracy: 0.9658 - val_loss: 0.2434 - val_bounding_box_loss: 0.0097 - val_class_label_loss: 0.2337 - val_bounding_box_accuracy: 0.6538 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 30/50\n",
    "# 30/30 [==============================] - 39s 1s/step - loss: 0.2064 - bounding_box_loss: 0.0116 - class_label_loss: 0.1947 - bounding_box_accuracy: 0.6571 - class_label_accuracy: 0.9562 - val_loss: 0.2813 - val_bounding_box_loss: 0.0104 - val_class_label_loss: 0.2709 - val_bounding_box_accuracy: 0.6635 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 31/50\n",
    "# 30/30 [==============================] - 41s 1s/step - loss: 0.1839 - bounding_box_loss: 0.0118 - class_label_loss: 0.1721 - bounding_box_accuracy: 0.6474 - class_label_accuracy: 0.9626 - val_loss: 0.2520 - val_bounding_box_loss: 0.0093 - val_class_label_loss: 0.2427 - val_bounding_box_accuracy: 0.6538 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 32/50\n",
    "# 30/30 [==============================] - 41s 1s/step - loss: 0.1824 - bounding_box_loss: 0.0115 - class_label_loss: 0.1709 - bounding_box_accuracy: 0.6474 - class_label_accuracy: 0.9637 - val_loss: 0.2624 - val_bounding_box_loss: 0.0093 - val_class_label_loss: 0.2530 - val_bounding_box_accuracy: 0.6731 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 33/50\n",
    "# 30/30 [==============================] - 40s 1s/step - loss: 0.1917 - bounding_box_loss: 0.0114 - class_label_loss: 0.1803 - bounding_box_accuracy: 0.6335 - class_label_accuracy: 0.9583 - val_loss: 0.2274 - val_bounding_box_loss: 0.0120 - val_class_label_loss: 0.2154 - val_bounding_box_accuracy: 0.6635 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 34/50\n",
    "# 30/30 [==============================] - 41s 1s/step - loss: 0.2205 - bounding_box_loss: 0.0122 - class_label_loss: 0.2083 - bounding_box_accuracy: 0.6442 - class_label_accuracy: 0.9530 - val_loss: 0.3884 - val_bounding_box_loss: 0.0111 - val_class_label_loss: 0.3773 - val_bounding_box_accuracy: 0.6058 - val_class_label_accuracy: 0.8942\n",
    "# Epoch 35/50\n",
    "# 30/30 [==============================] - 40s 1s/step - loss: 0.3799 - bounding_box_loss: 0.0168 - class_label_loss: 0.3630 - bounding_box_accuracy: 0.6389 - class_label_accuracy: 0.8889 - val_loss: 0.3143 - val_bounding_box_loss: 0.0111 - val_class_label_loss: 0.3032 - val_bounding_box_accuracy: 0.6442 - val_class_label_accuracy: 0.8942\n",
    "# Epoch 36/50\n",
    "# 30/30 [==============================] - 39s 1s/step - loss: 0.2077 - bounding_box_loss: 0.0123 - class_label_loss: 0.1953 - bounding_box_accuracy: 0.6239 - class_label_accuracy: 0.9551 - val_loss: 0.3106 - val_bounding_box_loss: 0.0217 - val_class_label_loss: 0.2889 - val_bounding_box_accuracy: 0.6923 - val_class_label_accuracy: 0.9038\n",
    "# Epoch 37/50\n",
    "# 30/30 [==============================] - 42s 1s/step - loss: 0.2632 - bounding_box_loss: 0.0151 - class_label_loss: 0.2482 - bounding_box_accuracy: 0.6261 - class_label_accuracy: 0.9199 - val_loss: 0.2888 - val_bounding_box_loss: 0.0209 - val_class_label_loss: 0.2679 - val_bounding_box_accuracy: 0.6731 - val_class_label_accuracy: 0.9231\n",
    "# Epoch 38/50\n",
    "# 30/30 [==============================] - 40s 1s/step - loss: 0.1972 - bounding_box_loss: 0.0126 - class_label_loss: 0.1846 - bounding_box_accuracy: 0.6015 - class_label_accuracy: 0.9562 - val_loss: 0.2316 - val_bounding_box_loss: 0.0119 - val_class_label_loss: 0.2197 - val_bounding_box_accuracy: 0.6635 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 39/50\n",
    "# 30/30 [==============================] - 40s 1s/step - loss: 0.1805 - bounding_box_loss: 0.0112 - class_label_loss: 0.1693 - bounding_box_accuracy: 0.6538 - class_label_accuracy: 0.9690 - val_loss: 0.2572 - val_bounding_box_loss: 0.0109 - val_class_label_loss: 0.2463 - val_bounding_box_accuracy: 0.6635 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 40/50\n",
    "# 30/30 [==============================] - 42s 1s/step - loss: 0.1807 - bounding_box_loss: 0.0112 - class_label_loss: 0.1696 - bounding_box_accuracy: 0.6517 - class_label_accuracy: 0.9647 - val_loss: 0.2627 - val_bounding_box_loss: 0.0113 - val_class_label_loss: 0.2514 - val_bounding_box_accuracy: 0.6538 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 41/50\n",
    "# 30/30 [==============================] - 42s 1s/step - loss: 0.1910 - bounding_box_loss: 0.0113 - class_label_loss: 0.1797 - bounding_box_accuracy: 0.6517 - class_label_accuracy: 0.9573 - val_loss: 0.2282 - val_bounding_box_loss: 0.0094 - val_class_label_loss: 0.2188 - val_bounding_box_accuracy: 0.6731 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 42/50\n",
    "# 30/30 [==============================] - 42s 1s/step - loss: 0.1820 - bounding_box_loss: 0.0106 - class_label_loss: 0.1714 - bounding_box_accuracy: 0.6581 - class_label_accuracy: 0.9637 - val_loss: 0.3030 - val_bounding_box_loss: 0.0137 - val_class_label_loss: 0.2893 - val_bounding_box_accuracy: 0.6538 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 43/50\n",
    "# 30/30 [==============================] - 39s 1s/step - loss: 0.1803 - bounding_box_loss: 0.0110 - class_label_loss: 0.1693 - bounding_box_accuracy: 0.6816 - class_label_accuracy: 0.9594 - val_loss: 0.2444 - val_bounding_box_loss: 0.0091 - val_class_label_loss: 0.2353 - val_bounding_box_accuracy: 0.6635 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 44/50\n",
    "# 30/30 [==============================] - 39s 1s/step - loss: 0.1745 - bounding_box_loss: 0.0109 - class_label_loss: 0.1636 - bounding_box_accuracy: 0.6613 - class_label_accuracy: 0.9679 - val_loss: 0.2532 - val_bounding_box_loss: 0.0107 - val_class_label_loss: 0.2424 - val_bounding_box_accuracy: 0.6923 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 45/50\n",
    "# 30/30 [==============================] - 39s 1s/step - loss: 0.1594 - bounding_box_loss: 0.0103 - class_label_loss: 0.1490 - bounding_box_accuracy: 0.6699 - class_label_accuracy: 0.9658 - val_loss: 0.2580 - val_bounding_box_loss: 0.0112 - val_class_label_loss: 0.2468 - val_bounding_box_accuracy: 0.6442 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 46/50\n",
    "# 30/30 [==============================] - 39s 1s/step - loss: 0.1659 - bounding_box_loss: 0.0104 - class_label_loss: 0.1555 - bounding_box_accuracy: 0.6432 - class_label_accuracy: 0.9690 - val_loss: 0.2184 - val_bounding_box_loss: 0.0099 - val_class_label_loss: 0.2084 - val_bounding_box_accuracy: 0.6827 - val_class_label_accuracy: 0.9519\n",
    "# Epoch 47/50\n",
    "# 30/30 [==============================] - 39s 1s/step - loss: 0.1654 - bounding_box_loss: 0.0101 - class_label_loss: 0.1553 - bounding_box_accuracy: 0.6528 - class_label_accuracy: 0.9647 - val_loss: 0.2731 - val_bounding_box_loss: 0.0102 - val_class_label_loss: 0.2629 - val_bounding_box_accuracy: 0.6635 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 48/50\n",
    "# 30/30 [==============================] - 41s 1s/step - loss: 0.1705 - bounding_box_loss: 0.0103 - class_label_loss: 0.1602 - bounding_box_accuracy: 0.6677 - class_label_accuracy: 0.9594 - val_loss: 0.2524 - val_bounding_box_loss: 0.0086 - val_class_label_loss: 0.2438 - val_bounding_box_accuracy: 0.6250 - val_class_label_accuracy: 0.9135\n",
    "# Epoch 49/50\n",
    "# 30/30 [==============================] - 42s 1s/step - loss: 0.1676 - bounding_box_loss: 0.0114 - class_label_loss: 0.1562 - bounding_box_accuracy: 0.6667 - class_label_accuracy: 0.9637 - val_loss: 0.2528 - val_bounding_box_loss: 0.0092 - val_class_label_loss: 0.2436 - val_bounding_box_accuracy: 0.6635 - val_class_label_accuracy: 0.9423\n",
    "# Epoch 50/50\n",
    "# 30/30 [==============================] - 41s 1s/step - loss: 0.1387 - bounding_box_loss: 0.0100 - class_label_loss: 0.1287 - bounding_box_accuracy: 0.6912 - class_label_accuracy: 0.9733 - val_loss: 0.2759 - val_bounding_box_loss: 0.0095 - val_class_label_loss: 0.2663 - val_bounding_box_accuracy: 0.6731 - val_class_label_accuracy: 0.9327"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting loss for modeling training\n",
    "plot_loss(model_name='custom_model_v7', num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training total loss during custom_model_v7 training: 0.1387065201997757\n",
    "# Final epoch training class label loss during custom_model_v7 training: 0.12869958579540253\n",
    "# Final epoch training bounding box loss during custom_model_v7 training: 0.010006933473050594\n",
    "# Final epoch validation total loss during custom_model_v7 training: 0.2758542001247406\n",
    "# Final epoch validation class label loss during custom_model_v7 training: 0.2663050889968872\n",
    "# Final epoch validation bounding box loss during custom_model_v7 training: 0.00954910647124052"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting accuracy for modeling training\n",
    "plot_accuracy(model_name='custom_model_v7', num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final epoch training class label accurracy during custom_model_v7 training: 0.9732906222343445\n",
    "# Final epoch validation class label accurracy during custom_model_v7 training: 0.932692289352417"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating model for mAP metrics and inference time and storing result\n",
    "result_val = evaluate_val(model=custom_model_v7, model_name='custom_model_v7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC PASCAL mAP for custom_model_v7 validation data: 0.26699012517929077\n",
    "# VOC PASCAL mAP in all points for custom_model_v7 validation data: 0.23832297325134277\n",
    "# COCO mAP for custom_model_v7 validation data: 0.42586636543273926\n",
    "# Average inference time for custom_model_v7 validation data: 0.03902188860453092"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending results of model evaluation to results dataframe (df_results)\n",
    "df_results_val = df_results_val.append(result_val, ignore_index=True)\n",
    "df_results_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we save all the results from the modeling process in a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving results dataframe as csv\n",
    "df_results_val.to_csv(r'../df_results_val.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the results from all the models' evaluations during training below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>voc_pascal_map</th>\n",
       "      <th>voc_pascal_map_allpts</th>\n",
       "      <th>coco_map</th>\n",
       "      <th>avg_inf_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xception_model_v1</td>\n",
       "      <td>0.396248</td>\n",
       "      <td>0.358191</td>\n",
       "      <td>0.477826</td>\n",
       "      <td>0.130748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xception_model_v2</td>\n",
       "      <td>0.129913</td>\n",
       "      <td>0.096901</td>\n",
       "      <td>0.338039</td>\n",
       "      <td>0.118183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vgg16_model_v1</td>\n",
       "      <td>0.477733</td>\n",
       "      <td>0.449670</td>\n",
       "      <td>0.530383</td>\n",
       "      <td>0.160573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vgg16_model_v2</td>\n",
       "      <td>0.143388</td>\n",
       "      <td>0.118884</td>\n",
       "      <td>0.411369</td>\n",
       "      <td>0.151350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vgg16_model_v3</td>\n",
       "      <td>0.325206</td>\n",
       "      <td>0.326035</td>\n",
       "      <td>0.543216</td>\n",
       "      <td>0.149908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>vgg16_model_v4</td>\n",
       "      <td>0.327362</td>\n",
       "      <td>0.309798</td>\n",
       "      <td>0.507422</td>\n",
       "      <td>0.158064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>vgg16_model_v5</td>\n",
       "      <td>0.344542</td>\n",
       "      <td>0.338866</td>\n",
       "      <td>0.522116</td>\n",
       "      <td>0.156698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>vgg16_model_v6</td>\n",
       "      <td>0.420486</td>\n",
       "      <td>0.405343</td>\n",
       "      <td>0.552509</td>\n",
       "      <td>0.157125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>vgg16_model_v7</td>\n",
       "      <td>0.336114</td>\n",
       "      <td>0.336334</td>\n",
       "      <td>0.531315</td>\n",
       "      <td>0.152425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>resnet152v2_model_v1</td>\n",
       "      <td>0.425562</td>\n",
       "      <td>0.424156</td>\n",
       "      <td>0.495845</td>\n",
       "      <td>0.258722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>resnet152v2_model_v2</td>\n",
       "      <td>0.138426</td>\n",
       "      <td>0.075679</td>\n",
       "      <td>0.338843</td>\n",
       "      <td>0.242696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>inceptionresnetv2_model_v1</td>\n",
       "      <td>0.367082</td>\n",
       "      <td>0.377072</td>\n",
       "      <td>0.476452</td>\n",
       "      <td>0.148716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>inceptionresnetv2_model_v2</td>\n",
       "      <td>0.233962</td>\n",
       "      <td>0.197513</td>\n",
       "      <td>0.401749</td>\n",
       "      <td>0.153812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mobilenetv2_model_v1</td>\n",
       "      <td>0.351913</td>\n",
       "      <td>0.344712</td>\n",
       "      <td>0.526454</td>\n",
       "      <td>0.072912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mobilenetv2_model_v2</td>\n",
       "      <td>0.130821</td>\n",
       "      <td>0.073480</td>\n",
       "      <td>0.338740</td>\n",
       "      <td>0.071017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>mobilenetv2_model_v3</td>\n",
       "      <td>0.229407</td>\n",
       "      <td>0.173645</td>\n",
       "      <td>0.296418</td>\n",
       "      <td>0.071613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>densenet201_model_v1</td>\n",
       "      <td>0.391484</td>\n",
       "      <td>0.366518</td>\n",
       "      <td>0.500181</td>\n",
       "      <td>0.211093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>densenet201_model_v2</td>\n",
       "      <td>0.191695</td>\n",
       "      <td>0.139780</td>\n",
       "      <td>0.399111</td>\n",
       "      <td>0.205991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>densenet201_model_v3</td>\n",
       "      <td>0.342318</td>\n",
       "      <td>0.321824</td>\n",
       "      <td>0.414973</td>\n",
       "      <td>0.207552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>nasnetlarge_model_v1</td>\n",
       "      <td>0.451958</td>\n",
       "      <td>0.437492</td>\n",
       "      <td>0.514896</td>\n",
       "      <td>0.273407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>nasnetlarge_model_v2</td>\n",
       "      <td>0.135382</td>\n",
       "      <td>0.066333</td>\n",
       "      <td>0.318429</td>\n",
       "      <td>0.371174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>custom_model_v1</td>\n",
       "      <td>0.293027</td>\n",
       "      <td>0.276247</td>\n",
       "      <td>0.526225</td>\n",
       "      <td>0.040169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>custom_model_v2</td>\n",
       "      <td>0.149513</td>\n",
       "      <td>0.111420</td>\n",
       "      <td>0.396043</td>\n",
       "      <td>0.040156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>custom_model_v3</td>\n",
       "      <td>0.113536</td>\n",
       "      <td>0.077397</td>\n",
       "      <td>0.490176</td>\n",
       "      <td>0.039542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>custom_model_v4</td>\n",
       "      <td>0.188992</td>\n",
       "      <td>0.165370</td>\n",
       "      <td>0.476062</td>\n",
       "      <td>0.042544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>custom_model_v5</td>\n",
       "      <td>0.170618</td>\n",
       "      <td>0.125233</td>\n",
       "      <td>0.368478</td>\n",
       "      <td>0.040562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>custom_model_v6</td>\n",
       "      <td>0.329115</td>\n",
       "      <td>0.307787</td>\n",
       "      <td>0.485723</td>\n",
       "      <td>0.039469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>custom_model_v7</td>\n",
       "      <td>0.266990</td>\n",
       "      <td>0.238323</td>\n",
       "      <td>0.425866</td>\n",
       "      <td>0.039022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         model  voc_pascal_map  voc_pascal_map_allpts  \\\n",
       "0            xception_model_v1        0.396248               0.358191   \n",
       "1            xception_model_v2        0.129913               0.096901   \n",
       "2               vgg16_model_v1        0.477733               0.449670   \n",
       "3               vgg16_model_v2        0.143388               0.118884   \n",
       "4               vgg16_model_v3        0.325206               0.326035   \n",
       "5               vgg16_model_v4        0.327362               0.309798   \n",
       "6               vgg16_model_v5        0.344542               0.338866   \n",
       "7               vgg16_model_v6        0.420486               0.405343   \n",
       "8               vgg16_model_v7        0.336114               0.336334   \n",
       "9         resnet152v2_model_v1        0.425562               0.424156   \n",
       "10        resnet152v2_model_v2        0.138426               0.075679   \n",
       "11  inceptionresnetv2_model_v1        0.367082               0.377072   \n",
       "12  inceptionresnetv2_model_v2        0.233962               0.197513   \n",
       "13        mobilenetv2_model_v1        0.351913               0.344712   \n",
       "14        mobilenetv2_model_v2        0.130821               0.073480   \n",
       "15        mobilenetv2_model_v3        0.229407               0.173645   \n",
       "16        densenet201_model_v1        0.391484               0.366518   \n",
       "17        densenet201_model_v2        0.191695               0.139780   \n",
       "18        densenet201_model_v3        0.342318               0.321824   \n",
       "19        nasnetlarge_model_v1        0.451958               0.437492   \n",
       "20        nasnetlarge_model_v2        0.135382               0.066333   \n",
       "21             custom_model_v1        0.293027               0.276247   \n",
       "22             custom_model_v2        0.149513               0.111420   \n",
       "23             custom_model_v3        0.113536               0.077397   \n",
       "24             custom_model_v4        0.188992               0.165370   \n",
       "25             custom_model_v5        0.170618               0.125233   \n",
       "26             custom_model_v6        0.329115               0.307787   \n",
       "27             custom_model_v7        0.266990               0.238323   \n",
       "\n",
       "    coco_map  avg_inf_time  \n",
       "0   0.477826      0.130748  \n",
       "1   0.338039      0.118183  \n",
       "2   0.530383      0.160573  \n",
       "3   0.411369      0.151350  \n",
       "4   0.543216      0.149908  \n",
       "5   0.507422      0.158064  \n",
       "6   0.522116      0.156698  \n",
       "7   0.552509      0.157125  \n",
       "8   0.531315      0.152425  \n",
       "9   0.495845      0.258722  \n",
       "10  0.338843      0.242696  \n",
       "11  0.476452      0.148716  \n",
       "12  0.401749      0.153812  \n",
       "13  0.526454      0.072912  \n",
       "14  0.338740      0.071017  \n",
       "15  0.296418      0.071613  \n",
       "16  0.500181      0.211093  \n",
       "17  0.399111      0.205991  \n",
       "18  0.414973      0.207552  \n",
       "19  0.514896      0.273407  \n",
       "20  0.318429      0.371174  \n",
       "21  0.526225      0.040169  \n",
       "22  0.396043      0.040156  \n",
       "23  0.490176      0.039542  \n",
       "24  0.476062      0.042544  \n",
       "25  0.368478      0.040562  \n",
       "26  0.485723      0.039469  \n",
       "27  0.425866      0.039022  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading validation results dataframe (df_results_val) from csv\n",
    "df_results_val = pd.read_csv('../Evaluation/df_results_val.csv')\n",
    "df_results_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current standard for object detection model metrics is the COCO Mean Average Precision score so we will be using that as our main driving metric for our models' evaluation. However, we feel it is important to also consider the inference time as secondary metric due to potential processing constraints in the final model's deployment use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will order the model iterations from higest to lowest COCO mAP socre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>voc_pascal_map</th>\n",
       "      <th>voc_pascal_map_allpts</th>\n",
       "      <th>coco_map</th>\n",
       "      <th>avg_inf_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>vgg16_model_v6</td>\n",
       "      <td>0.420486</td>\n",
       "      <td>0.405343</td>\n",
       "      <td>0.552509</td>\n",
       "      <td>0.157125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vgg16_model_v3</td>\n",
       "      <td>0.325206</td>\n",
       "      <td>0.326035</td>\n",
       "      <td>0.543216</td>\n",
       "      <td>0.149908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>vgg16_model_v7</td>\n",
       "      <td>0.336114</td>\n",
       "      <td>0.336334</td>\n",
       "      <td>0.531315</td>\n",
       "      <td>0.152425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vgg16_model_v1</td>\n",
       "      <td>0.477733</td>\n",
       "      <td>0.449670</td>\n",
       "      <td>0.530383</td>\n",
       "      <td>0.160573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mobilenetv2_model_v1</td>\n",
       "      <td>0.351913</td>\n",
       "      <td>0.344712</td>\n",
       "      <td>0.526454</td>\n",
       "      <td>0.072912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>custom_model_v1</td>\n",
       "      <td>0.293027</td>\n",
       "      <td>0.276247</td>\n",
       "      <td>0.526225</td>\n",
       "      <td>0.040169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>vgg16_model_v5</td>\n",
       "      <td>0.344542</td>\n",
       "      <td>0.338866</td>\n",
       "      <td>0.522116</td>\n",
       "      <td>0.156698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>nasnetlarge_model_v1</td>\n",
       "      <td>0.451958</td>\n",
       "      <td>0.437492</td>\n",
       "      <td>0.514896</td>\n",
       "      <td>0.273407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>vgg16_model_v4</td>\n",
       "      <td>0.327362</td>\n",
       "      <td>0.309798</td>\n",
       "      <td>0.507422</td>\n",
       "      <td>0.158064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>densenet201_model_v1</td>\n",
       "      <td>0.391484</td>\n",
       "      <td>0.366518</td>\n",
       "      <td>0.500181</td>\n",
       "      <td>0.211093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>resnet152v2_model_v1</td>\n",
       "      <td>0.425562</td>\n",
       "      <td>0.424156</td>\n",
       "      <td>0.495845</td>\n",
       "      <td>0.258722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>custom_model_v3</td>\n",
       "      <td>0.113536</td>\n",
       "      <td>0.077397</td>\n",
       "      <td>0.490176</td>\n",
       "      <td>0.039542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>custom_model_v6</td>\n",
       "      <td>0.329115</td>\n",
       "      <td>0.307787</td>\n",
       "      <td>0.485723</td>\n",
       "      <td>0.039469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xception_model_v1</td>\n",
       "      <td>0.396248</td>\n",
       "      <td>0.358191</td>\n",
       "      <td>0.477826</td>\n",
       "      <td>0.130748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>inceptionresnetv2_model_v1</td>\n",
       "      <td>0.367082</td>\n",
       "      <td>0.377072</td>\n",
       "      <td>0.476452</td>\n",
       "      <td>0.148716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>custom_model_v4</td>\n",
       "      <td>0.188992</td>\n",
       "      <td>0.165370</td>\n",
       "      <td>0.476062</td>\n",
       "      <td>0.042544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>custom_model_v7</td>\n",
       "      <td>0.266990</td>\n",
       "      <td>0.238323</td>\n",
       "      <td>0.425866</td>\n",
       "      <td>0.039022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>densenet201_model_v3</td>\n",
       "      <td>0.342318</td>\n",
       "      <td>0.321824</td>\n",
       "      <td>0.414973</td>\n",
       "      <td>0.207552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vgg16_model_v2</td>\n",
       "      <td>0.143388</td>\n",
       "      <td>0.118884</td>\n",
       "      <td>0.411369</td>\n",
       "      <td>0.151350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>inceptionresnetv2_model_v2</td>\n",
       "      <td>0.233962</td>\n",
       "      <td>0.197513</td>\n",
       "      <td>0.401749</td>\n",
       "      <td>0.153812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>densenet201_model_v2</td>\n",
       "      <td>0.191695</td>\n",
       "      <td>0.139780</td>\n",
       "      <td>0.399111</td>\n",
       "      <td>0.205991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>custom_model_v2</td>\n",
       "      <td>0.149513</td>\n",
       "      <td>0.111420</td>\n",
       "      <td>0.396043</td>\n",
       "      <td>0.040156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>custom_model_v5</td>\n",
       "      <td>0.170618</td>\n",
       "      <td>0.125233</td>\n",
       "      <td>0.368478</td>\n",
       "      <td>0.040562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>resnet152v2_model_v2</td>\n",
       "      <td>0.138426</td>\n",
       "      <td>0.075679</td>\n",
       "      <td>0.338843</td>\n",
       "      <td>0.242696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mobilenetv2_model_v2</td>\n",
       "      <td>0.130821</td>\n",
       "      <td>0.073480</td>\n",
       "      <td>0.338740</td>\n",
       "      <td>0.071017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xception_model_v2</td>\n",
       "      <td>0.129913</td>\n",
       "      <td>0.096901</td>\n",
       "      <td>0.338039</td>\n",
       "      <td>0.118183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>nasnetlarge_model_v2</td>\n",
       "      <td>0.135382</td>\n",
       "      <td>0.066333</td>\n",
       "      <td>0.318429</td>\n",
       "      <td>0.371174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>mobilenetv2_model_v3</td>\n",
       "      <td>0.229407</td>\n",
       "      <td>0.173645</td>\n",
       "      <td>0.296418</td>\n",
       "      <td>0.071613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         model  voc_pascal_map  voc_pascal_map_allpts  \\\n",
       "7               vgg16_model_v6        0.420486               0.405343   \n",
       "4               vgg16_model_v3        0.325206               0.326035   \n",
       "8               vgg16_model_v7        0.336114               0.336334   \n",
       "2               vgg16_model_v1        0.477733               0.449670   \n",
       "13        mobilenetv2_model_v1        0.351913               0.344712   \n",
       "21             custom_model_v1        0.293027               0.276247   \n",
       "6               vgg16_model_v5        0.344542               0.338866   \n",
       "19        nasnetlarge_model_v1        0.451958               0.437492   \n",
       "5               vgg16_model_v4        0.327362               0.309798   \n",
       "16        densenet201_model_v1        0.391484               0.366518   \n",
       "9         resnet152v2_model_v1        0.425562               0.424156   \n",
       "23             custom_model_v3        0.113536               0.077397   \n",
       "26             custom_model_v6        0.329115               0.307787   \n",
       "0            xception_model_v1        0.396248               0.358191   \n",
       "11  inceptionresnetv2_model_v1        0.367082               0.377072   \n",
       "24             custom_model_v4        0.188992               0.165370   \n",
       "27             custom_model_v7        0.266990               0.238323   \n",
       "18        densenet201_model_v3        0.342318               0.321824   \n",
       "3               vgg16_model_v2        0.143388               0.118884   \n",
       "12  inceptionresnetv2_model_v2        0.233962               0.197513   \n",
       "17        densenet201_model_v2        0.191695               0.139780   \n",
       "22             custom_model_v2        0.149513               0.111420   \n",
       "25             custom_model_v5        0.170618               0.125233   \n",
       "10        resnet152v2_model_v2        0.138426               0.075679   \n",
       "14        mobilenetv2_model_v2        0.130821               0.073480   \n",
       "1            xception_model_v2        0.129913               0.096901   \n",
       "20        nasnetlarge_model_v2        0.135382               0.066333   \n",
       "15        mobilenetv2_model_v3        0.229407               0.173645   \n",
       "\n",
       "    coco_map  avg_inf_time  \n",
       "7   0.552509      0.157125  \n",
       "4   0.543216      0.149908  \n",
       "8   0.531315      0.152425  \n",
       "2   0.530383      0.160573  \n",
       "13  0.526454      0.072912  \n",
       "21  0.526225      0.040169  \n",
       "6   0.522116      0.156698  \n",
       "19  0.514896      0.273407  \n",
       "5   0.507422      0.158064  \n",
       "16  0.500181      0.211093  \n",
       "9   0.495845      0.258722  \n",
       "23  0.490176      0.039542  \n",
       "26  0.485723      0.039469  \n",
       "0   0.477826      0.130748  \n",
       "11  0.476452      0.148716  \n",
       "24  0.476062      0.042544  \n",
       "27  0.425866      0.039022  \n",
       "18  0.414973      0.207552  \n",
       "3   0.411369      0.151350  \n",
       "12  0.401749      0.153812  \n",
       "17  0.399111      0.205991  \n",
       "22  0.396043      0.040156  \n",
       "25  0.368478      0.040562  \n",
       "10  0.338843      0.242696  \n",
       "14  0.338740      0.071017  \n",
       "1   0.338039      0.118183  \n",
       "20  0.318429      0.371174  \n",
       "15  0.296418      0.071613  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorting results dataframe by COCO mAP metric\n",
    "df_results_val_mAP = df_results_val.sort_values(by='coco_map', ascending=False)\n",
    "df_results_val_mAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will plot each model's COCO mAP score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting COCO mAP for each model validation data\n",
    "x_models_mAP = df_results_val_mAP['model']\n",
    "y_mAP = (df_results_val_mAP['coco_map'])*100\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "plt.bar(x_models_mAP, y_mAP, color='sienna')\n",
    "plt.xlabel(\"Models\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"COCO mAP (%)\")\n",
    "plt.title(\"COCO mAP for Crop & Weed Detection Models\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../Plots/COCO mAP for Crop & Weed Detection Models\", bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the model with the VGG16 backbone and the 6th version of the model heads had the highest COCO Mean Average Precision score. It's COCO mAP score on the validation set was 55.3%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will look at our secondary metric the average inference time the model takes to make a predition on an image.\n",
    "First we order the model iterations in order of lowest to highest average inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>voc_pascal_map</th>\n",
       "      <th>voc_pascal_map_allpts</th>\n",
       "      <th>coco_map</th>\n",
       "      <th>avg_inf_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>custom_model_v7</td>\n",
       "      <td>0.266990</td>\n",
       "      <td>0.238323</td>\n",
       "      <td>0.425866</td>\n",
       "      <td>0.039022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>custom_model_v6</td>\n",
       "      <td>0.329115</td>\n",
       "      <td>0.307787</td>\n",
       "      <td>0.485723</td>\n",
       "      <td>0.039469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>custom_model_v3</td>\n",
       "      <td>0.113536</td>\n",
       "      <td>0.077397</td>\n",
       "      <td>0.490176</td>\n",
       "      <td>0.039542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>custom_model_v2</td>\n",
       "      <td>0.149513</td>\n",
       "      <td>0.111420</td>\n",
       "      <td>0.396043</td>\n",
       "      <td>0.040156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>custom_model_v1</td>\n",
       "      <td>0.293027</td>\n",
       "      <td>0.276247</td>\n",
       "      <td>0.526225</td>\n",
       "      <td>0.040169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>custom_model_v5</td>\n",
       "      <td>0.170618</td>\n",
       "      <td>0.125233</td>\n",
       "      <td>0.368478</td>\n",
       "      <td>0.040562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>custom_model_v4</td>\n",
       "      <td>0.188992</td>\n",
       "      <td>0.165370</td>\n",
       "      <td>0.476062</td>\n",
       "      <td>0.042544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mobilenetv2_model_v2</td>\n",
       "      <td>0.130821</td>\n",
       "      <td>0.073480</td>\n",
       "      <td>0.338740</td>\n",
       "      <td>0.071017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>mobilenetv2_model_v3</td>\n",
       "      <td>0.229407</td>\n",
       "      <td>0.173645</td>\n",
       "      <td>0.296418</td>\n",
       "      <td>0.071613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mobilenetv2_model_v1</td>\n",
       "      <td>0.351913</td>\n",
       "      <td>0.344712</td>\n",
       "      <td>0.526454</td>\n",
       "      <td>0.072912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xception_model_v2</td>\n",
       "      <td>0.129913</td>\n",
       "      <td>0.096901</td>\n",
       "      <td>0.338039</td>\n",
       "      <td>0.118183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xception_model_v1</td>\n",
       "      <td>0.396248</td>\n",
       "      <td>0.358191</td>\n",
       "      <td>0.477826</td>\n",
       "      <td>0.130748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>inceptionresnetv2_model_v1</td>\n",
       "      <td>0.367082</td>\n",
       "      <td>0.377072</td>\n",
       "      <td>0.476452</td>\n",
       "      <td>0.148716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vgg16_model_v3</td>\n",
       "      <td>0.325206</td>\n",
       "      <td>0.326035</td>\n",
       "      <td>0.543216</td>\n",
       "      <td>0.149908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vgg16_model_v2</td>\n",
       "      <td>0.143388</td>\n",
       "      <td>0.118884</td>\n",
       "      <td>0.411369</td>\n",
       "      <td>0.151350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>vgg16_model_v7</td>\n",
       "      <td>0.336114</td>\n",
       "      <td>0.336334</td>\n",
       "      <td>0.531315</td>\n",
       "      <td>0.152425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>inceptionresnetv2_model_v2</td>\n",
       "      <td>0.233962</td>\n",
       "      <td>0.197513</td>\n",
       "      <td>0.401749</td>\n",
       "      <td>0.153812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>vgg16_model_v5</td>\n",
       "      <td>0.344542</td>\n",
       "      <td>0.338866</td>\n",
       "      <td>0.522116</td>\n",
       "      <td>0.156698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>vgg16_model_v6</td>\n",
       "      <td>0.420486</td>\n",
       "      <td>0.405343</td>\n",
       "      <td>0.552509</td>\n",
       "      <td>0.157125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>vgg16_model_v4</td>\n",
       "      <td>0.327362</td>\n",
       "      <td>0.309798</td>\n",
       "      <td>0.507422</td>\n",
       "      <td>0.158064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vgg16_model_v1</td>\n",
       "      <td>0.477733</td>\n",
       "      <td>0.449670</td>\n",
       "      <td>0.530383</td>\n",
       "      <td>0.160573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>densenet201_model_v2</td>\n",
       "      <td>0.191695</td>\n",
       "      <td>0.139780</td>\n",
       "      <td>0.399111</td>\n",
       "      <td>0.205991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>densenet201_model_v3</td>\n",
       "      <td>0.342318</td>\n",
       "      <td>0.321824</td>\n",
       "      <td>0.414973</td>\n",
       "      <td>0.207552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>densenet201_model_v1</td>\n",
       "      <td>0.391484</td>\n",
       "      <td>0.366518</td>\n",
       "      <td>0.500181</td>\n",
       "      <td>0.211093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>resnet152v2_model_v2</td>\n",
       "      <td>0.138426</td>\n",
       "      <td>0.075679</td>\n",
       "      <td>0.338843</td>\n",
       "      <td>0.242696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>resnet152v2_model_v1</td>\n",
       "      <td>0.425562</td>\n",
       "      <td>0.424156</td>\n",
       "      <td>0.495845</td>\n",
       "      <td>0.258722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>nasnetlarge_model_v1</td>\n",
       "      <td>0.451958</td>\n",
       "      <td>0.437492</td>\n",
       "      <td>0.514896</td>\n",
       "      <td>0.273407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>nasnetlarge_model_v2</td>\n",
       "      <td>0.135382</td>\n",
       "      <td>0.066333</td>\n",
       "      <td>0.318429</td>\n",
       "      <td>0.371174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         model  voc_pascal_map  voc_pascal_map_allpts  \\\n",
       "27             custom_model_v7        0.266990               0.238323   \n",
       "26             custom_model_v6        0.329115               0.307787   \n",
       "23             custom_model_v3        0.113536               0.077397   \n",
       "22             custom_model_v2        0.149513               0.111420   \n",
       "21             custom_model_v1        0.293027               0.276247   \n",
       "25             custom_model_v5        0.170618               0.125233   \n",
       "24             custom_model_v4        0.188992               0.165370   \n",
       "14        mobilenetv2_model_v2        0.130821               0.073480   \n",
       "15        mobilenetv2_model_v3        0.229407               0.173645   \n",
       "13        mobilenetv2_model_v1        0.351913               0.344712   \n",
       "1            xception_model_v2        0.129913               0.096901   \n",
       "0            xception_model_v1        0.396248               0.358191   \n",
       "11  inceptionresnetv2_model_v1        0.367082               0.377072   \n",
       "4               vgg16_model_v3        0.325206               0.326035   \n",
       "3               vgg16_model_v2        0.143388               0.118884   \n",
       "8               vgg16_model_v7        0.336114               0.336334   \n",
       "12  inceptionresnetv2_model_v2        0.233962               0.197513   \n",
       "6               vgg16_model_v5        0.344542               0.338866   \n",
       "7               vgg16_model_v6        0.420486               0.405343   \n",
       "5               vgg16_model_v4        0.327362               0.309798   \n",
       "2               vgg16_model_v1        0.477733               0.449670   \n",
       "17        densenet201_model_v2        0.191695               0.139780   \n",
       "18        densenet201_model_v3        0.342318               0.321824   \n",
       "16        densenet201_model_v1        0.391484               0.366518   \n",
       "10        resnet152v2_model_v2        0.138426               0.075679   \n",
       "9         resnet152v2_model_v1        0.425562               0.424156   \n",
       "19        nasnetlarge_model_v1        0.451958               0.437492   \n",
       "20        nasnetlarge_model_v2        0.135382               0.066333   \n",
       "\n",
       "    coco_map  avg_inf_time  \n",
       "27  0.425866      0.039022  \n",
       "26  0.485723      0.039469  \n",
       "23  0.490176      0.039542  \n",
       "22  0.396043      0.040156  \n",
       "21  0.526225      0.040169  \n",
       "25  0.368478      0.040562  \n",
       "24  0.476062      0.042544  \n",
       "14  0.338740      0.071017  \n",
       "15  0.296418      0.071613  \n",
       "13  0.526454      0.072912  \n",
       "1   0.338039      0.118183  \n",
       "0   0.477826      0.130748  \n",
       "11  0.476452      0.148716  \n",
       "4   0.543216      0.149908  \n",
       "3   0.411369      0.151350  \n",
       "8   0.531315      0.152425  \n",
       "12  0.401749      0.153812  \n",
       "6   0.522116      0.156698  \n",
       "7   0.552509      0.157125  \n",
       "5   0.507422      0.158064  \n",
       "2   0.530383      0.160573  \n",
       "17  0.399111      0.205991  \n",
       "18  0.414973      0.207552  \n",
       "16  0.500181      0.211093  \n",
       "10  0.338843      0.242696  \n",
       "9   0.495845      0.258722  \n",
       "19  0.514896      0.273407  \n",
       "20  0.318429      0.371174  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorting results dataframe by average inference time\n",
    "df_results_val_inf = df_results_val.sort_values(by='avg_inf_time')\n",
    "df_results_val_inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will plot each model's average inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting average inference times per image for each model validation data\n",
    "x_models_inf = df_results_val_inf['model']\n",
    "y_inf = df_results_val_inf['avg_inf_time']\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "plt.bar(x_models_inf, y_inf, color='sienna')\n",
    "plt.xlabel(\"Models\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"Average Inference Time Per Image (seconds)\")\n",
    "plt.title(\"Average Inference Time Per Image (CPU) for Crop & Weed Detection Models\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../Plots/Average Inference Time Per Image (CPU) for Crop & Weed Detection Models\", bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the model with the custom backbone and the 7th version of the model heads had the lowest average inference time. It's average inference time on the validation set was 0.039 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets evaluate both those models on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating model for mAP metrics and inference time and storing result\n",
    "result_test = evaluate_test(model=vgg16_model_v6, model_name='vgg16_model_v6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC PASCAL mAP for vgg16_model_v6 test data: 0.37646448612213135\n",
    "# VOC PASCAL mAP in all points for vgg16_model_v6 test data: 0.3488677442073822\n",
    "# COCO mAP for vgg16_model_v6 test data: 0.5667098164558411\n",
    "# Average inference time for vgg16_model_v6 test data: 0.16412345812870907"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending results of model evaluation to results dataframe (df_results)\n",
    "df_results_test = df_results_test.append(result_test, ignore_index=True)\n",
    "df_results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating model for mAP metrics and inference time and storing result\n",
    "result_test = evaluate_test(model=custom_model_v7, model_name='custom_model_v7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC PASCAL mAP for custom_model_v7 test data: 0.24317646026611328\n",
    "# VOC PASCAL mAP in all points for custom_model_v7 test data: 0.20839717984199524\n",
    "# COCO mAP for custom_model_v7 test data: 0.44890815019607544\n",
    "# Average inference time for custom_model_v7 test data: 0.03511611956816453"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending results of model evaluation to results dataframe (df_results)\n",
    "df_results_test = df_results_test.append(result_test, ignore_index=True)\n",
    "df_results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However for comparitive purposes, since custom_model_v7 and custom model_v1 had nearly identical average inference times, only a difference of 0.001 seconds, lets compare the best COCO mAP score model to the custom model with the best COCO mAP score as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating model for mAP metrics and inference time and storing result\n",
    "result_test = evaluate_test(model=custom_model_v1, model_name='custom_model_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC PASCAL mAP for custom_model_v1 test data: 0.2634250521659851\n",
    "# VOC PASCAL mAP in all points for custom_model_v1 test data: 0.2261561155319214\n",
    "# COCO mAP for custom_model_v1 test data: 0.5533956289291382\n",
    "# Average inference time for custom_model_v1 test data: 0.036563799931452826"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending results of model evaluation to results dataframe (df_results)\n",
    "df_results_test = df_results_test.append(result_test, ignore_index=True)\n",
    "df_results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we save all the results from the test set evaluation in a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving results dataframe as csv\n",
    "df_results_test.to_csv(r'../Evaluation/df_results_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading results for finals models evaluation on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>voc_pascal_map</th>\n",
       "      <th>voc_pascal_map_allpts</th>\n",
       "      <th>coco_map</th>\n",
       "      <th>avg_inf_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vgg16_model_v6</td>\n",
       "      <td>0.376464</td>\n",
       "      <td>0.348868</td>\n",
       "      <td>0.566710</td>\n",
       "      <td>0.164123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>custom_model_v7</td>\n",
       "      <td>0.243176</td>\n",
       "      <td>0.208397</td>\n",
       "      <td>0.448908</td>\n",
       "      <td>0.035116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>custom_model_v1</td>\n",
       "      <td>0.263425</td>\n",
       "      <td>0.226156</td>\n",
       "      <td>0.553396</td>\n",
       "      <td>0.036564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             model  voc_pascal_map  voc_pascal_map_allpts  coco_map  \\\n",
       "0   vgg16_model_v6        0.376464               0.348868  0.566710   \n",
       "1  custom_model_v7        0.243176               0.208397  0.448908   \n",
       "2  custom_model_v1        0.263425               0.226156  0.553396   \n",
       "\n",
       "   avg_inf_time  \n",
       "0      0.164123  \n",
       "1      0.035116  \n",
       "2      0.036564  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading validation results dataframe (df_results_val) from csv\n",
    "df_results_test = pd.read_csv('../Evaluation/df_results_test.csv')\n",
    "df_results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting COCO mAP for each model validation data\n",
    "x_models_final = df_results_test['model']\n",
    "y_mAP_final = (df_results_test['coco_map'])*100\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "plt.bar(x_models_final, y_mAP_final, color='sienna')\n",
    "plt.xlabel(\"Models\")\n",
    "plt.xticks(rotation=30)\n",
    "plt.ylabel(\"COCO mAP (%)\")\n",
    "plt.title(\"COCO mAP for Crop & Weed Detection Final Models\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../Plots/COCO mAP for Crop & Weed Detection Final Models\", bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the model with the VGG16 backbone and 6th version of the model heads had the highest COCO mAP score on the test set. Its COCO mAP score on the test set was 56.7% which is acutally higher than its score on the validation set (55.3%). It is also interesting to note that the model with the custom backbone and 1st version of the model heads, which was much faster than its vgg16 counterpart, 0.037 seconds compared to 0.164 seconds, respecitvely, only had a 1.4% reduction in COCO mAP score on the test set at 55.3% when compared to VGG16 backbone model (56.7%)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
